<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[RNA-seq中的那些统计学问题（一）为什么是负二项分布？]]></title>
    <url>%2F2019%2F05%2F07%2Frna-seq-data-analysis1%2F</url>
    <content type="text"><![CDATA[1. 转录组数据统计推断的难题在RNA-seq中进行两组间的差异分析是最正常不过的了。 我们在其它实验中同样会遇到类似的分析，通常，我们可以用方差分析判定两组“分布”数据间是否存在显著差异。原理是：当组间方差大于组内方差（误差效应），并且统计学显著时，则认为组间处理是可以引起差异的。 那这不就是咱们学过的统计学里普普通通的假设检验问题吗？用熟悉的算法简单地进行计算，分分钟就能搞定吧——凸样凸拿衣服，骚年要是事情都那么简单，要科学家干嘛，问题就在于：常规方法搞不定啊！ 其实统计学家也很无奈啊，看看我们转录组实验得到的这些数据吧：我们的实验只进行少得可怜的生物学重复（n&lt;10），而且，任何基因的表达量都不能是负数，这些数据并不符合正态分布，用于表征表达量的counts是非连续的（芯片信号是连续的），RNA-seq数据的离散通常是高度扭曲的，方差往往会大于均值……，就这些奇怪的特征，使得准确估计方差并没有想象的那么容易。 我们面临两个核心问题： 基因表达数据适合用什么统计学分布进行差异显著性检验？ 如何利用少量生物学重复数据估算基因表达的标准差？ 2. 泊松分布 or 负二项分布？从统计学的角度出发，进行差异分析肯定会需要假设检验，通常对于分布已知的数据，运用参数检验结果的假阳性率会更低。转录组数据中，raw count值符合什么样的分布呢？ count值本质是reads的数目，是一个非零整数，而且是离散的，其分布肯定也是离散型分布。对于转录组数据，学术界常用的分布包括泊松分布 (poisson)和负二项分布 (negative binomial)两种。 2.1. 为什么泊松分布不行？首先有必要简单地介绍一下泊松分布 泊松分布适合于描述单位时间（或空间）内随机事件发生的次数（事件发生的次数只能是离散的整数）。如某一服务设施在一定时间内到达的人数，电话交换机接到呼叫的次数，汽车站台的候客人数，机器出现的故障数，自然灾害发生的次数，一块产品上的缺陷数，显微镜下单位分区内的细菌分布数等等。 $$P(X=k)=\frac{\lambda^k }{k!}e^{-\lambda},\quad k=0,1,…$$ 泊松分布大概长这样： λ是波松分布所依赖的唯一参数。 λ值愈小分布愈偏倚， 随着λ的增大 ， 分布趋于对称。 当λ=20时分布接近于正态分布；当λ=50时， 可以认为波松分布呈正态分布。 在数据分析的早期，确实有学者采用泊松分布进行差异分析，但是发展到现在，几乎全部都是基于负二项分布了，究竟是什么因素导致了这种现象呢？为了解释这个问题，我们必须提到一个概念 overdispersion。 dispersion指的是离散程度，研究一个数据分布的离散程度，我们常用方差这个指标。对于泊松分布而言，其均值和方差是相等的，但是我们的数据确不符合这样的规律。通过计算所有基因的均值和方差，可以绘制如下的图片： 横坐标为基因在所有样本中的均值，纵坐标为基因在所有样本中的方差，直线的斜率为1，代表泊松分布的均值和方差的分布。可以看到，真实数据的分布是偏离了泊松分布的，方差明显比均值要大。 如果假定总体分布为泊松分布， 根据我们的定量数据是无法估计出一个合理的参数，能够符合上图中所示分布的，这样的现象就称之为overdispersion。 由于真实数据与泊松分布之间的overdispersion，选择泊松分布分布作为总体的分布是不合理。 以上只证明了泊松分布是个不太恰当的分布估计，那怎么证明负二项分布就是合适的分布估计呢？ 2.2. 为什么负二项分布行？主要是从均值与方差之间的关系去证明 同样的，也先简单介绍一下负二项分布： 二项分布描述的是n重伯努利实验，在n重贝努利试验中，事件A恰好发生x(0≤x≤n)次的概率为： $$P_n(x)=C_n^x p^x(1-p)^{n-x}$$ 它的概率分布图如下： 负二项分布描述的也是伯努利实验，不过它的目标事件变成了：对于Bernoulli过程，我们设定，当某个结果出现固定次数的时候，整个过程的数量，比如我们生产某个零件，假设每个零件的合格与否都是相互独立的，且分布相同，那么当我们生产出了五个不合格零件时，一共生产了多少合格的零件，这个数量就是一个负二项分布，公式如下： $$f(k;r;p)=P(x=k)=C_{r+k-1}^k p^k(1-p)^r$$ 该公式描述的是，在合格率为p的一堆产品中，进行连续有放回的抽样，当抽到r个次品时，停止抽样，此时抽到的正品正好为k个的概率 它的概率分布如下： 负二项分布的均值和方差分别为： $$\mu=\frac{pr}{1-p}$$ $$\sigma^2=\frac{pr}{(1-p)^2}$$ 将p用μ表示，得到： $$p=\frac{\mu}{\mu+r},\quad 1-p=\frac{r}{\mu+r}$$ 将上一步推出的p和1-p带入到方差的表达式中，得到： $$\sigma^2=\frac{\mu^2}{r}+\mu$$ 记1/r=α，则 $$\sigma^2=\mu+\alpha\mu^2$$ 从上面的式子可以看出，均值是方差的二次函数，方差随着均值的增加而进行二次函数形式的递增，正好符合上文 2.1. 为什么泊松分布不行？ 部分均值与方差分布图的情况 其中α和r被称为dispersion parameter 负二项分布与泊松分布的关系，可以用α或r推出： 当 r -&gt; ∞ 时，α -&gt; 0，此时 σ2= μ，为泊松分布； 当 r -&gt; 0 时，α -&gt; ∞，此时overdispersion 3. 方差估计在生物学重复很少时，我们是很难准确计算每个基因表达的标准差的（相当于这个数据集的离散程度）。我们很可能会低估数据的离散程度。 被逼无奈的科学家提出了一个假设：表达丰度相似的基因，在总体上标准差应该也是相似的。我们把不同生物学重复中表达丰度相同的基因的总标准差取个平均值，低于这个值的都用这个值，高于这个值的就用算出来的值。 参考资料： (1) 【生信修炼手册】负二项分布在差异分析中的应用 (2) 【 生信百科】转录组差异表达筛选的真相 (3) 【生信媛】RNA-seq分析中的dispersion，你知道吗？ (4) H. J. Pimentel, et al. Differential analysis of RNA-Seq incorporatingquantification uncertainty. bioRxiv, 2016 注：本文章已经发表于微信公众号《宇宙实验媛》，如需转载，请联系本人或该公众号 欢迎关注宇宙实验媛]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[运用AI对科研文章中的图片进行绘图]]></title>
    <url>%2F2019%2F05%2F07%2Fai-drawing%2F</url>
    <content type="text"><![CDATA[TMJ universebiologygirl本次视频主要以《Polarizing brain organoids》这篇文章内的Fig. 1为例，使用Adobe Illustrator CS6 对Fig.1 进行绘图，供大家学习参考。此次视频主要涉及的内容为“基础图形的绘制”、“渐变色添加”、“图形反光绘制”、“路径查找器工具的使用”等。AI drawing下面将对绘图过程进行展开说明：第一步：文件→新建→对画板进行设置。一般为方便投稿，配置文件选择“打印”，大小选择“A4”，颜色模式CMYK，(具体可根据投稿要求选择，CMYK模式是彩色印刷时采用的一种套色模式) （视频内设置仅为画图方便所设）。第二步：绘制图a的图片的基本形态。采用左边工具栏“画笔工具”，按住鼠标，绘制出想要的图形（画出大致轮廓即可，可后期通过锚点调整）。第三步：通过锚点调整形状，主要用到两个工具添加/删除锚点工具、直接选择工具。直接选择工具可对锚点进行调整，从而达到调整基本形状的目的。若第二步画出的基本形状的锚点不足以调整，可通过添加/删除锚点工具，在线条上添加或删除后，再用直接选择工具调整。转换工具可使锚点周围线条变得平滑。（如下图标出的红框所示）第四步：填充背景颜色。该步骤使用到几个工具：吸管工具、填色、描边、渐变。将图形填充后可用画笔用具将边缘颜色画好，同样可用锚点、直接选择工具对边缘形状进行调整。吸管工具可对已知颜色进行选取。第五步：绘制培养皿。先画出一个基本的椭圆形，再使用3D工具对这个椭圆形进行3D加工。红框界面可调整视觉角度。调整结束后可采用对象→拓展外观，然后添加描边。（本视频中采用的是添加线条的方法，没有拓展外观工具方便）。随后可用矩形工具、椭圆工具等画出培养皿底部，培养物等基本形状。第六步：继续用矩形、椭圆工具画出圆底烧杯的基本图形框架。用窗口→路径查找器→形状模式→联集，对图形轮廓进行合并（可自己尝试多种路径查找器功能，拼接出自己想要的图案）。其余填色等步骤与前文所提及的步骤异曲同工。第七步：绘制器皿的反光和阴影。用弧形工具绘制出曲线后，选择线条形状，如“等比”等，再通过锚点工具、直接选择工具对线条进行调整即可。阴影的绘制可采用镜像工具对已画出的反光图形进行翻转。第八步：先画出一个椭圆，改成白色的渐变色，采用渐变工具对渐变方向做调整，对透明度作出调整（可自行调节到需要的透明度），使用径向模糊工具作出调整。其他步骤与上述步骤均相似，不再赘述。Tip：在word等工具性软件常用的通用快捷键如下（常用的复制粘贴就不说啦） 撤销：Ctrl+z 剪切：ctrl+x 加粗：ctrl+b 绘图时希望圆形呈正圆或画出的直线是笔直不倾斜时，按住shift再进行拖拽放大/缩小 AI里整个图形放大缩小，需要按住Alt而不是Ctrl。参考文献：https://www.nature.com/articles/s41587-019-0084-4）]]></content>
      <categories>
        <category>Paper writing</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[R语言进行样本间相关性分析]]></title>
    <url>%2F2019%2F05%2F07%2Fcorrelation-analysis-in-R%2F</url>
    <content type="text"><![CDATA[熊东彦 universebiologygirl 写在前面的话生物学数据分析中相关性分析是十分重要的一个环节，但这个环节又经常被忽视。以生物信息学分析为例：非生物信息专业的学生通常在分析实验获得测序数据时都会忽略这个问题。 大多数非专业生物信息学背景学生的生物信息数据分析技巧都是从网上的一些教程或者是培训机构的视频中学到的，遗憾的是培训机构仅仅只是教会了你跑一个组学分析的流程，其中涉及的很多重要的细节并没有教给你。因此，从细节着手进行相关性分析显得尤为重要。例如：在转录组测序时候通常要选择3个生物学重复。但是选择了3个生物学重复后送去测序，拿到数据跑完一遍流程真的就可以用来进行差异表达分析吗？绝大多数情况下是可以的，但如果你研究的是细菌、病毒这些变异相对较多的物种，你的一个处理的不同生物学重复可能就不一定是一致的了。如果这种情况发生，那就要剔除这组变异数据。用剩下的数据进行差异表达分析。毕竟设置3个生物学重复一方面是保证一定的重复性，一方面也是预防某个重复值不能用的情况。当然在其他生物学数据分析中，也有这种情况。 今天介绍一个利用R语言编程来进行相关性分析的方法。毕竟当样本数据量较大的情况，手动挨个做相关性分析非常费时间。同时用R语言做完相关性分析后的最大特点是可视化，可以直接提升文章数据的档次感。 手把手教程呈现：回顾一下相关性分析概念：相关性分析是指对两个或多个具备相关性的变量元素进行分析进而衡量两个变量因素的相关密切程度。相关性的元素之间需要存在一定的联系或者概率才可以进行相关性分析。相关性分析分为pearson相关分析、spearman相关性分析和kendall相关性分析。本次主要介绍Pearson相关分析。它适用于变量为连续性变量且变量之间存在线性关系的情况，二者缺一不可。 示例数据：来源于小鼠感染某种烈性病毒前后的转录组测序数据。实验设计为随机选择3只同一窝出生的小鼠，先提取血液进行转录组测序，随后同时向同窝小鼠注射某种烈性病毒，感染病毒第7天提取血液再进行转录组测序。拿到测序数据后跑完部分转录组分析流程，获取基因表达的reads计数结果。 在进行Pearson相关分析前，按照Pearson相关分析的基本要求先判断两个变量之间是否存在线性关系。随机选取同一个处理的两个生物学重复，用ggplot2绘制两个变量之间的散点图。 library(ggplot2) ggplot(data = OUT883_withoutgenename[,1:2],mapping = aes(x=OUT883_withoutgenename[,1],y=OUT883_withoutgenename[,2],color="red"))+geom_point()+geom_smooth(method = lm) #geno_point()绘制散点图，geom_smooth()进行拟合，method选择lm是线性拟合 从上图可以看出，除去个别区间，两个变量主要呈现线性关系。之所以个别区间如x=0附近是这样的关系是由于我没有对数据进行过滤。（比如因为测序误差导致的在两个生物学重复之间由于测序仪误差一个基因在被测到而在对应生物学重复内没有测到）。 判断符合Pearson相关性分析条件后可以进行后续分析。现在要对这些数据进行两两相关性分析，以初步判断样本之间是否有较大差异。 readscount]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[如何理解和计算FDR?]]></title>
    <url>%2F2019%2F05%2F07%2Ffdr%2F</url>
    <content type="text"><![CDATA[按照惯例，先来个自我介绍： 大家好，我是实习6年的生信狗小明同学，不会唱、跳、Rap，喜欢装X，瞎逼逼和开车（话说我驾照都没啊），以后我将在这里给大家带来生信、统计学和机器学习方面的分享，希望大家能够喜欢（欢呼声和掌声现在可以起来了╭(●｀∀´●)╯） 最后给大家提供一个关注我的传送门：https://ming-lian.github.io，第一时间的更新在我的个人主页上，发到这里的都是在个人主页基础上的整理 爱你们哟｡:.ﾟヽ(｡◕‿◕｡)ﾉﾟ.:｡+ﾟ 下面是正文 1. 多重假设检验的必要性统计学中的假设检验的基本思路是： 设立零假设（null hypothesis）$H_0$，以及与零假设$H_0$相对应的非零假设（alternative hypothesis， or reject null hypothesis）$H_1$，在假设$H_0$成立的前提下，计算出$H_0$发生的概率，若$H_0$的发生概率很低，基于小概率事件几乎不可能发生，所以可以拒绝零假设 但是这些传统的假设检验方法研究的对象，都是一次试验 在一次试验中（注意：是一次试验， 即single test），0.05 或0.01的cutoff足够严格了(想象一下，一个口袋有100个球，95个白的，5个红的, 只让你摸一次，你能摸到红的可能性是多大？) 但是对于多次试验，又称多重假设检验，再使用p值是不恰当的，下面来分析一下为什么： 大家都知道墨菲定律：如果事情有变坏的可能，不管这种可能性有多小，它总会发生 用统计的语言去描述墨菲定律： 在数理统计中，有一条重要的统计规律：假设某意外事件在一次实验（活动）中发生的概率为p（p&gt;0），则在n次实验（活动）中至少有一次发生的概率为 $p_n=1-(1-p)^n$ 由此可见，无论概率p多么小（即小概率事件），当n越来越大时，$p_n$越来越接近1 这和我们的一句俗语非常吻合：常在河边走，哪有不湿鞋；夜路走多了，总能碰见鬼 在多重假设检验中，我们一般关注的不再是每一次假设检验的准确性，而是控制在作出的多个统计推断中犯错误的概率，即False Discover Rate（FDR），这对于医院的诊断情景下尤其重要： 假如有一种诊断艾滋病(AIDS)的试剂，试验验证其准确性为99%（每100次诊断就有一次false positive）。对于一个被检测的人（single test)）来说，这种准确性够了；但对于医院 （multiple test)）来说，这种准确性远远不够 因为每诊断10 000个个体，就会有100个人被误诊为艾滋病(AIDS)，每一个误诊就意味着一个潜在的医疗事故和医疗纠纷，对于一些大型医院，一两个月的某一项诊断的接诊数就能达到这个级别，如果按照这个误诊率，医院恐怕得关门，所以医院需要严格控制误诊的数量，宁可错杀一万也不能放过一个，因为把一个没病的病人误判为有病，总比把一个有病的病人误判为没病更保险一些 100 independent genes. (We have 100 hypotheses to test) No significant differences in gene expression between 2 classes (H0 is true). Thus, the probability that a particular test (say, for gene 1) is declared significant at level 0.05 is exactly 0.05. (Probability of reject H0 in one test if H0 is true = 0.05) However, the probability of declaring at least one of the 100 hypotheses false (i.e. rejecting at least one, or finding at least one result significant) is: $$1-(1-0.05)^{100}\approx 0.994$$ 2. 区别p值和q值 $H_0$ is true $H_1$ is true Total Not Significant TN FN TN+FN Significant FP TP FP+TP Total TN+FP FN+TP m 首先从上面的混淆矩阵来展示p值域q值的计算公式，就可以看出它们之间的区别： p值 p值实际上就是false positive rate(FPR，假正率)： $$p-value=FPR=\frac{FP}{FP+TN}$$ 直观来看，p值是用上面混淆矩阵的第一列算出来的 q值 q值实际上就是false discovery rate (FDR)： $$q-value=FDR=\frac{FP}{FP+TP}$$ 直观来看，q值是用上面混淆矩阵的第二行算出来的 但是仅仅知道它俩的计算公式的差别还不够，我们还有必要搞清楚一个问题：它俩在统计学意义上有什么不同呢？ p值衡量的是一个原本应该是$H_0$的判断被错误认为是$H_1 \, (reject H_0)$的比例，所以它是针对单次统计推断的一个置信度评估； q值衡量的是在进行多次统计推断后，在所有被判定为显著性的结果里，有多大比例是误判的 据此，我们可以推导出p值域q值（q值有两种定义，FWER或FDR，这里指的是FWER）之间的关系： 总共有n个features(可以是基因，GWAS中的snp位点等)，对它们执行n重假设假设检验后，得到各自对应的p值分别为$\{p^{(i)} \mid i=1,2,…,n\}$ 当p值显著性水平取$\alpha$时，得到$k$个features具有p值显著性，它们的p值为$\{p^{(i)}{(j)} \mid j=1,2,…,k\}$，其中$p^{(i)}{(j)}$表示第i个feature它的p值在升序中的排名为j，那么这k个features的FWER可以表示为： $$FWER=1-\prod_{j=1}^{k}(1-p^{(i)}_{(j)})$$ 3. 如何计算q值？统计检验的混淆矩阵： $H_0$ is true $H_1$ is true Total Significant V S R Not Significant U T m-R Total m0 m-m0 m FWER (Family Wise Error Rate) 作出一个或多个假阳性判断的概率 $$FWER=Pr(V\ge 1)$$ 使用这种方法的统计学过程： The Bonferroni procedure Tukey’s procedure Holm’s step-down procedure FDR (False Discovery Rate) 在所有的单检验中作出假阳性判断比例的期望 $$FDR=E\left[\frac{V}{R}\right]$$ 使用这种方法的统计学过程： Benjamini–Hochberg procedure Benjamini–Hochberg–Yekutieli procedure 3.1. Benjamini-Hochberg procedure (BH)对于m个独立的假设检验，它们的P-value分别为：$p_i,i=1,2,…,m$ （1）按照升序的方法对这些P-value进行排序，得到： $$p_{(1)} \le p_{(2)} \le … \le p_{(m)}$$ （2）对于给定是统计显著性值$\alpha \in (0,1)$，找到最大的k，使得 $$p_{(k)} \le \frac{\alpha * k}{m}$$ （3）对于排序靠前的k个假设检验，认为它们是真阳性 (positive ) 即：$reject \, H_0^{(i)},\, 1 \le i \le k$ $$\begin{array}{c|l}\hlineGene &amp; p-value \\\hlineG1 &amp; P1 =0.053 \\\hlineG2 &amp; P2 =0.001 \\\hlineG3 &amp; P3 =0.045 \\\hlineG4 &amp; P4 =0.03 \\\hlineG5 &amp; P5 =0.02 \\\hlineG6 &amp; P6 =0.01 \\\hline\end{array}\, \Rightarrow \,\begin{array}{c|l}\hlineGene &amp; p-value \\\hlineG2 &amp; P(1) =0.001 \\\hlineG6 &amp; P(2) =0.01 \\\hlineG5 &amp; P(3) =0.02 \\\hlineG4 &amp; P(4) =0.03 \\\hlineG3 &amp; P(5) =0.045 \\\hlineG1 &amp; P(6) =0.053 \\\hline\end{array}$$ $$\alpha = 0.05$$ $P(4) =0.03&lt;0.05*\frac46=0.033$ $P(5) =0.045&gt;0.05*\frac56=0.041$ 因此最大的k为4，此时可以得出：在FDR&lt;0.05的情况下，G2，G6，G5 和 G4 存在差异表达 可以计算出q-value： $$p_{(k)} \le \frac{\alpha*k}{m} \, \Rightarrow \, \frac{p_{(k)}*m}{k} \le \alpha$$ Gene P q-value G2 P(1) =0.001 0.006 G6 P(2) =0.01 0.03 G5 P(3) =0.02 0.04 G4 P(4) =0.03 0.045 G3 P(5) =0.045 0.054 G1 P(6) =0.053 0.053 根据q-valuea的计算公式，我们可以很明显地看出： $$q^{(i)}=p_{(k)}^{(i)}*\frac{Total \, Gene \, Number}{rank(p^{(i)})}=p_{(k)}^{(i)}*\frac{m}{k}$$ 即，根据该基因p值的排序对它进行放大，越靠前放大的比例越大，越靠后放大的比例越小，排序最靠后的基因的p值不放大，等于它本身 我们也可以从可视化的角度来看待这个问题： 对于给定的$\alpha \in (0,1)$，设函数$y=\frac{\alpha}{m}x \quad (x=1,2,…,m)$，画出这条线，另外对于每个基因，它在图上的坐标为$(rank(p_{(k)}^{(i)}),p_{(k)}^{(i)})=(k,p_{(k)}^{(i)})$，图如下： 通过设置$\alpha$可以改变图中直线的斜率，$\alpha$越大，则直线的斜率越大，落在直线下方的点就越多，通过FDR检验的基因也就越多，反之，直线的斜率越小，落在直线下方的点就越少，通过FDR检验的基因也就越少 当固定$\alpha$，而统计检验次数m增加时，这条直线的斜率变小，落在直线下方的点就越少，通过FDR检验的基因也就越少 参考资料： (1) Storey, J.D. &amp; Tibshirani, R. Statistical signifcance for genomewide studies.Proc. Natl. Acad. Sci. USA 100, 9440–9445 (2003) (2) 国科大研究生课程《生物信息学》，陈小伟《基因表达分析》]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[NCBI Taxonomy数据处理：TaxonKit工具详解]]></title>
    <url>%2F2019%2F04%2F22%2Ftaxonkit-usage%2F</url>
    <content type="text"><![CDATA[原创：老板，来一打TPU 遇到的问题 在做宏基因组分析时，通过基因注释得到一个包含10k之多种微生物物种名list(scientific name)，现在想统计这些物种在界、门、纲、目、科、属等不同分类水平的总的数量。这就是本篇推送想解决的问题，10000多种微生物的拉丁名称示例如下： [NeptuneYT$] head scientific_name.txt Abiotrophia defectivaAbiotrophia sp.Absiella dolichumAcaryochloris marinaAcetanaerobacterium sp.Acetivibrio cellulolyticusAcetoanaerobium noteraeAcetoanaerobium sticklandiiAcetobacter acetiAcetobacter ghanensis [NeptuneYT$] wc -l all_bacteria_genomic_fna.species 10146 all_bacteria_genomic_fna.species 打开NCBI Taxonomy输入一个拉丁名，如Acetobacter aceti，搜索之后默认获得完整的lineage信息，但我们这里只需要7个层次的，因此再点击一次Lineage获得缩略的谱系信息，如下：得到的Lineage字段后以分号隔开的就是对应于7个分类层次的结果，后续以分号切割之后统计不同列的结果即可。很自然的，我们想到爬虫，其搜索接口为:https://www.ncbi.nlm.nih.gov/taxonomy/?term=拉丁名（空格以+号连接），如https://www.ncbi.nlm.nih.gov/taxonomy/?term=Acetobacter+aceti，然后对结果页面进行后续解析。但是10k之多的查询量，必然要设置爬取频率，否则就要被NCBI关小黑屋了，考虑时间代价，果断放弃。其实，从网上查询的原理也是基于Taxonomy后台的数据库，而这个文件在ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz，可以从解压之后的names.dmp和nodes.dmp文件写代码解析，但是其内容过于妖孽，为了少撸掉点头发，因此先看看网上是否有造好的轮子。果然，动动手指发现有三个工具可以实现以上诉求，ETE toolkit, taxadb和 TaxonKit，这里选择最近发表的TaxonKit，优势在于其直接基于names.dmp和nodes.dmp文件的解析，本地搜索速度块，尤其是大批量的查找和格式转换，另外使用也极简单。TaxonKit paper相比于另外两种工具，TaxonKit在处理大批量数据时更快，占用内存也可接受 taxonkit 概述说完废话，进入今天的主题，说说TaxonKit这个工具的使用。TaxonKit是处理NCBI Taxonomy数据库中结构性数据的良心工具，19年1月在bioRxiv上online，作者Wei Shen, Jie Xiong，隶属于Department of ClinicalLaboratory, General Hospital of Western Theater Command，特地查了一下，原来是位于成都的中国人民解放军西部战区总医院（好牛的感觉），看来生信真是无处不在。它是Go语言编写的，可以在Windows，Linux和Mac OS X运行，直接使用NCBI Taxonomy的数据（需手动下载）而无需构建本地数据库。 taxonkit安装安装选择对应系统的版本安装，推荐conda安装。详见https://github.com/shenwei356/taxonkit,conda安装:conda install -c bioconda taxonkit 下载依赖数据下载NCBI taxonomy数据库的taxdump.tar.gz文件，解压后将names.dmp和 nodes.dmp拷贝到家目录下的.taxonkit目录下。 wget -c ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz tar -zxvf taxdump.tar.gz mkdir -p $HOME/.taxonkit cp names.dmp nodes.dmp $HOME/.taxonkit 确认文件完整： [NeptuneYT]$ ll -h ~/.taxonkit/ total 155494-rw-r–r– 1 xx UsersGrp 169.3M Mar 18 16:07 names.dmp-rw-r–r– 1 xx UsersGrp 134.4M Mar 18 16:07 nodes.dmp 配置完成，开始使用。 taonkit使用taxonkit –help Usage: taxonkit [command] Available Commands: genautocomplete generate shell autocompletion script help Help about any command lineage query lineage of given taxids list list taxon tree of given taxids name2taxid query taxid by taxon scientific name reformat reformat lineage version print version information and check for update…Use “taxonkit [command] –help” for more information about a command. taxonkit按照功能分成不同的子命令，其中最主要的功能包括4块：1）列出给定taxonomy id(taxid)的子分类树：list2）从taxid获取完整谱系：lineage3）重新构造谱系的格式：reformat4）通过物种拉丁名查询taxid：name2taxid 1）列出给定taxonomy id的子分类树[NeptuneYT$] taxonkit list --help #查看list子命令使用方法 list taxon tree of given taxidsUsage: taxonkit list [flags]Flags: -h, –help help for list –ids string taxid(s), multiple values should be separated by comma –indent string indent (default “ “) –json output in JSON format. you can save the result in file with suffix “.json” and open with modern text editor –show-name output scientific name –show-rank output rankGlobal Flags: –data-dir string directory containing nodes.dmp and names.dmp (default “/home/xx/.taxonkit”) –line-buffered use line buffering on output, i.e., immediately writing to stdin/file for every line of output -o, –out-file string out file (“-“ for stdout, suffix .gz for gzipped out) (default “-“) -j, –threads int number of CPUs. (default value: 1 for single-CPU PC, 2 for others) (default 2) –verbose print verbose information 实例：给定taxid：9606和10090 [NeptuneYT$] taxonkit list --ids 9606,10090 --show-name --show-rank -j 2 #–ids 给定的taxid，多个以英文逗号分割 #–show-name 输出科学命名 #–show-rank 输出分类等级 #-j 线程数，默认是2 9606 [species] Homo sapiens 63221 [subspecies] Homo sapiens neanderthalensis 741158 [subspecies] Homo sapiens subsp. ‘Denisova’ 10090 [species] Mus musculus 10091 [subspecies] Mus musculus castaneus 10092 [subspecies] Mus musculus domesticus 35531 [subspecies] Mus musculus bactrianus 39442 [subspecies] Mus musculus musculus 46456 [subspecies] Mus musculus wagneri 57486 [subspecies] Mus musculus molossinus 80274 [subspecies] Mus musculus gentilulus 116058 [subspecies] Mus musculus brevirostris 179238 [subspecies] Mus musculus homourus 477815 [subspecies] Mus musculus musculus x M. m. domesticus 477816 [subspecies] Mus musculus musculus x M. m. castaneus 947985 [subspecies] Mus musculus albula 1266728 [subspecies] Mus musculus domesticus x M. m. molossinus 1385377 [subspecies] Mus musculus gansuensis 1643390 [subspecies] Mus musculus helgolandicus 1879032 [subspecies] Mus musculus isatissus 2）从taxid获取完整谱系[NeptuneYT$] echo 9606|taxonkit lineage -d &quot;-&quot; -t -r #-d 输出谱系树分割符，默认分号 #-t 显示包含taxid的谱系树 #-r 显示给定taxid的分类等级 9606 cellular organisms-Eukaryota-Opisthokonta-Metazoa-Eumetazoa-Bilateria-Deuterostomia-Chordata-Craniata-Vertebrata-Gnathostomata-Teleostomi-Euteleostomi-Sarcopterygii-Dipnotetrapodomorpha-Tetrapoda-Amniota-Mammalia-Theria-Eutheria-Boreoeutheria-Euarchontoglires-Primates-Haplorrhini-Simiiformes-Catarrhini-Hominoidea-Hominidae-Homininae-Homo-Homo sapiens 131567-2759-33154-33208-6072-33213-33511-7711-89593-7742-7776-117570-117571-8287-1338369-32523-32524-40674-32525-9347-1437010-314146-9443-376913-314293-9526-314295-9604-207598-9605-9606 species 3）重新构造谱系的格式上一步通过taxid提取的谱系信息复杂，往往需要根据我们的需求重新格式化 [NeptuneYT$] echo 9606|taxonkit lineage |taxonkit reformat 9606 cellular organisms;Eukaryota;Opisthokonta;Metazoa;Eumetazoa;Bilateria;Deuterostomia;Chordata;Craniata;Vertebrata;Gnathostomata;Teleostomi;Euteleostomi;Sarcopterygii;Dipnotetrapodomorpha;Tetrapoda;Amniota;Mammalia;Theria;Eutheria;Boreoeutheria;Euarchontoglires;Primates;Haplorrhini;Simiiformes;Catarrhini;Hominoidea;Hominidae;Homininae;Homo;Homo sapiensEukaryota;Chordata;Mammalia;Primates;Hominidae;Homo;Homo sapiens 输出结果的第三列就是重新格式化的结果，默认是(“{k};{p};{c};{o};{f};{g};{s}”)7个水平。查询给定taxid9606的谱系，并按照门：科；属的格式输出 [NeptuneYT$] echo 9606|taxonkit lineage |taxonkit reformat -f &quot;{p}:{f};{s}&quot; |cut -f3 #{}内是分类等级，大括号之间是输出的连接符 Chordata:Hominidae;Homo sapiens 4）通过物种拉丁名查询taxid：name2taxid按人的拉丁名查询taxid [NeptuneYT$] echo &quot;Homo sapiens&quot; |taxonkit name2taxid Homo sapiens 9606批量查询 [NeptuneYT$] head scientific_name.txt |taxonkit name2taxid --show-rank Abiotrophia defectiva 46125 speciesAbiotrophia sp. 76631 speciesAbsiella dolichum 31971 speciesAcaryochloris marina 155978 speciesAcetanaerobacterium sp.Acetivibrio cellulolyticus 35830 speciesAcetoanaerobium noterae 745369 speciesAcetoanaerobium sticklandii 1511 speciesAcetobacter aceti 435 speciesAcetobacter ghanensis 431306 species 回到问题基于Taxonkit上述用法，回到之前的问题就好解决了1.将scientific name先转化成taxid，便于查找lineage time taxonkit name2taxid scientific_name.txt >scientific_name_taxid.txt &amp; awk -F"\t" '$2!=""{print $2}' scientific_name.txt >find_taxid.txt #去掉未查到的，即空值 awk -F"\t" '$2==""' scientific_name_taxid.txt >NotFindName.txt #输出未查到物种名 awk -F"\t" 'BEGIN{OFS="\t";print "findTaxid\tNull\tTotal"}{$2!=""?taxid++:null++}\ END{print taxid,null,taxid+null}' scientific_name_taxid.txt |column -t #统计查找到的和未查到的数量 real 0m6.279sfindTaxid Null Total9606 541 10147 可以看到，查询速度相当之快。由于待批量查询的物种名不是规范的拉丁名称，导致出现两个问题，一是输入的list是10146个,转换id后（找到和未找到）的行数却增加了1个，统计之后发现是同一个物种名有两个taxid；二是没找到的高达541个！！！为了说明完整的处理过程，541个后面再说。 [NeptuneYT$]$ awk -F"\t" '{print $1}' scientific_name_taxid.txt |sort |uniq -d Deinococcus soli 手工查询发现是两个种，但是根据部分拉丁名Deinococcus soli可以查到俩taxid我也是醉了。2.查找lineage [NeptuneYT$] time taxonkit lineage find_taxid.txt >lineage.txt&amp; real 0m7.860s 3.重新格式化lineage [NeptuneYT$] time taxonkit reformat lineage.txt|cut -f3 &gt;newformat.txt&amp; real 0m11.465s 结果：现在就按照界门纲目科属种的层次变成整齐划一的格式了，通过简单处理即可统计不同层次的物种数量和分布，首先构建一个用于循环处理的分类标签 [NeptuneYT$] cat tag.txt 1 Kingdom2 Phylum3 Class4 Order5 Family6 Genus7 Species 详细的物种分类层次数量统计： [NeptuneYT$] cat tag.txt|while read num lev;do awk -v v=$num -F";" '{print $v}' newformat.txt|sort |uniq -c|sort -nr |awk -v v=$lev '{print v"\t"$0}' \ |awk '$3!=""';done >detail_taxonomy_range.txt [NeptuneYT$] head detail_taxonomy_range.txt Kingdom 6722 BacteriaKingdom 4 EukaryotaPhylum 2682 ProteobacteriaPhylum 1390 FirmicutesPhylum 1099 ActinobacteriaPhylum 729 Bacteroidetes 按7个类别统计数量： [NeptuneYT$] cat tag.txt|while read index level;do num=$(awk -v v=$index -F";" '{print $v}' \ newformat.txt|sort |uniq |wc -l);printf "${level}\t${num}\n";done |tee taxonomy_stat.txt Kingdom 2Phylum 118Class 82Order 181Family 401Genus 1824Species 6519 简单画个图瞅瞅： databracket_minus_name.txt #minus all bracket awk '{filed1=$1;$1="";sub(/ /,"");gsub(/ /,"_");print filed1,$0}'\ NotFindName.txt >underline_plus_name.txt #plus all filed of split underline except first one 将后面查到的taxid加到之前的taxid list再按之前的流程跑一遍即可，实在查不到的那就手工吧。（此时配乐起~“这是自由的感觉，鼠标咔哒咔哒点击这些可爱的物种名称，凭着一颗永不哭泣勇敢的心”） 参考taxonkit githubTaxonKit: a cross-platform and efficient NCBI taxonomy toolkit]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
      <tags>
        <tag>taxonkit</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[source/about/about.md]]></title>
    <url>%2F2019%2F04%2F19%2Fsource-about-about-md%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[来吧，让我们相爱吧--硬核推送]]></title>
    <url>%2F2019%2F04%2F10%2Fpush-us%2F</url>
    <content type="text"><![CDATA[原创: 伊颜落芸 宇宙实验媛 友情公众号推送​ 一直关注我们生信技能树公众号的小伙伴肯定知道，我们一向的原则是分享学习心得，提供知识的干货。所以今天就向大友情家推荐一个亲民又有料的公众号—宇宙实验媛。 生信技能树 &amp; 宇宙实验媛十年同窗，五年同行。 因生信而结缘，因科学而励志 我们的创始人Jimmy和宇宙实验媛的主编yuzy是大学同学，都说妇女能顶半边天，yuzy既然喊出了豪言壮语，那小编不得不推荐一下了😜 关于“宇宙实验媛”​ 一听这个名字就很霸气😲，这个公众号的重点分享基因组学实验操作和生物信息学方法，但也关注优秀文献的分享，实用方法的解读，理论模型的创新。立足于全方位多角度揭示科学的故事。 下面是该公众号的往期精选 模式动物选择：​ 微生物组学研究的那些奇葩动物学模型 介绍了研究终极目标—明确三种关系：生境内各微生物之间的关系；微生物与物质代谢/物质循环的关系；微生物与生境（比如宿主内）稳态的关系 所进行的差异性模型选择。 ​ 实用论文投稿及发表技巧​ 浅谈期刊投稿与论文发表（上） 分享了如何进行目标期刊的筛选和规避投稿的雷区；浅谈期刊投稿与论文发表（下） 结合笔者自身经历讲述何为“行百里者半九十”。 复杂计算模型及概念​ 如果你对机器学习和经典的数据模型感兴趣，你可以看看机器学习数据分析极简思路及sklearn算法实践小试 实用数据挖掘​ 没有实验思路和线索怎么办？没关系，看看这些或许能给你一点思路： 基于组学数据的分子功能挖掘 骨肉瘤中发现DANCR作为ceRNA促进增殖和转移 优秀文献赏析​ “单细胞”中研究调控细胞周期起始的分子机制 深度剖析细胞周期研究的精细调控机制，明星分子非经典功能研究与组学数据挖掘的结合 紧跟热点研究方向，挖掘多组学平台对经典生物学分子的功能研究。 生物实验狗最关心的详细实验技巧CHIP-Seq经验总结 为你保驾护航，Western Blot详细攻略 让你显影的时候不再如履薄冰。 非主流的快乐运营这个公众号的小伙伴是一群对科学有兴趣的年轻人，他们不光分享干货还创造快乐，比如偶尔皮一下的编辑们，画风是这样的。。。。。。 ​ 上面的介绍如果让你心动就赶紧关注起来吧～]]></content>
      <categories>
        <category>We</category>
        <category>Photo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[浅说动力学模型（上篇）]]></title>
    <url>%2F2019%2F04%2F09%2Fdynamic-model1%2F</url>
    <content type="text"><![CDATA[原创： 赵洪龙 宇宙实验媛 插曲以前我还在中科院-马普学会计算生物学伙伴研究所的时候，常常有朋友问我诸如“会不会做DNA数据分析？”，“会不会做甲基化数据分析？”之类的生物信息学（狭义）问题。很惭愧，我没有这方面的经验。又有朋友觉得奇怪， 追问：“你不做实验，又不做生物信息学分析，那你到底是干什么的？”我是做动力学模型的。“什么模型？”基于常微分方程的动力学模型，是动力学模型中的一种。每次说到这里，对方的反应总是一愣，随后一蒙，然后一呵呵。这里，我就来简单说说动力学模型到底是个什么东西。 关于模型的简介说到模型，很多人首先就是一哆嗦，因为想到了一堆符号公式和复杂的拓扑结构图形等等。其实大可不必紧张，模型实际上就是我们对客观世界认知和归纳的一个抽象体。就像我们买房子的时候，售楼小姐姐和小哥哥给我们展示的小区规划图、等比缩放沙盘一样，这些都是模型。地产商只是把一些我们想了解的楼盘信息抽象出来，用图画或者微缩造型的形式呈现在我们面前，再或者直接用语言表达：这个楼背靠青山，那个楼面朝大海。这些模型能告诉我们哪些楼有哪些特点，让我们不用亲临现场就能明白住进去后的基本感受。同样地，科学家们说是在研究自然，实际上大部分的工作都是在观察，以及建立模型来描述研究对象，最后再基于模型的研究结果去改造这个世界。而那些带有令人望而生畏的符号和公式的数学模型与生物学中心法则的描述的差别在于，前者用的是数学语言，后者用的是自然语言。要了解数学模型，首先就要了解其基本的功能：第一，整合信息，根据所研究的问题综合现有的信息对研究对象进行系统地描述。第二，预测，通过对模型中的参数和环境条件的扰动可预测系统在宏观层面的表现和变化。第三，提出可验证的假说。 模型在生物学研究中的必要性经过过去几百年的观察和研究，人类越来越意识到生物学系统的复杂性。由于技术和资源的限制，大量的研究都是局限于特定过程的离散的研究。这些离散的研究让我们获得了对特定生命过程的了解，并建立了生物学模型。然而，在实际的生命体中，这些生命过程往往相互联系并交织成一张极为复杂的网络，很多现象并不能直观地根据已有的法则和原理所解释，也不能根据经验进行可靠地预测。即使现在我们能够轻易获得海量的组学数据，但是当面对如此复杂的网络时，我们也几乎无法对其进行人为的归纳和演绎。因此，数学建模结合计算机模拟应运而生。它们能够整合这些网络中的主要原件和内在互作过程进行模拟仿真，从而帮助我们理解复杂网络中的扰动对系统表现的影响。通过这样的方式，我们知道了，生命过程是遵循一些共性或者说是规律而自发涌现出来的一种状态。 模型构建的原则和方法数学模型包括两个基本要素：结构和参数。说白了，结构就是关系，就是所研究的问题中包含的要素（转录因子，基因，酶等等）和要素之间的相互作用与联系（如化学计量学比例，事件发生先后，调控等等）。而每一种要素的状态变化及其相互作用都需要用数学表达式来描述。建模的具体步骤如图1所示。图1. 数学建模的关键步骤下面将通过一个简单的例子来说明建模的必要性和过程（本例子也许不是最合适的，但却能够简单描述建模的过程及意义）：（1）确立研究目标：A同学研究一个酶促反应的反应特征，并希望通过构建反应速率与底物浓度之间的模型，利用模型预测来指导工程改造酶的特征，以更好地得到产物B。 S→B （2）收集信息：根据前人研究发现，该反应服从米氏动力学特征的不可逆反应。故可以用米氏动力学方程来描述这个反应过程。并且文献记载该酶的最大催化速率V1为2，米氏常数Km1为2。（3）建立模型： v=V*[S]/(Km+[S] ) 其中，[S]表示底物浓度；V表示目标酶在特定条件下的最大催化速率；Km表示米氏常数（最大反应速率一半时底物的浓度）；v表示因变量，即在底物S的浓度为[S]时对应的实际反应速率。（4）模型的参数化及验证：将V1 = 2，Km1 = 2作为参数输入模型，在所有浓度范围内，均可计算出该反应的实际反应速率（图2蓝线所示）。并且文献查找获得了一系列底物浓度条件下该反应的反应速率数据（图2中黑色实心点）。（5）模型预测并提出假设：通过改变模型中参数V和Km比较反应速率与底物浓度之间的关系，发现改变V时改变了该反应的最大催化能力，而改变Km时改变了催化过程对底物的敏感度。提出假设：在底物浓度高的反应器中需要增加该酶的量，而在反应物浓度比较低的反应器中需要改变酶对底物的敏感性！（6）用实验构建分别改变V和Km的酶，测量底物与反应速率之间的关系，如果能和预测结果吻合，说明该模型对于需要研究的目标问题具有一定的预测能力，并且用于实际的化工合成或者生物合成；否则，需要重新建模或者补充信息，直至能吻合为止。图2. 反应速率与反应物S的浓度在不同特征酶催化条件下的关系。图中的蓝线表示当前条件下模型模拟的关系趋势线，其余的线条表示模型预测改变酶特征后可能的关系；黑色实心圆点表示实验测量在当前酶特征条件下，不同底物浓度时对应的反应速率；彩色实心椭圆点表示改造后最大反应速率的一半以及对应的底物所需浓度。南京诺维赞南京诺唯赞生物科技有限公司]]></content>
      <categories>
        <category>Bio Cooking</category>
      </categories>
      <tags>
        <tag>dynamic model</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ChIP-seq实验以及分析总结(上篇)]]></title>
    <url>%2F2019%2F04%2F01%2Fchip-seq1%2F</url>
    <content type="text"><![CDATA[原创：丸子 宇宙实验媛 由于基因表达调控机制的复杂性，从不同的层面探究生物问题越来越重要，因此需要我们对多种组学数据的整合分析。从RNA-Seq层面，我们可以探究哪些基因具有显著差异，上调或下调；但是想进一步探究调控某一生物学过程的关键因子（包括顺式调控元件和转录因子），以及哪个转录因子调控了感兴趣的基因，需要结合ChIP-Seq来分析。从ChIP-Seq层面，我们可以研究某个特定转录因子的调控作用，以及调控区的组蛋白修饰等。今天，我们就来学习下ChIP-Seq的实验部分，之后的推送也会为您献上ChIP-Seq的数据分析。 概念染色质免疫共沉淀技术(Chromatin Immunoprecipitation,ChIP)是一种用于研究蛋白质DNA的体内相互作用的经典实验技术。采用特异性抗体将目的蛋白进行免疫沉淀，由此可以把目的蛋白所结合的基因组DNA片段也富集下来。通过与高通量测序技术的结合，对ChIP后的DNA产物进行测序分析，从全基因组范围内寻找目的蛋白的DNA结合位点，以高效率的测序手段得到高通量的数据结果。ENCODE数据总览 技术原理在生理状态下，把细胞内的DNA与蛋白质交联（Crosslink）后裂解细胞，分离染色体，通过超声或酶处理将染色质随机切割； 利用抗原抗体的特异性识别反应，将与目的蛋白相结合的DNA片段沉淀下来； 再通过反交联（Reverse crosslink）释放结合蛋白的DNA片段； 纯化； 测序获得DNA片段的序列，最后将这些DNA片段比对到对应的参考基因组上。ChIP实验原理 应用领域以及技术优势由于 ChIP-Seq 的数据是 DNA 测序的结果，为研究者提供了进一步深度挖掘生物信息的资源，研究者可以在以下几方面展开研究：（1）判断 DNA 链的某一特定位置会出现何种组蛋白修饰； （2）检测 RNA polymerase II 及其它反式因子在基因组上结合位点的精确定位；（3）研究组蛋白共价修饰与基因表达的关系； （4）转录因子研究。ChIP-Seq能够在全基因范围内捕获转录因子或者表观修饰标记结合的目标DNA，鉴定转录因子结合位点，揭示基因调控网络，并且适合多种多样的样本。表观遗传修饰与转录调控 实验流程（1）甲醛处理细胞，使DNA-protein的相互结合作用被交联固定（2）裂解细胞，得到全细胞的裂解液（3）超声处理或者用限制性内切酶处理，将基因组DNA打断至100-500bp（4）抗体免疫沉淀，在细胞裂解液中加入一抗和beads，进行孵育（5）采用合适的实验条件进行洗脱，并进行解交联（6）通过qPCR对ChIP结果进行验证（7）准备好的ChIP后的DNA样品用于ChIP-Seq建库ChIP实验流程 建库流程（1）DNA片段末端修复（2）3’端加A碱基（3）连接测序接头（4）PCR扩增及DNA产物片段大小选择（一般为100-300bp，包括接头序列在内）想要了解详细步骤，请参考诺唯赞公司VAHTS™ Universal DNA Library Prep Kit for Illumina® V3。VAHTS™ Universal DNA Library Prep Kit for Illumina® V3建库原理以及流程VAHTS™ Universal DNA Library Prep Kit for Illumina® V3建库&nbsp;Input DNA量最低可至100 pg，且DNA片段末端修复&amp;加A尾，一步完成。经过了严格的质量控制和功能验证，最大程度上保证了文库构建的稳定性和重复性。 注意事项 10^6~10^7 个细胞才能保证最终得到10到100ng ChIP DNA。一般10^6可以满足高丰度蛋白（如RNA polymerase II）和局部组蛋白修饰（如H3K4me3）的ChIP。如果是低丰度的转录因子蛋白和其他组蛋白修饰则需要10^7个细胞。 超声处理在含有SDS的缓冲液中可能会破坏蛋白质－蛋白质和蛋白质－DNA相互作用。但是含有SDS的缓冲液能增加超声的效率，适应与DNA紧密结合的转录因子的ChIP-Seq，如果结合较弱的话不推荐使用加SDS的缓冲液。 ChIP 样品中如含有明显的蛋白质或离子浓度过高或其它杂质污染，可能会使库检时 2100峰图异常，对建库过程中的酶反应产生影响，导致建库失败。建议在完成ChIP实验后，选择某一已知的阳性DNA结合区域设计Q-PCR实验，由此验证ChIP实验的可靠性。但此建议不适合于没有阳性对照序列的ChIP实验。 IgG通常pull down非常少的DNA，这样导致在后期的建库过程中PCR Cycles 数增加，导致不能达到作为control去除背景噪音的目的（会缺失和放大部分信息）。因此比较而言，Input更适合作为control。建议用不同公司的抗体来做生物重复，以避免抗体导致的结果差异，保证结果的准确性。参考文献1.Jothi et al. (2008) Genome-wide identification of in vivo protein–DNA binding sites from ChIP-seq data. Nucleic Acids Res 36(16) 5221–5231.2.Bernstein, BE; et al. (2005). “Genomic maps and comparative analysis of histone modifications in human and mouse”. Cell. 120: 169–181.3.Johnson, DS; Mortazavi, A; et al. (2007). “Genome-wide mapping of in vivo protein–DNA interactions”. Science. 316: 1497–1502.]]></content>
      <categories>
        <category>Bio Cooking</category>
      </categories>
      <tags>
        <tag>chip-seq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈期刊筛选与文章投稿-上篇]]></title>
    <url>%2F2019%2F03%2F31%2Fpaper-writing01%2F</url>
    <content type="text"><![CDATA[原创： 伊颜落芸 宇宙实验媛 1月3日 随着近十年生物医学科技的迅猛发展，SCI文章的在线发表量以及PubMed等主流数据库对文章的收录数呈指数型增长；尤其是伴随着几大出版集团&gt;的合并重组，开源期刊（OA）的流行，科学研究者越来越重视成果发表的时效性、深刻度和影响力。笔者也参与过多篇SCI论文发表，现就期刊筛选&gt;和论文发表的流程分享一些心得体会，由于篇幅限制，将分为两期讨论，请读者持续关注。 编者按随着近十年生物医学科技的迅猛发展，SCI文章的在线发表量以及PubMed等主流数据库对文章的收录数呈指数型增长；尤其是伴随着几大出版集团的合并重组，开源期刊（OA）的流行，科学研究者越来越重视成果发表的时效性、深刻度和影响力。笔者也参与过多篇SCI论文发表，现就期刊筛选和论文发表的流程分享一些心得体会，由于篇幅限制，将分为两期讨论，请读者持续关注。 期刊筛选在进行目标杂志的筛选和确定时，采取以下几个核心的原则：1.综合考虑期刊影响力和近５年平均影响因子：发CNS是生物医学研究者的最高目标，它们也是影响力最高的综合性期刊，无论从数据的质量、完整度，科学问题的选择和语言撰写方面都是一流的。取法乎上，得乎其中，取法乎中，得乎其下。我们追求的不是发表CNS的结果，而应该在数据呈现形式，实验设计的角度以及英文文章的撰写这三个方面尽可能高标准看齐。建议遵循小领域期刊&gt;泛领域期刊&gt;综合性期刊。例如一篇研究临床心血管致病机制和临床相关性的文章，选择层级是： JACC&gt; JCI/JEM&gt;Nature Communications/Science Advances。在这些层级范围内，方向的聚焦性逐层递减，而科学问题的接受度和广阔性逐层增加，显而易见反映到次要指标IF上也有所体现。另一个重要的指标是五年平均影响因子 受市场因素和编辑部综合决策影响，IF值在2-3年内存在波动较为常见，而五年的时间范围内，能够尽可能排除非学术因素的影响，像Elsevier旗下的Cell系列子刊和Nature旗下子刊，在相当长的周期内，IF的波动都不会太明显，而同出版集团的有些OA杂志可能有比较明显的波动，特别是显著下跌的期刊，存在着大量灌水的可能，比如Cellular Signalling, Stem Cell Reports这几年影响因子下跌，发文量也明显增多，还有已经被列入黑名单和准黑名单的scientific reports，tumor biology以及oncotarget等杂志。2.综合考虑期刊发文量和审稿周期从出版周期上看，一般期刊分为周刊，双周刊，月刊，季刊。有的杂志只有电子版没有纸质版，比如Nature Communications, Scientific Reports等大部分open access期刊，这类期刊的发文量比较多，这给了我们研究该期刊文章风格的参考性，同时也增大了被录用的概率。由于文章投稿人和in press的期刊之间供需关系不平衡，可能存在审稿周期慢的情况，通常在文章的核心论点突出，基本证据链完备的情况下就应该尽早考虑期刊的筛选和布局。因为一旦进入审稿周期，就是编辑方为主的考量，我们虽然可以发站内信催促，但是主动权还是不在我们。3.兼顾考虑期刊编辑友好度及与本研究相关度须知：建立在相当程度的文献阅读基础上，我们才能知道国内外同行的关注度和竞争性。比如研究XX蛋白质的翻译后修饰在某种肿瘤代谢疾病模型中的应用。 第一步，文献查对我们需要知道国内外研究该基因的实验室和课题组有哪些？他们对该基因蛋白质翻译后修饰的研究在哪个层面，特别是首次报道的某种修饰及其调控模式的团队或相关成员，例如近年来关注较多的细胞能量代谢与乙酰化修饰，现在研究系统性较高、占比量较大的工作基本来自于Choudhary、Matthias 团队和华人教授管坤良、熊跃团队，既有蛋白质组学层面的全谱研究【1,2】，又有单基因层面的机制功能研究【3-5】。知悉这些，我们才能大致知道需要关注哪些学者，其基本观点和研究出发点等, 以其投稿结果和期刊类型为参照制定我们的投稿目标；第二步，前审稿阶段 类似各种基金的小同行评审，最好能给了解你工作的相关教授审阅并提出文章架构和数据完整度方面的意见，这是十分必要的，毕竟在实验研究中我们要保持思维方式的独立性，而在投稿过程中需要依赖于导师的学术判断和投稿经验。这一方面取决于之前成功发表的期刊编辑团队是否青睐于本团队所在方向的工作，另一方面取决于拟投稿文章和该期刊近期相关研究方向的工作是否具有研究点的深入和研究面的开发，并在投稿内容中突出和强化这一点；第三步，热点分析 利用Web of Science 或者 基于perl的网络爬虫方法，可以对相关领域内的发文量，以及资助情况进行分析总结。以热点的研究方向Crispr-Cas9介导的基因编辑为例。可见资助情况和发文情况在相关领域的关注度逐年提升。提示我们在投稿杂志选择方面，近3-5年侧重交叉热点整合的文章可能会另编辑更感兴趣。4.兼顾考虑版面费与是否为开源期刊考虑到科研经费的有限性和时效性，在保证发表期刊质量的前提下，版面费也必须具有高性价比。通常来讲，高版面费的杂志很多是开源期刊，相关文章水平参差不齐的几率更大，所以我们必须综合考虑这些因素并珍惜每个目标期刊的初次投稿机会。 参考文献： Choudhary, C. et al. Lysine acetylation targets protein complexes and co-regulates major cellular functions. Science 325, 834-840 (2009). Zhao, S. et al. Regulation of cellular metabolism by protein lysine acetylation. Science327, 1000-1004 (2010). Zhao, D. et al. Lysine-5 acetylation negatively regulates lactate dehydrogenase A and is decreased in pancreatic cancer. Cancer cell 23, 464-476 (2013). Yang, H.B. et al. Acetylation of MAT IIalpha represses tumour cell growth and is decreased in human hepatocellular cancer. Nature communications 6, 6973 (2015).5.Saito, M. et al. Acetylation of intrinsically disordered regions regulates phase separation. Nature chemical biology 15, 51-61 (2019).这期的总结就到这里，欢迎各位关注者们的分享和讨论。这期重在理论经验，下次将着重就投稿细节展开讨论。]]></content>
      <categories>
        <category>Paper Writing</category>
      </categories>
      <tags>
        <tag>paper writing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[师兄又在胡扯了-Western Blot]]></title>
    <url>%2F2019%2F03%2F31%2Fwestern-blot%2F</url>
    <content type="text"><![CDATA[大家好，阿呆师兄又出现了。不知道上个月说的几本书有几个人看了或准备看的。其实原本我应该深藏功与名，大隐隐于市了，不知道大家还记不记得短腿师妹前阵子说“宇宙实验媛喊你来拿三位数稿费了”阿呆 宇宙实验媛大家好，阿呆师兄又出现了。不知道上个月说的几本书有几个人看了或准备看的。其实原本我应该深藏功与名，大隐隐于市了，不知道大家还记不记得短腿师妹前阵子说“宇宙实验媛喊你来拿三位数稿费了”一开始在考虑要不要投稿的时候，其实我是拒绝的。因为，你不能让我投，我就马上去投，第一我要酝酿一下，因为我不愿意投完了以后再加一些特技上去，稿子“咣”一下，很亮、很炫，这样观众出来一定会骂我，根本没有内容的稿子，就证明是个假的。后来发生了一件惨绝人寰的事情……是的，我的肾6摔了个脸着地。当时第一反应就是，啊！又是一笔开销啊！！手里没粮，心里很慌！于是我就来水了……除了短腿师妹变成了短腿老板外，其他也没什么嘛……（凑表脸）可是写啥呢，大家知道短腿他们偏干实验，而我是个纯种湿实验员，没有干货只会浑水摸鱼……直到有一次和师妹聊天提到Western Blot（WB）。好吧我就来试试看写这个，虽然我觉得可能没有观众愿意看这些……毕竟作为一个生物狗，不可能不知道WB啊！我打开了某度，输入“Western Blot”去掉最上面的广告，大家可以看到，确实是随便找找一大把，这还需要我写？就随便点开那个“这一篇就足够了!Western blot详细步骤与经验交流”看下。Emmm……写得蛮清楚的嘛……（唉咦，谁扔的番茄？）好吧好吧，如果你跟着我点进去看了，会发现这个网页里的内容确实基础地描述了WB的过程。但我知道各位看官一定是不会满足于这些。来来反正你都打开网页了，不如点到某度文库里去，请输入“Western_Blot详解及问题分析”看见那个53页的PPT没？没有下载券就和我一样在线看吧，把它仔细看完，WB的理论知识可以说已经掌握了。如果你是个WB小白，现在已经升级成WB王语嫣；如果已经有点经验了，那现在有没有一种通透了的感觉。然而上手做起来依然会有各种问题。很正常啊，不然怎么会有那么多WB技术求教贴、经验分享贴一直在冒出来？做实验不会不知道丁香园论坛的吧，emmm…对对就是那个临床的丁香园。去丁香园论坛搜索“Western Blot”我只是随便截图的，各种技术问题几乎都能在上面找到，毕竟你不是第一个吃螃蟹的人。当新手光环还在的时候，一般都会觉得WB果然是个基础实验。当光环褪去的时候，就知道自己连个基础都没有。当然了，有些人可能光环的时间比较长，有些人可能……没有光环……不记得谁说的选择比努力重要，在WB里真是体现的淋漓尽致。上样前你得选合适浓度与孔数的胶，（取决于目标蛋白大小和上样体积）；run起来的时间不能太长也不能太短（还是大小）；转起来除了选时间，先要选择转膜方式和膜的种类（蛋白大小、结果显影还是扫描）；封闭你是选奶粉还是BSA（搞不搞磷酸化）；孵个一抗简直就是WB的灵魂啊没选好全都白辛苦（这个靠try啊）；准备化学发光呢还是荧光……哪一条没选好，努力产生的结果大概率是垃圾（戾气了，要“inner peace”……呼）。如果对WB有点情怀，可以看看丁香园的那篇帖子“【建议】穷人的劳斯莱斯-我的五年western blot体会”老人大概会有点感触吧。好了，就不多说了，字数凑够了，拿去换点米……授人以鱼不如授人以渔嘛，是吧…（还想要鱼？没看见前面写的么，跟我这个浑水摸鱼的人要鱼？）好吧，来个太长不看版吧：去看文库的ppt，不会的朋友当科普，会点的朋友当详细复习，万一醍醐灌顶了呢。动手问题去丁香园，生物秀啥的也行，平时多看看抗体说明书也会有惊喜。反正我就是这么过来的……想苦练WB技艺的朋友可能会对此篇不满吧，哎朋友啊你是不知道，其实我是在帮你啊……贴一个全自动WB仪器的视频若干年后：“在岗位上竞争不过机器是种怎样啊体验”……瑟瑟发抖.jpg（所有图片均来源于网页截图或自己手机截图）]]></content>
      <categories>
        <category>Bio Cooking</category>
      </categories>
      <tags>
        <tag>western blot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习数据分析极简思路及sklearn算法小试]]></title>
    <url>%2F2019%2F03%2F27%2FMachine-learnig-KFC%2F</url>
    <content type="text"><![CDATA[原创：老板，来一打TPU 机器学习拥有庞大的知识体系，这里对机器学习的数据分析的整个思路和流程作最最简单的归纳。机器学习的步骤大致包括：1）理解和清理数据2）特征选择3）算法建模4）测试评估模型 机器学习数据分析极简思路1）理解和清理数据 理解数据数据是机器学习大餐的原始食材，对数据分析起着至关重要的作用，理解原始数据的含义将有助于进一步分析。例如，甲基化图谱与年龄有着显著的相关性，而与性别关系不大，因此在数据分析中，对这两个特征（faeature）需要区别对待。更好的理解方式是直接可视化某些数据，例如对于经典的鸢尾花数据集，可以通过python seaborn绘图包可视化各个特征（feature）之间的关系；对于大数据，则可以进行降维分析（PCOA、tSNE），理解数据组成主成分贡献度。 #pip install seaborn import matplotlib.pyplot as pyplot import seaborn as sb import pandas as pd %matplotlib inline data=pd.read_csv('iris.csv') #pandas 读入数据 data.head(3) #查看数据 data.describe() #数据基本统计 sb.pairplot(data.dropna(),hue='Species') 鸢尾花数据组成鸢尾花数据所有feature基本数理统计鸢尾花数据不同feature相互关系 剔除异常值清理数据的目的在于去除原始数据中的异常值和想办法处理缺失值，我们拿到手上的数据不可能尽善尽美，总有一些妖孽作祟，对于异常值我们应当剔除。举个栗子，假设在鸢尾花数据集中，有一个样本显示鸢尾花花瓣长度10m，其他诸如花瓣宽度、花萼长宽值都正常，可以脑补一下这是一朵什么样的花，那么这个样本显然应该剔除。 处理缺失值缺失值在数据分析中很常见，总有一些样本观测值会因为这那的问题缺失，处理缺失值如果样本数量很大，而包含缺失值的样本又少，这个时候果断去掉这些样本，眼不见为净；如果因为样本有限或者缺失值太多，就要想办法补全缺失值（imputation）,常常利用逻辑回归建立模型，找出这些数据变化的规律，从而预测缺失值。如何合理推断和填回这些缺失值是一门大学问，哪种方法好，我也不敢妄言。 2）特征选择工业界广泛流传的一句话是：数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。由此可见，特征选择是机器学习的关键的关键。特征选择目的在于提取跟目标最为有效的信息，降低数据维度和计算成本，同时防止过拟合（overfitting，训练集特征用的太多太细，以致于在测试集中不适）。要知道，并不是特征越多，结果越好，没有严格意义上的特征累加效应，有时候好的几个特征胜于大量零碎的特征取得的效果。在许多大数据挖掘竞赛中（国内的阿里天池和国外的kaggle平台），最复杂的过程莫过于特征工程建立阶段，大概占据了整个竞赛过程的70%的时间和精力，最终建立的模型的好坏大多也取决于特征工程建立的好坏。遗憾的是，特征工程不像模型建立的过程有着固定的套路，特征工程的建立凭借的更多的是经验，因此没有统一的方法。这里抛砖引玉介绍一些常见的办法，更为详细的内容请参考文后链接。a)特征过滤法比较简单，它按照特征的发散性或者相关性指标对各个特征进行评分，设定评分阈值或者待选择阈值的个数，选择合适特征。例如，我们可以简单的计算出每个feature的方差，方差越大说明这个feature在样本中变异大，即有区分性；而越小的（极端时方差为0），即表示在所有样本中一样，特征选择时则可不考虑这些特征。我们可以选择方差最大的前n个feature用于建模，这就是最为简单的方差筛选法。b)包装法根据目标函数，通常是预测效果评分，每次选择部分特征，或者排除部分特征。c)嵌入法则稍微复杂一点，它先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据权值系数从大到小来选择特征。类似于过滤法，但是它是通过机器学习训练来确定特征的优劣，而不是直接从特征的一些统计学指标来确定特征的优劣。踩雷说：特征选择之后，需要从原始数据矩阵提取相应的特征重构矩阵，那么训练集和测试集的特征经过你各种变换之后，应当保持一一对应，类别和顺序在矩阵中都应该一致。目前，已经有一些套路化的特征选择工具，例如python的FeatureSlector包，见链接。 3）算法建模针对具体的问题，是分类问题？回归问题？还是其他？选择合适的模型，或者使用集成的算法模型。常见的算法模型包括：对于回归问题：a)线性回归（回归，LinearRegression）b)岭回归（回归，Ridge）Ridge是线性回归加L2正则平方，以防止过拟合c)拉索回归（Lasso），加入惩罚函数L1正则绝对值，防止过拟合d)弹性网络回归（回归），同时使用L1和L2正则。e)K近邻（回归和分类，KNeighborsRegressor）f)决策树（回归和分类，DecisionTreeRegressor）g)支持向量机（回归和分类，SVR） 对于分类问题：a)支持向量机（回归和分类，SVC）b决策树（回归与分类,DecisionTreeClassifier）c)逻辑回归（分类,LogisticRegression)d)LDA线性判别分析（分类,LinearDiscriminantAnalysis)e)K近邻（分类,KNeighborsClassifier)值得一提的是无论是分类还是回归问题，基于决策树和SVM的算法都有比较好的表现。 4）测试评估模型测试评估模型的目的在于，解决模型的欠拟合（under-fitting）和过拟合（over-fitting）问题，通过即时的反馈不断调整模型、优化模型，使得模型更加稳健。实际上，测试评估模型应该在你建模之前就考虑，例如是否需要设置纯粹的外部数据验证集，若没有这样的数据，你怎样划分数据进行建模预测？在实际训练中，模型通常对训练数据好，但是对训练数据之外的数据拟合程度差。用于评价模型的泛化能力（即模型普适性）。交叉验证的基本思想是把在某种意义下将原始数据进行分组,一部分做为训练集(train set),另一部分做为验证集(validation set or test set),首先用训练集对模型进行训练,再利用验证集来测试模型的泛化误差。另外，现实中数据总是有限的，为了对数据形成重用，比较常用的是k-fold交叉验证法。测试评估模型的时候，常常结合AUC曲线判断模型好坏。 sklearn 算法小试实现目的sklearn是python中一个强大的机器学习模块,拥有众多的机器学习算法和功能。这里，通过sklearn的datasets构建一个数据集，并用4种常用算法：逻辑回归（LogisticRegression）、支持向量机（SVM）、决策树（DecisionTree）和集成算法（VotingClassifier）对训练集建模，然后对测试集预测，最终通过得分看一下4种算法的差异。 步骤1）构建本次使用的数据集2）将数据拆分成训练集和测试集3）用4种算法分别建模、预测 代码环境python版本：python31）如果不想被python各种安装包困扰，推荐Jupyter在线python，Jupyter官网，点击”Try Jupyter with Python”，点击“+”号即可。安装包的时候直接pip install packages_name,例如pip install sklearn,点击“Run”，提示“Successfully installed sklearn-0.0”即安装完成。2）Pycharm，专业、高效、强大的python开发端。 源码 import numpy as np import matplotlib.pyplot as plt from sklearn import datasets #built-in datasets #make_moons,generated datasets X,y = datasets.make_moons(n_samples=500,noise=0.3,random_state=42) plt.scatter(X[y==0,0],X[y==0,1]) plt.scatter(X[y==1,0],X[y==1,1]) plt.show() #plot for datasets #split datasets for train and test part from sklearn.model_selection import train_test_split X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42) #1.logistic regression model from sklearn.linear_model import LogisticRegression log_clf=LogisticRegression() #create LR classifer log_clf.fit(X_train,y_train) #train and fit the model log_score=log_clf.score(X_test,y_test) #test the model #2.svm model from sklearn.svm import SVC svm_clf=SVC() svm_clf.fit(X_train,y_train) svm_score=svm_clf.score(X_test,y_test) #3.decision tree model from sklearn.tree import DecisionTreeClassifier dt_clf=DecisionTreeClassifier() dt_clf.fit(X_train,y_train) dt_score=dt_clf.score(X_test,y_test) #4.ensemble method from sklearn.ensemble import VotingClassifier voting_clf=VotingClassifier(estimators= [('log_clf',LogisticRegression()), ('svm_clf',SVC()), ('dt_clf',DecisionTreeClassifier()) ],voting="hard") voting_clf.fit(X_train,y_train) voting_score=voting_clf.score(X_test,y_test) print("log score:%s"%log_score) print("svm score:%s"%svm_score) print("dt score:%s"%dt_score) print("voting score:%s"%voting_score) 构建数据集：4种算法预测结果：可以看到，单一算法SVM比较好，集成算法较单一的算法还是有一定的提高。 参考机器学习中，有哪些特征选择的工程方法？FeatureSlector:一个可以进行机器学习特征选择的python工具机器学习中的交叉验证]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UBGBs]]></title>
    <url>%2F2019%2F03%2F20%2FUBGBs%2F</url>
    <content type="text"><![CDATA[Yeap,Family!]]></content>
      <categories>
        <category>Photo</category>
      </categories>
      <tags>
        <tag>photo</tag>
        <tag>family</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈科研论文发表与投稿-下篇]]></title>
    <url>%2F2019%2F03%2F19%2Fpaper-submit-experience%2F</url>
    <content type="text"><![CDATA[原创： 伊颜落芸 宇宙实验媛生物医学科学研究的过程犹如马拉松长跑，投稿的经历更是具有西天取经般的朝圣感。上线发表的文章只是最后的工作总结之一，背后的故事&gt;往往更精彩。 上篇我们已经就期刊筛选分享了初步经验，本期将以发表在Nature Communications中题为Znhit1 controls intestinal stem cell maintenance by regulating H2A.Z incorporation的工作为例，就论文投稿和回修过程论述心得体会，便于读者熟悉投稿的关键环节并规避常见误区。 Nature Communications 杂志作为NPG旗下开源期刊，近几年不光收录文章数量质量上升，交互性功能也日趋开放。值得一提的是，大多数文章上传了同行评审（peer review process）的记录，使读者在欣赏高水平研究工作的同时，也能全面了解到文章的逻辑性提升过程和实验证据完善的过程。 这篇文章讲述的是SRCAP染色质重塑复合体的重要组分—锌指蛋白Znhit1，通过促进组蛋白变异体H2A.Z整合到和Lgr5+肠道上皮细胞命运决定相关基因的TSS区域，增强其分子伴侣YL1的相互作用和磷酸化，进而维持Lgr5+细胞的干性稳态并促进小肠上皮平衡的故事。结合peer review file。我们凝练出以下五个投稿过程要点： 1. 初次投稿注意事项 首先要根据拟投稿的期刊，认真阅读投稿指南，确认正文材料和补充材料在形式和内容上符合杂志社的要求。如果能在初次投稿时候准备完善，将给编辑和审稿人工作可信度高和完成度高的良好印象。即使是同一个出版社的不同子刊也可能在这些方面存在很大差异。以NPG集团为例：主刊和经典子刊，uncropped scans 和 reportingsummary （图1）应该在数据的整理阶段完成，后者的统计学分析方法，原始生物信息的溯源，和其他重要实验方法的陈述亦可辅助文章的撰稿过程。这些细节也是审稿人评价本工作严谨性和完成度的重要层面，因此不可掉以轻心。该集团其余代表性刊物ncogene、Cell Death andDsease虽然要求不同，但是如能秉承“取法乎上”的原则一定能得到好的效果。 其次的重点就是Cover Letter了，好的CV应该观点突出，逻辑清楚。通常都是体现“发现了什么”，“几大创新点”这样的逻辑思路，当然也可根据投稿杂志的领域类型和刊物亚类进行相应调整。如果一份cover letter不能吸引到审稿人，就像招聘时简历不能得到HR的青睐，由此摘要的部分可能也不够凝练突出。总结起来这两个部分就是功夫在平时，投稿再加强。 2. 如何评价初次回修审稿意见 从这篇文章的投稿历程看，初次投稿时间2017.07.07，收稿日期为2019.02.15。虽然不能轻易评价该工作在初次投稿时与杂志社的投稿标准间的差距和差异，但从审稿人提出的问题来看，原始版本还是存在很多细节问题和可提升空间。可以总结为以下几个层面：a. 检测指标：审稿人认为需要关注Znhit1在小肠上皮中的表达模式，才能更好的确认Znhit1对肠稳态的调控是依赖对小肠上皮干细胞的调节实现，而非通过goblet细胞或肠内分泌细胞；b. 评价指标：审稿人认为单独通过Lgr5+ISC的缺失不足以评价小肠上皮的稳态失衡，毕竟还有+4 Bmi1+ 细胞、Dll1+分泌前体细胞、Alpi+肠上皮前体细胞发挥的代偿效应；c. 模型选择：审稿人认为经典的表征Lgr5+的小肠干细胞所选用的工具鼠模型还应包括Lgr5-EGFP-IRES-creERT，作者选择的模型Olfm4-CreER似乎并不是一个最佳的标记指标。d. 开放问题:审稿人认为作者提到的Znhit1调控yl-1磷酸化的机制不足从机制上揭示Znhit1对Lgr5+干细胞的调控，需要提供更多的修饰启动者，修饰位点，修饰模式和结合区域等的详细信息。虽然不同的文章研究内容迥异，但是审稿人提出问题的架构有迹可循。这篇文章在初次投稿时获得了3个审稿人的评审意见，其中a.c.d三点在多个审稿人处提及，结合这篇文章的审稿意见和笔者自身的投稿经历，可总结为：共性问题着重回答，开放问题难点攻破，合理质疑引用文献，数据highlight，remark中关注态度。 3. 写好回修信 在对审稿意见进行客观全面的分析+针对性的实验补充后，回修信的撰写是对文章调整进步的总结。核心原则有四：1) 点对点 作者对所有调整的图片内容进行了详细的梳理和总结（图2）；2) 全面性 不论该问题出现在审稿人的综合评述中还是单独的major/minor concerns，建议均进行必要的回复；3) 不卑不亢 审稿人提出的质疑及其佐证的参考文献，如能丰富文章架构和纠正偏颇，应该加以肯定；如果原有数据与提出质疑一致，也可以rebuttal，但前期是基于对所指文献和观点的充分了解和对观点原创者的深入认知，否则还是不要班门弄斧；4) 忌画蛇添足 这一点在回答开放问题时尤其注意。 4. 多轮回修的心理和技术储备 文章一旦踏上投稿过程，便是开弓没有回头箭之感，多轮回修更是司空见惯。在这个过程中需要积极的寻求相关领域专家专业意见和导师的讨论指点，寻求合作来弥补技术的短板；需要补实验小分队相互沟通缓解疲乏和压力。或许成为一个杰出的科学研究工作者，最重要的是心态，其次才是技术，一篇篇的文章是提升和总结也是修行，切不可偏执。 5. 正式发表前的材料准备 编辑部的主体接受或接收函到手并不能马上庆祝最后的胜利，还需按照要求重点准备各项材料。原始图片：尤其在分辨率和图片的格式要求上进行二次审核；统计学方法的选择，代表性图片的呈现是否符合投稿要求；文字的表示和单词拼写是否有错误。特别注意：图片的信息标注要反复检查一遍，这是最容易出现疏漏的环节。 最后祝大家都能享受充实的投稿过程！！科研论文写作与投稿-上篇]]></content>
      <categories>
        <category>Paper Writing</category>
      </categories>
      <tags>
        <tag>paper writing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Date with Bioawk]]></title>
    <url>%2F2019%2F03%2F16%2FDate-with-Bioawk%2F</url>
    <content type="text"><![CDATA[原创：老板，来一打TPU 废话这是生平第一篇blog，因此难免要说点废话，所以前戏，啊不对，前言就叫废话吧。git+hexo搭建的平台杠杠的，感谢师弟MingLian小大佬成功带入写blog的坑。第一篇就拿生信李恒大神的神作bioawk镇楼吧！** bioawk简介除了鼎鼎大名的BWA、samtools等，李恒大神应群众呼声又做了出于awk又胜于awk的bioawk供广大生信猿们玩耍。bioawk是用C写的，用法和awk基本一样。 安装分步安装 $ sudo apt-get install byacc git #安装git $ git clone git://github.com/lh3/bioawk.git #默认安装在当前目录 git clone git://github.com/lh3/bioawk.git ${path}/bioawk #若加path则必须为空，因此可在想要安装的目录下起一个新名称，git会clone到其下 $ cd bioawk $ make $ echo "export PATH=${path}/bioawk/:\${PATH}" >> ~/.bashrc $ . ~/.bashrc ### 一步安装脚本 #!/bin/bash read -p 'Input installed path: ' tmp_path path=${tmp_path/\//} if [ ! -d "${path}" ];then echo "No such file:${path}" &amp;&amp; exit ;fi git clone git://github.com/lh3/bioawk.git ${path}/bioawk &amp;&amp; \ cd ${path}/bioawk &amp;&amp; make &amp;&amp; \ echo "export PATH=${path}/bioawk/:\${PATH}" >> ~/.bashrc &amp;&amp; . ~/.bashrc &amp;&amp; \ echo 'Successfully install bioawk!' || echo 'Failed install bioawk' 使用bioawk基本思想是把组成不同类型的文件（sam、bam、fasta、fastq、vcf etc）的基本元素封装成变量，直接调用即可。 语法usage: bioawk [-F fs] [-v var=value] [-c fmt] [-tH] [-f progfile | 'prog'] [file ...] -c 支持输入文件格式，查看帮助： bioawk -c -h bed: 1:chrom 2:start 3:end 4:name 5:score 6:strand 7:thickstart 8:thickend 9:rgb 10:blockcount 11:blocksizes 12:blockstarts sam: 1:qname 2:flag 3:rname 4:pos 5:mapq 6:cigar 7:rnext 8:pnext 9:tlen 10:seq 11:qual vcf: 1:chrom 2:pos 3:id 4:ref 5:alt 6:qual 7:filter 8:info gff: 1:seqname 2:source 3:feature 4:start 5:end 6:score 7:strand 8:frame 9:attribute fastx: 1:name 2:seq 3:qual 4:comment 上面出现的名称即可引用其变量 实例 打印fasta序列ID、序列、长度、GC含量 bioawk -cfastx '{print "ID: "$name"\tlength: "length($seq)"\tGC: "gc($seq)"\t"$seq}' demo.fa 结果: ID: g1 length: 18 GC: 0.722222 atcccccccccccccttt ID: g2 length: 26 GC: 0.0769231 cattatatcttatttttttttttttt ID: g3 length: 48 GC: 0.229167 acccccccccctttttttttttttcatttttttttttttttttttttt原始文件： g1 atccccccccccccctttg2 enzyme cattatatcttattttttttttttttg3 accccccccccttttttttttttt catttttttttttttttttttttt注意：可以看到bioawk在提取ID的时候只提取了空格分隔后的第一个字段，默认输出域分割符是\t 输出反向互补序列 bioawk -cfastx '{print $name,$seq,revcomp($seq)}' demo.fa 结果： g1 atcccccccccccccttt aaagggggggggggggat g2 cattatatcttatttttttttttttt aaaaaaaaaaaaaataagatataatg g3 acccccccccctttttttttttttcatttttttttttttttttttttt aaaaaaaaaaaaaaaaaaaaaatgaaaaaaaaaaaaaggggggggggt 根据序列ID提取序列，这个是最爱的功能 bioawk -cfastx 'BEGIN{while((getline k &lt;"id.txt")>0)i[k]=1}{if(i[$name])print ">"$name"\n"$seq}' demo.fa id.txt: g1 g2 g2 enzyme g3 结果： &gt;g1 atcccccccccccccttt &gt;g2 cattatatcttatttttttttttttt注意：id.txt 需要去掉&gt; 当然还有其他的花样，大家自己去探索吧 Referencehttps://github.com/lh3/bioawk]]></content>
      <categories>
        <category>Software</category>
      </categories>
      <tags>
        <tag>bioinformatics</tag>
        <tag>bioawk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MARKDOWN's KFC]]></title>
    <url>%2F2019%2F03%2F16%2FMARKDOWN-s-KFC%2F</url>
    <content type="text"><![CDATA[Markdown 是一种轻量级的「标记语言」，创始人为约翰·格鲁伯，用简洁的语法代替排版，目前被越来越多的知识工作者、写作爱好者、程序员或研究员广泛使用。其常用的标记符号不超过十个，相对于更为复杂的 HTML 标记语言来说，Markdown 十分的轻量，学习成本也不需要太多，且一旦熟&gt;悉这种语法规则，会有沉浸式编辑的效果。 MARKDOWN使用Markdown 是什么？Markdown 是一种轻量级的「标记语言」，创始人为约翰·格鲁伯，用简洁的语法代替排版，目前被越来越多的知识工作者、写作爱好者、程序员或研究员广泛使用。其常用的标记符号不超过十个，相对于更为复杂的 HTML 标记语言来说，Markdown 十分的轻量，学习成本也不需要太多，且一旦熟悉这种语法规则，会有沉浸式编辑的效果。另外，印象笔记 Markdown 支持 CommonMark 和 GFM (GitHub Flavored Markdown) 标准。 印象笔记里 Markdown 有什么特点？ 一键创建：支持 Markdown 独立的一键新建入口，为深度 Markdown 用户提供更好的效率体验； 支持丰富的主流 Markdown 语法：支持文字相关样式、序号列表、任务列表、表格、TOC 目录、多种图表、数学公式、流程图、时序图、甘特图等； 支持插入图片：可插入网络图片 或 直接拖动本地图片、复制剪贴板中的图片到 Markdown 笔记中； 支持多种模式切换：编辑与预览模式、纯编辑模式以及纯预览模式； 支持多种编辑主题：预置了白色、黑色、深空灰和印象绿主题，默认为印象绿，未来会有更多主题提供； 跨平台同步：创建的 Markdown 笔记可在登录了印象笔记帐户的各端查看，未来更多端会支持创建和编辑 Markdown 笔记； 演示模式：Markdown 笔记支持演示模式查看； 支持其他印象笔记特点功能：笔记标注、导出 PDF、设置提醒、工作群聊共享-查看&amp;编辑笔记等。 如何创建 Markdown 笔记？ 点击左上角「新建 Markdown 笔记」来创建新的 Markdown 笔记 点击顶部菜单栏-文件-新建Markdown笔记 使用快捷键 CMD+D 来快速创建 Markdown 笔记 印象笔记 Markdown 笔记支持哪些语法？—— 以下语法均支持在编辑工具栏直接操作 —— 设置分级标题语法示例： 一级标题二级标题三级标题四级标题五级标题六级标题 加粗文本语法示例：印象笔记 斜体语法示例：印象笔记 下划线语法示例：印象笔记 删除线语法示例：印象笔记不支持Markdown 添加分隔线语法示例： 引用文本语法示例： 近日，印象笔记宣布完成重组。作为Evernote已在中国独立运营近6年的品牌，印象笔记将成为由中方控股的中美合资独立运营实体，并获得红杉宽带跨境数字产业基金首轮数亿元人民币投资。 添加符号列表或者数字列表语法示例：使用 iOS 版本印象笔记如何快速保存内容？ 启用印象笔记 Widget ——印象笔记·剪贴板 复制粘贴任意内容 微信 滑动到 Widget 插件区域即可完成保存印象笔记·剪贴板有什么特点？ 快：开启自动模式，可以自动保存剪贴板的任意内容 一切：只要可以复制粘贴就可以保存 有序：全部保存在「我的剪贴板」笔记本并以时间来命名 添加待办事项语法示例：三只青蛙 第一只青蛙 第二只青蛙 第三只青蛙 插入链接语法示例：印象笔记官网 插入图片印象笔记支持嵌入网络图片或者直接拖入本地图片，其中本地图片格式支持 jpg、png 和 gif。语法示例： 另外，针对插入的本地图片可以控制图片大小，在拖拽、拷贝或者点击插入本地图片之后，直接在图片名称后面（无需空格）添加以下语法均可以按照以下要求控制图片大小： @w=300 @h=150 @w=200h=100 @h=100w=200示例笔记782d277a1dbc7dea8480267cf5f87ebd.png@w=300 插入表格语法示例：| 帐户类型 | 免费帐户 | 标准帐户 | 高级帐户 || — | — | — | — || 帐户流量 | 60M | 1GB | 10GB || 设备数目 | 2台 | 无限制 | 无限制 || 当前价格 | 免费 | ￥8.17/月 | ￥12.33/月| 插入图表目前支持饼状图、折线图、柱状图和条形图，只需将 type 改为对应的pie、line、column 和 bar。 ,预算,收入,花费,债务 June,5000,8000,4000,6000 July,3000,1000,4000,3000 Aug,5000,7000,6000,3000 Sep,7000,2000,3000,1000 Oct,6000,5000,4000,2000 Nov,4000,3000,5000, type: pie title: 每月收益 x.title: Amount y.title: Month y.suffix: $ 插入行内代码或代码块印象笔记 Markdown 语法支持几十种编程语言的高亮的显示。语法示例： #!/usr/bin/python import re line = "Cats are smarter than dogs" matchObj = re.match( r'(.*) are (.*?) .*', line, re.M|re.I) if matchObj: print "matchObj.group() : ", matchObj.group() print "matchObj.group(1) : ", matchObj.group(1) print "matchObj.group(2) : ", matchObj.group(2) else: print "No match!!" 插入数学公式印象笔记 Markdown 支持绝大多数的 LaTeX 数学公式语法示例： e^{i\pi} + 1 = 0 更多数学公式的输入可以参考： https://khan.github.io/KaTeX/docs/supported.html 插入流程图语法示例： graph TD A[模块A] -->|A1| B(模块B) B --> C{判断条件C} C -->|条件C1| D[模块D] C -->|条件C2| E[模块E] C -->|条件C3| F[模块F] 插入时序图语法示例：sequenceDiagram A->>B: 是否已收到消息？ B-->>A: 已收到消息 插入甘特图语法示例： gantt title 甘特图 dateFormat YYYY-MM-DD section 项目A 任务1 :a1, 2018-06-06, 30d 任务2 :after a1 , 20d section 项目B 任务3 :2018-06-12 , 12d 任务4 : 24d 设置目录设置之后可以自动根据设置的分级标题来自动生成目录。语法示例：[TOC] FBI waring本篇文档完全搬运自markdown官方中文文档，仅供测试网页使用，如侵删。https://list.yinxiang.com/markdown/eef42447-db3f-48ee-827b-1bb34c03eb83.php]]></content>
      <categories>
        <category>Software</category>
      </categories>
      <tags>
        <tag>markdown usage</tag>
      </tags>
  </entry>
</search>
