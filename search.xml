<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[浅谈文献综述二三事(上篇)]]></title>
    <url>%2F2019%2F05%2F22%2Fliterature-summary1%2F</url>
    <content type="text"><![CDATA[浅谈文献综述二三事(上篇) 大家好，阿白师姐第一次上路，还请多多关照。前一段时间刚写完 academic proposal ，老板说，哎呀，你这文字功底不太好啊，那你再写一篇 review 给我瞅瞅吧。就这样，我才有机会和大家谈一下文献综述两三事。ok，言归正传。。。 什么是文献综述？ 文献综述（Literature Review）是科研论文中重要的文体之一。它以作者对各种大量相关的文献资料的整理、归纳、分析和比较为基础，就某个专题的历史背景、前人工作、研究现状、 争论的焦点及发展前景等方面进行综合、总结和评论。文献综述通常发表在专业期刊的Review栏目中，常见的形式有Survey, Advances, Progress, Recent Advances, Update和Annual Review等。综述的种类• 按内容划分：动态性综述、成就性综述和争鸣性综述• 按时间划分：回顾性综述和前瞻性综述 为什么要写文献综述？ 文献综述的作用（1）通过阅读文献综述这一方法，读者可花较少的时间来获得较多的有关某一专题系统而具体的信息，了解其研究现状、存在的问题和未来的发展方向。（2）回顾相关文献，确立研究论题，再提出进一步的研究，从而建立一个研究项目。 因而，文献综述也常常是撰写研究计划、确立原创性研究问题的基础和起点。 研究生写文献综述的必要性（1）通过搜集文献资料，熟悉文献的查找方法和资料信息的积累方法；在查找的过程中同时也扩大了知识面；（2）查找文献资料、写文献综述是进行科研的第一步， 因此学习文献综述的撰写也是为今后科研活动打基础的过程；（3）通过综述的写作过程，能提高归纳、分析、综合 能力，有利于独立工作能力和科研能力的提高。（4）针对刚入门者可快速整理，梳清研究课题的研究背景和研究方向。 写作前的准备 综述写作的一般步骤（1）确定文献综述的目的 论文投稿or选题报告or练一下手？（不要错过Deadline）（2）检索和阅读文献资料，全面、客观地收集文献，并在阅读文献的基础上进一 步筛选、分类、归纳和整理（3）确定研究选题，综合分析前辈们研究的贡献、结合不同观点以及研究的不足， 提炼出问题的焦点，确定自己要研究和解决的问题（4）建立论证方案，拟定写作框架 （逻辑，结构，小标题）（5）论文撰写（6）修订 如何检索和阅读文献 首先，快速浏览文献摘要和文内结论，筛选出与自己研究有关的文献。 通过阅读，详细、系统地摘要出文献中研究的问题、目标、 视角、方法、结果和结论，以及简要评述该文献研究的不 足与尚未解决的问题。 然后将文献根据所要研究的问题按照研究领域（学科）、 研究方法、研究视角、研究的问题等进行分类整理。 检索文献的方法（1）通过各种检索工具和搜索引擎进行检索，例如 Web of Science，Google Scholar，Scopus，Medline， ResearchGate，Sci-hub，百度学术……（2）从文献中找文献，顺藤摸瓜，再次检索综述性文章、专著、教科书、博士论文等的参考文献目录 。 文献检索和阅读时注意： （1）收集到的文献要具有全面性、代表性、科学性和可靠性（2）瞄准主流文献，如该领域的核心期刊、经典著作、专职部门的研究 报告、重要的观点和论述等；（3）兼顾广度与深度，粗读和精读，高度相关的文献可能要反复阅读；（4）随时整理，如对文献进行分类，记录文献信息，分文件夹存档。 参考文献的管理 使用参考文献管理软件的好处：（1） 便于在线检索和导入文献摘要或全文；（2）将各种来源的文献进行规范的电子存档；在文献中实时记录读书笔记和启发性思考；（3） 撰写论文时快捷而准确地进行文献标引，并随时修订格式以适应期刊投稿的需要；（4） 与研究同行或合作者在线分享文献资源（5）科学地管理参考文献有助于准确而有效地使用文献，起到事半功倍的效果。 常用的参考文献管理软件（1）Endnote (http://www.endnote.com)（2）Reference Manager (http://www.refman.com)（3）ProCite (http://www.procite.com)（4）WriteNote (http://www.writenote.com)（5）RefWorks (http://www.refworks.com)（6）Mendeley (https://www.mendeley.com/) 参考资料：[1]本刊编辑部.文献综述的格式与注意事项[J].临床误诊误治,2008(08):105.[2]崔建军.谈研究生学位论文中的文献综述写作[J].陕西广播电视大学学报,2007(03):59-61.[3]熊易寒.文献综述与学术谱系[J].读书,2007(04):82-84.[4]国科大研究生课程.《科研论文写作》]]></content>
  </entry>
  <entry>
    <title><![CDATA[R语言编程入门（数据可视化及可视化思维）]]></title>
    <url>%2F2019%2F05%2F22%2Fr-language3%2F</url>
    <content type="text"><![CDATA[R语言编程入门（数据可视化及可视化思维）写在前面的 上期推文主要介绍了如何学习R语言的函数包，并以rvest爬虫函数包为例，详细介绍了函数包学习、爬虫知识和R语言编程的强化训练并掌握R语言爬虫的三个重要函数read_html、html_nodes、html_text使用方法及搭配管道符使用的技能。了解HTML语言格式。在上期推文最后我画了一个词云图。这个图对大家来说还还是比较新鲜的，本次我来向大家介绍R语言的另外一大功能：强大的可视化功能 数据可视化R语言的可视化同样也涉及到了一定的编程语法，但是这套语法和前面讲的基本R语言编程语法不同，它有一套自己的风格。很多人会疑问为什么一个编程语言数据分析和数据可视化的语法不同呢？这是因为R语言的很多可视化功能都是用一些R语言工作者开发的函数包，不同的工作者风格不同，设计的函数、功能都有属于他们自己的风格。这也是为什么我在讲可视化之前专门讲一下如何学习R语言函数包。 目前很多教程都有讲R语言的可视化，甚至有的教程还真把R语言当一个作图的软件了。不过这些教程基本都是在讲怎么画柱状图、怎么画散点图，最多就是讲讲怎么画热图。其实说句实在话，很多教程并没有真正领悟到可视化的精髓，其实我也没有领悟到。这次我就以我的视角，讲一下如何去思考可视化。以我们最开始的那片推文为例子，在R语言样本间相关性分析中我做了一个这样的图 这个图是根据RNA-seq的样本的readscount值进行样本间相关性分析，用样本间相关性皮尔逊系数进行的可视化。A、B、C是三个小鼠感染病毒前的readscount值，AD、BD、CD是它们对应的感染病毒后安乐死的readscount值。下面我们继续在这个图上进行可视化深入研究。上次我使用了如下代码可视化这个图 library(corrplot) #使用corrplot函数可视化相关系数矩阵 corrplot(GE_cormatrix, type = &quot;upper&quot;, order = &quot;hclust&quot;, tl.col = &quot;black&quot;, tl.srt = 45) 上面这个代码使用了corrplot函数，这个函数是可视化相关性系数矩阵。而GE_cormatrix是相关系数矩阵，这个矩阵是这样的 矩阵中第i行j列代表第i个样本和第j个样本之间的相关性系数，由于任何一个样本和它本身默认为100%相关，所以矩阵的对角线都是1。而可视化则是画出了这个矩阵的上半部分，如下图所示所以type=”upper”就是这个含义，而order参数选择的是=hclust，这个是说最后可视化的结果是不是按照某种方式排序，我选择的hclust则是以聚类的方式排序。tl.col = “black”, tl.srt = 45这两个参数是选择字体颜色和是否倾斜。接下来我们来详细以order=hclust来详细衍生这张图的可视化从统计学的视角来说，热图是一种以颜色的不同或者深浅来展示数据与数据之前区别的可视化方法，而在可视化的视角来说，可视化则是以图形或者颜色的变化来展示数据的变化。如果把数据看做自变量，那么图形或者颜色就是数据的依变量。所以学习数据的可视化，并不是大量地学习各种函数包怎么用啊，也不是大量去学习各种软件的使用。而是去理解可视化的数学意义！就像我说的一样，生物信息学编程到最后就是看个人的数学与逻辑修养。上面这张图我就是通过了图形大小的变化和颜色深浅的变化来展示样本间的相关关系。既然样本之间有相关性系数矩阵，那么我们还可以用什么方法展示这个数据呢 聚类树我们已经有了样本与样本之间的皮尔逊相关系数矩阵，我们如果把每个样本看作空间中的点，例如我们的数据中，A和B相关系数、A和C的相关系数很高，那么在空间中可以看做A和B还有C的距离很近，而AD又与BD和CD的相关系数高，在空间中AD与BD还有CD的距离很近。所以引入距离的概念就很容易指导我们使用聚类分析来可视化数据。而聚类分析不能直接用相关系数矩阵转化为距离矩阵。使用命令 #dist函数是R语言自带的计算距离矩阵的函数 dist(GE_cormatrix) 这样就算出了A、B、C、AD、BD、CD作为空间中样本点之间的空间距离。然后我们就可以进行可视化了 #hclust函数是R语言自带的将距离矩阵聚类的函数，而plot函数也是R语言自带的画图函数，将聚类信息可视化出来 plot(hclust(dist(GE_cormatrix))) 这样我们在不使用任何函数包的情况下可视化出了样本间的相关关系。聚类树显示出，小鼠感染病毒前后readscount确实是有差异的。相同处理的小鼠readscount差异不大 热图如果觉得上面的图只是线条，没有色彩感，我们也可以使用热图来可视化。热图是用颜色来表示数据，比如我们同样利用样本之间的空间距离做热图。 #pheatmap：：是引用热图的函数包，这样可以不通过使用library函数加载。as.matrix(dist(GE_cormatrix))则是将相关系数矩阵计算空间距离后矩阵化，因为pheatmap函数要求输入数据是矩阵格式。display_numbers参数则是是否显示距离值 pheatmap::pheatmap(as.matrix(dist(GE_cormatrix)),display_numbers = T) 从热图中看，颜色越红代表距离值越大，也就是空间距离越远，两个样本越不相关，颜色越蓝代表距离值越小，空间距离越近，两个样本越相关。 本次重点从根本上理解可视化的数学含义！可视化并不单单只是作图，可视化是通过数学图形或者颜色，以一定的映射关系呈现并反映数据的变化趋势。同一个数据有多种不同的方式可视化，不同的数据也可以使用同一种方式进行可视化。所以不要去盲目学习可视化软件，也不要轻信各种讲R语言可视化的培训班，因为没有良好的数学理解能力，学再多的可视化技巧都是等于0，由于可视化类型还有很多，下期推文我们继续讲可视化]]></content>
      <tags>
        <tag>r</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R语言编程入门（爬虫函数包的使用）]]></title>
    <url>%2F2019%2F05%2F22%2Fr-language2%2F</url>
    <content type="text"><![CDATA[R语言编程入门（爬虫函数包的使用）写在前面的 上期推文介绍了R语言编程入门，从了解R语言的5种数据结构和两种控制结构开始，再到借上上篇推文的Readscount数据综合利用R语言编程证明Readscount属于负二项分布。现在再回顾一下R语言的五个数据结构：向量、矩阵、列表、数据框、因子，以及两种控制结构，循环和判断。今天我们来学习R语言编程的进阶——函数包的使用：掌握了一定R语言编程语法后，就要面对更高阶的学习了，我在前面说过，编程最后看的不是你对编程语言的掌握程度，而是看的你的数学能力和逻辑能力。当然在实际的生物信息学或者生物学研究生的工作中，我们没有那么多时间来进行数学模型建立、编程。那么巧妙应用已有的R语言函数包则是完成工作的捷径。R语言函数包非常多，积累并掌握一定数量的R语言函数包对于工作数据处理是有很显著的效率提升！为了让大家学会如何自学R语言函数包，我今天介绍一个大家听说过但又几乎没有接触过的内容——爬虫。 R语言爬虫那什么是爬虫呢？爬虫又称网络爬虫，网络爬虫是一种按照一定的规则，自动地抓取互联网信息的程序或者脚本。也就是要通过一个程序将互联网网页上的内容获取，互联网上的内容是按照一定的格式或规范展示的，要通过程序获取互联网的内容就需要了解互联网的信息的格式。一般情况下互联网信息都使用一种叫HTML的格式，这种格式的定义是超级文本标记语言（英语:HyperText Markup Language,简称:HTML）是标准通用标记语言下的一个应用，也是一种规范，一种标准，它通过标记符号来标记要显示的网页中的各个部分。网页文件本身是一种文本文件，通过在文本文件中添加标记符，可以告诉浏览器如何显示其中的内容（如：文字如何处理，画面如何安排，图片如何显示等）。浏览器按顺序阅读网页文件，然后根据标记符解释和显示其标记的内容，对书写出错的标记将不指出其错误，且不停止其解释执行过程，编制者只能通过显示效果来分析出错原因和出错部位。但需要注意的是，对于不同的浏览器，对同一标记符可能会有不完全相同的解释，因而可能会有不同的显示效果。 “超文本”就是指页面内可以包含图片、链接，甚至音乐、程序等非文字元素。 超文本标记语言的结构包括“头”部分（英语：Head）、和“主体”部分（英语：Body），其中“头”部提供关于网页的信息，“主体”部分提供网页的具体内容。 简而言之，HTML是一种标记语言，通常情况下是由&lt;元素&gt;内容&lt;/元素&gt;的形式存在，比如你新建一个txt文本文件在里面输入Bioinformatic保存后修改.txt后缀为.html格式，再用你自己的浏览器打开就会出现下图效果 所以在每个HTML文件中，每个内容都会在某个或者某些个元素之下即&lt;元素1&gt;&lt;元素2&gt;&lt;元素n&gt;你的内容&lt;/元素1&gt;&lt;/元素2&gt;&lt;/元素3&gt;，所以我们要找相关内容只需要找到这些内容对应的元素集合，我们将之称为xpath。所以我们只要找到目标信息对应的xpath就可以批量找到目标信息。读到这里可能很多读者也是蒙圈的，毕竟不是计算机专业出生不会这么快理解这些概念。但是不用担心，有一个Google浏览器插件SelectorGadget，这个可以自动帮助你抓取你想要的信息的路径。下面以一个实际例子来学会如何使用R语言爬虫的函数包。首先安装R语言爬虫函数包rvest #安装rvest install.packages(“rvest”) 其次在百度搜索SelectorGadget下载，拖拽到谷歌浏览器的拓展程序界面进行安装。以PeerJ文献期刊数据库为爬虫网站，选择数据库的Life &amp; Biology，出现以下内容注意观察我的图片红色向上箭头指着的部分，这个是网页的变化地址代表第几页。所以在R语言里可以用一个循环来动态访问这里的每一页。接下来就是用SelectorGadget抓取每篇文章的注意观察上图中红色箭头标出的部分，由于网页中可能有其他版块的CSS与你想抓取的部分是一样的，要注意观察Clear中的数量是不是与你在当前页面想抓取的版块数量一致。比如我只想抓取每篇文章的题目，一页放了15篇文章的链接，所以数量是15个。如果数量大于你的预期数量应该找找有没有被多抓取的版块，再用鼠标点击一下他们就可以删除掉多余部分。接下来就是如何利用rvest包进行爬虫了。要学会一个新的函数包首先应该在R语言里使用??函数 ??rvest Rstudio的右下部分会弹出这个界面，里面说明了一些有用的教程网站。具体教程读者可以点击进去学习。这里我主要介绍三个有用的爬虫函数及管道符号的使用。首先爬虫的三个重要函数是read_html、html_nodes、html_text。这三个函数无需了解其余参数。首先read_html函数的意思是让R访问并读取你想爬虫的网页，html_nodes函数则是输入你想抓取的信息的xpath路径或者CSS路径（xpath和CSS指代相同的能找到该信息的路径，只是语法不同，例如用SelectorGadget抓取的是CSS路径，那么html_nodes函数中则输入css = ），html_text函数则是将爬虫到的信息以文本形式传递给变量。而管道在R语言中的作用则是将上一条命令的输出作为下一条命令的输入，下面的代码中将会利用管道符串联爬虫函数。 以爬取Peerj文献期刊新发表文献的题目，并可视化这些题目都有哪些词汇的例子来讲解何使用rvest函数包并强化R语言编程能力 #加载rvest函数包 library(rvest) #先定义Bio_Paper变量用于储存爬虫PeerJ数据库中新发表的文献题目 Bio_Paper&lt;-NULL for (i in 1:7) { #在循环中动态标记要爬虫网页的地址，由于所有网页的不同点在于page后的页码不同，所以使用paste函数在每次循环中粘贴不同的页码 url&lt;-paste(&quot;https://peerj.com/articles/?journal=peerj&amp;discipline=biology&amp;q=&amp;page=&quot;,i,sep = &quot;&quot;) #使用read_html函数读取网页信息 myPeerJ&lt;-read_html(url) #使用管道符将myPeerJ变量内容作为输入内容，并根据css路径用html_nodes抓取文献题目,又以管道操作符传递给html_text函数以文本方式传递回peerjBio peerjBio&lt;-myPeerJ %&gt;% html_nodes(css = &quot;.search-item-title a&quot;) %&gt;% html_text() #使用正则表达式函数将爬取的网页信息中的换行符替换掉,gsub函数中pattern参数是指代想被替换的内容，replacement函数是想替换的内容 peerjBio&lt;-gsub(pattern = &quot;\n&quot;,replacement = &quot;&quot;,peerjBio) #使用正则表达式函数将爬取的网页信息中多余空格替换掉 peerjBio&lt;-gsub(pattern = &quot; &quot;,replacement = &quot;&quot;,peerjBio) #依次将peerjBio中的信息传递给Bio_Paper Bio_Paper&lt;-c(Bio_Paper,peerjBio) } #先定义一个列表类型变量，用于最后储存文献题目所用的词汇信息和词频信息 Bio_list&lt;-list() 使用While循环统计词汇和词频信息，这里是一个重点，希望读者认真思考一下这里的代码为什么是这样写的（提示：列表的标签具有唯一性！） i&lt;-1 while (i&lt;=length(Bio_Paper)) { word&lt;-strsplit(as.character(Bio_Paper[i]),&quot; &quot;)[[1]] for (j in 1:length(word)) { wd&lt;-word[j] Bio_list[[wd]]&lt;-c(Bio_list[[wd]],j) } i=i+1 } #将列表的所有标签以向量形式赋值给wd wd&lt;-names(Bio_list) #统计每个词汇的词频，并以向量形式给num num&lt;-NULL for (j in 1:length(Bio_list)) { num&lt;-c(num,length(Bio_list[[j]])) } #将词汇和词频组成数据框，用于后期可视化 Paper_wd&lt;-data.frame(a=wd,b=num) #建立一个新的向量，储存and in the等这些定冠词、连词等英文中无实意的单词，用于过滤数据框中这部分数据 words&lt;-c(&quot;and&quot;,&quot;in&quot;,&quot;the&quot;,&quot;of&quot;,&quot;on&quot;,&quot;a&quot;) #新定义Paper_wd2数据框用于储存过滤了无实意英文单词的文献词汇及词频数据框 Paper_wd2&lt;-data.frame() for (i in 1:nrow(Paper_wd)) { if(as.character(Paper_wd[i,1]) %in% words){next} else{Paper_wd2&lt;-rbind(Paper_wd2,Paper_wd[i,])} } #加载词云图函数包，用于可视化最新发表的文献题目中的词汇 library(wordcloud2) wordcloud2(Paper_wd2) 从词云图可以看出新发表文献主要还是以研究分析、基因、表达等内容较多。当然还有The这些首字母大写的定冠词没有过滤掉。本次重点，掌握自学R语言函数包的技能，强化R语言编程能力。掌握R语言爬虫的三个重要函数read_html、html_nodes、html_text使用方法及搭配管道符使用的技能。了解HTML语言本次涉及到的可视化内容我将在下一讲专门介绍R语言可视化知识。 往期精彩文章回顾从零开始入门R语言编程RNA-seq中的那些统计学问题（一）为什么是负二项分布]]></content>
      <tags>
        <tag>r</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[群体结构分析三种常用方法 (下篇)]]></title>
    <url>%2F2019%2F05%2F22%2Fpopulation-analysis2%2F</url>
    <content type="text"><![CDATA[写在前面在上篇文章中，我们学习了PCA分析基本过程，接下来接着学习系统发育树的构建和祖先成分分析。 二、 邻接法构建系统发生树基于SNP数据构建系统发生树，省略了序列比对的步骤，这里推荐使用邻接法（Neighbour-Joining，NJ）。 1. 简单介绍邻接法和系统发生树系统发生树（Phylogenetic tree）又称为演化树（evolutionary tree），是表明被认为具有共同祖先的各物种间演化关系的树，是一种亲缘分支分类方法（cladogram）。在树中，每个节点代表其各分支的最近共同祖先，而节点间的线段长度对应演化距离（如估计的演化时间）NJ法是基于最小进化原理经常被使用的一种算法，它不检验所有可能的拓扑结构，能同时给出拓扑结构和分支长度。在重建系统发育树时，它取消了UPGMA法所做的假定，认为在进化分支上，发生趋异的次数可以不同。该方法通过确定距离最近(或相邻)的成对分类单位来使系统树的总距离达到最小。它的特点是重建的树相对准确，假设少，计算速度快，只得到一棵树。 2. 数据处理和准备 过滤缺失率较高（–geno 0.05）和强连锁SNP位点（–indep-pairwise) #!/bin/bash plink=/software/biosoft/software/plink/plink time $plink --vcf 60_dog.merge.vcf --geno 0.05 --dog --autosome --set-missing-var-ids @:# --indep-pairwise 50 10 0.3 --out 60_dog &amp;&amp; echo "----prune SNP done----" time $plink --vcf 60_dog.merge.vcf --geno 0.05 --dog --autosome --set-missing-var-ids @:# --extract 60_dog.prune.in --recode vcf-iid --out 60dog_geno0.05_prune_in "----extract prune in SNP done----" 计算样本间IBS距离，这里会生成.mdist.id和.mdist两个文件。.mdist.id文件记录样本名称，.mdist文件记录样本间IBS距离time $plink --vcf 60dog_geno0.05_prune_in.vcf --distance 1-ibs --double-id --dog --autosome --out 60dog_geno0.05_prune_in 将.mdist.id和.mdist两个文件处理成MEGA软件可以读入的.meg格式，处理代码如下 #!/usr/bin/python import sys,re,os ### 读取样本id ID_file = open(r"60dog_geno0.05_prune_in.mdist.id",""r") arr = [] for i in ID_file: arr.append(i.strip().split()[0]) head = map(str,range(1,len(arr)+1)) ID_file.close() ### 读取样本间IBS距离 dist_file =open(r"60dog_geno0.05_prune_in.mdist","r").readlines() print ("#mega\n!Title: fasta file;\n!Format DataType=Distance DataFormat=LowerLeft NTaxa=%d;\n" % len(arr)) for i,j in enumerate(arr): print ('['+str(i+1)+']'+' #'+j) #print print ('[ '+' '.join(head)+' ]') print ('[1]') for l,m in enumerate(infile2): tmp = m.strip().split() #tmp[-1] = '' print ('['+str(l+2)+'] '+' '.join(tmp)) dist_file.close() 将上面生成的.meg文件导入到MEGA软件中，可以看到数据长这个样子3. 系统发生树可视化 在MEGA软件中，读取数据后，选择NJ法进行构建进化树，在弹出的窗口中可以看到树形轮廓基本构建出来 接下来将树形图导出为nwk格式文件，进行下一步美化。nwk文件记录样本间亲缘关系相对远近，可以被大部分树形图软件识别。可以看一下我们这里的nwk文件。直接输出在这里，60个样本。(((DQZA01,(((DQZA06,DQZA12),((((DQZA18,DQZA55),(DQZA80,DQZA81)),DQZA23),DQZA19)),DQZA33)),((((((DQ25,((DQ28,DQ33),DQ52)),(DQ32,DQ54)),DQ45),DQ29),DQ60),DQ59)),(((((YJ01,YJ10),YJ03),((YJ08,YJ16),(YJ14,YJ26))),YJ17),(YJ15,YJ21)),((((((KD106,KD117),(KD110,KD113)),(KD107,(KD207,KD208))),((DKD301,DKD302),((((((DKD303,DKD307),DKD304),DKD306),(DKD305,DKD309)),DKD308),DKD310))),(KD215,(KD222,KD223))),(WM3,(YL1,(LJ6,((YL5,YL7),(((LJ1,LJ2),LJ12),(LJ13,LJ8)))))))); 使用FigureTree软件进行美化。首先用FigureTree打开nwk文件然后，选择树形为针叶状最后为不同子群样本分配颜色，就可以得到文章发表级别的图片了。小结这里构建的是物种树，和基因树有所不同。基于单个同源基因差异构建的系统发生树应称之为基因树，因为这种树代表的仅仅是单个基因的进化历史，而不是它所在物种的进化历史。物种树一般最好是从多个基因数据的分析中得到。此外，NJ法构建发生树适用于进化距离不大的样本，这也算是缺点之一。 三、 祖先成分分析祖先成分分析主要是探究个体的混血程度和群体间的基因交流程度。目前，可以做祖先成分分析的软件主要有 STRUCTURE，frqppe 和 admixture。 admixture运算速度快，操作简单，成为了主流的分析软件。软件下载链接 admixture 1. 简单介绍admixture算法原理admixture用于从多位点SNP基因型数据集对个体祖先进行最大似然估计，虽然与STRUCTURE使用相同的统计模型，但其快速数值优化算法可以更快速地进行计算估计。具体来说，admixture使用 block relaxation 方法来交替更新等位基因频率和血统比例参数。通过解决大量独立凸优化问题来处理每个block更新，而这些凸优化问题则使用快速序列二次规划算法来解决。通常来说，SNP基因型数据集中的个体应该是不相关的，例如case-control相关性研究中的个体。 2. 数据处理和祖先成分估计 数据过滤 plink=/software/biosoft/software/plink/plink time $plink --vcf 60_dog.merge.vcf --dog --geno 0.05 --maf 0.05 --hwe 0.0001 --make-bed --out 60_dog_qc &amp;&amp; echo "----QC and vcf2bed done" 估计最优祖先个数，即群体分为几个亚群更合理，这里亚群数目称为K。通常情况下，由于不知道这个群体实际包含几个亚群，可以预设K的范围为2~n。软件就会模拟在K=x的情况下，推算群体是如何分群的，以及每个个个体的血统构成比例。对每个K值模拟的结果，软件都会计算出一个CV error值和最大似然值，error值和最大似然值两个指标都可以挑选最佳K值。对于error值来说 admixture=/software/biosoft/software/admixture_linux-1.3.0/admixture for K in {2..5} do time $admixture --cv 60_dog_qc.bed $K |tee log${K}.out &amp;&amp; echo "----admixture K=$K done----" done 对每个K值模拟的结果，软件都会计算出一个CV error值和最大似然值，error值越小越好，似然值越大越好，两个指标都可以挑选最佳K值。一般而言就是要挑选出error值最小和K值最小的分群策略或者挑选出似然值最大和K值最小的分群策略，即”拐点”。然而，目前学界对于如何判断分群最佳仍有很大争议，CV error值和最大似然值只是参考，无法起决定性作用，因为，目前主流做法是把K值（2~n）都画出来，分别解释。这里取K=2~5，来看下具体结果。很不幸，这里error值和似然值都是单调的，没有所谓的拐点，可以都画出来看一下。 admixture运行结束后，会产生.Q文件，记录每个个体不同群体祖先成分信息，以K=2为例截图看下 添加上样本ID和所属分组ID，进行可视化。这里以K=2时，来看下处理后的文件 3. 祖先成分堆叠图可视化这里使用R完成祖先成分堆叠图可视化，由于篇幅限制，这里只放主体部分 library(ggplot2) library(cowplot) plotFile]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[群体结构分析三种常用方法 (上篇)]]></title>
    <url>%2F2019%2F05%2F13%2Fpopulation-analysis1%2F</url>
    <content type="text"><![CDATA[写在前面在群体遗传学和进化生物学相关的项目中，群体结构分析是最常见也是最初步的分析内容，可以帮助我们确认样本分群是否符合预期以及检测离群样本。群体结构分析最常用的三种方法就是PCA、系统发生树和祖先成分堆叠图，下面我们将使用发表在Genome Rearch上的Gou et al,2014中的数据（60只狗全基因组SNP）逐一讲解。 一、 PCA分析1. 简单介绍PCA原理PCA (Principal Component Analysis) ，即主成分分析方法，是一种使用最广泛的数据降维算法，通过正交变换将一组数量庞大且可能存在相关性的变量转换为一组低维的线性不相关的变量。对于一个全基因组测序样本call出的SNP数目通常是百万级甚至是千万级别，比如本文所用数据为17M SNP，如果直接使用这些SNP位点作为指标对个体进行区分，就会由于信息过于冗杂而无法把握重点，并造成计算资源和时间的浪费。PCA分析的过程就是从千万级别的SNP位点中提取关键信息，以便我们使用更少的变量就可以对样本进行有效的刻画和区分。 2. PCA实践在Gou et al, 2014文章中，作者使用GCTA进行PCA分析，在本文中，还会讲解另外一种主流的PCA分析软件–EIGENSOFT中的smartpca。 软件下载链接：GCTAEIGENSOFT GCTA进行PCA分析 首先准备GCTA中PCA模块所需的输入文件，GCTA可以直接读取.bed , .bim , .fam文件，使用plink将vcf格式文件转换成上述三种文件,同时进行简单的过滤(–geno 0.05)。 #!/bin/bash plink=/software/biosoft/software/plink/plink time $plink --vcf 60_dog.merge.vcf --geno 0.05 --dog --make-bed --out 60_dog_geno0.05 &amp;&amp; echo &quot;---- vcf2bed done -----&quot; 然后使用GCTA软件，–make-grm 生成个体对之间的遗传关系矩阵(genetic relationship matrix),并将GRM的下三角元素保存为二进制文件.grm.id , .grm.bin ， .grm.N.bin。这里要强调一点，对于非人物种，要设定好染色体数目参数，–autosome-num。否则会因为不识别26以上的染色体编号而报错。 gcta=/software/biosoft/software/gcta_1.92.1beta6/gcta64 time $gcta --bfile 60_dog_geno0.05 --make-grm --autosome-num 38 --out 60_dog_geno0.05 &amp;&amp; echo &quot;----make-grm done----&quot; 最后就是要进行PCA分析使用 –pca 设置要生成主成分的数目，这里设置为4，一般来说就可以刻画出群体结构。这一步会生成 .eigenval 和 .eigenvec 两个文件。.eigenval文件为各主成分可解释遗传信息的比例，.eigenvec文件为每个样本在top4主成分上的分解值。 time $gcta --grm 60_dog_geno0.05 --pca 4 --out 60_dog_geno0.05 &amp;&amp; echo &quot;----gcta-pca done----&quot; 补充样本所属子群信息在生成的.eigenvec文件中，并没有包含样本所属子群信息，在进行可视化之前，需要添加上样本群体信息。样本较少时可以手动添加，样本数目较多时推荐写代码完成，避免错误。文章Gou et al,2014中根据采样地点划分子群。添加上子群信息后，.eigenvec长这个样子。EIGENSOFT进行PCA分析 使用plink将vcf格式文件转换成.bed , .bim , .fam文件，步骤同上，这里不再赘述。 smartpca需要提供三种输入文件，文件1记录每个样本在每个SNP位点的信息，文件2记录每个SNP位点的信息，文件3记录每个样本的性别和所属子群信息。熟悉plink格式的同学应该知道.bed文件储存的就是genotype信息，.bim文件存储的就是SNP位点信息，那么我们只需要自己生成文件3就可以了。文件3格式如下，第一列为样本ID，第二列为性别（M，F，U），第三列为所属子群。 接下来把所有参数写入一个文件中，就可以运行smartpca了，其中 numoutevec 是输出主成分的数目。 运行smartpca，生成 .evec 和 .eval 文件。time /software/biosoft/software/EIG-6.1.4/bin/smartpca -p PCA.par &gt; smartpca.log &amp;&amp; echo &quot;----smartpca----done&quot; 截图来看下 .evec 文件，两种软件运行结果差不多。 3. 绘制PCA散点图通过得到的.evec和eval两个文件，这里使用python完成可视化，代码如下： import matplotlib.pyplot as plt import collections import re hash = collections.OrderedDict() eval_file = open(&quot;60_dog_geno0.05.eigenval&quot;,&quot;r&quot;) evec_file = open(&quot;60_dog_geno0.05.eigenvec&quot;,&quot;r&quot;) ### 从.eval文件中读取top4主成分 eval_1 = eval_file.readline() eval_2 = eval_file.readline() eval_3 = eval_file.readline() eval_4 = eval_file.readline() ### 从.evec文件中读取在pc上的值 for i in evec_file: if re.match(r&#39;#&#39;,i.strip()): continue tmp = i.strip().split()[1:] if tmp[-1] in hash.keys(): hash[tmp[-1]][&#39;1st&#39;].append(eval(tmp[1])) hash[tmp[-1]][&#39;2nd&#39;].append(eval(tmp[2])) hash[tmp[-1]][&#39;3rd&#39;].append(eval(tmp[3])) hash[tmp[-1]][&#39;4th&#39;].append(eval(tmp[4])) else: hash[tmp[-1]] = hash.get(tmp[-1],collections.OrderedDict()) hash[tmp[-1]][&#39;1st&#39;] = hash[tmp[-1]].get(&#39;1st&#39;,[]) hash[tmp[-1]][&#39;2nd&#39;] = hash[tmp[-1]].get(&#39;2nd&#39;,[]) hash[tmp[-1]][&#39;3rd&#39;] = hash[tmp[-1]].get(&#39;3rd&#39;,[]) hash[tmp[-1]][&#39;4th&#39;] = hash[tmp[-1]].get(&#39;4th&#39;,[]) hash[tmp[-1]][&#39;1st&#39;].append(eval(tmp[1])) hash[tmp[-1]][&#39;2nd&#39;].append(eval(tmp[2])) hash[tmp[-1]][&#39;3rd&#39;].append(eval(tmp[3])) hash[tmp[-1]][&#39;4th&#39;].append(eval(tmp[4])) ### 绘制散点图，这里只绘制pc1和pc2,其他pc可按照下面代码逐一画出。 fig = plt.figure(figsize=(20,10)) ax = fig.add_subplot(1, 1, 1) ### 2D figure #ax = fig.add_subplot(111,projection=&#39;3d&#39;) ### 3D figure mark = [&#39;o&#39;,&#39;v&#39;,&#39;s&#39;,&#39;*&#39;,&#39;x&#39;,&#39;+&#39;]*10 ### 设置散点性状 col = [&#39;b&#39;,&#39;g&#39;,&#39;y&#39;,&#39;r&#39;,&#39;c&#39;]*10 ### 设置散点颜色 for m,n in enumerate(hash.keys()): ### 逐点画出 ax.scatter(hash[n][&#39;1st&#39;],hash[n][&#39;2nd&#39;],c=col[m],s=75,marker=mark[m],label=n,alpha=0.7) ax.legend(loc=&#39;best&#39;,scatterpoints=1,fontsize=&#39;12&#39;,framealpha=0) ax.set_xlabel(&#39;Eigenvector1 ({}%)&#39;.format(float(&#39;%.2f&#39; % eval(eval_1))),fontsize=13,fontweight=&#39;bold&#39;) ax.set_ylabel(&#39;Eigenvector2 ({}%)&#39;.format(float(&#39;%.2f&#39; % eval(eval_2))),fontsize=13,fontweight=&#39;bold&#39;) #### 保存图片 save_FileName = &#39;60dog_geno0.05_gcta_pc1_pc2.png&#39; plt.savefig(save_FileName,dpi=400,bbox_inches=&#39;tight&#39;) PCA散点图如下#### 小结PCA运用方差分解，将海量SNP的差异反映在二维坐标图上，坐标轴轴取对样品差异性解释度最高的两个或三个特征值，样本SNP位点信息越相似，反映在PCA图中的距离越近。PCA可以帮助我们清楚掌握手上样本的群体结构，并有效检测出离群样本，为下游分析减少不必要的麻烦，同时，实现PCA分析的方法很多，PCA结果图也简单易懂，称得上是低调实力派。最后强调一点，PCA分析是降维，不是聚类，希望大家不要搞混了。对PCA理论和分析过程感兴趣的同学，可以进一步看下这篇文章。 谢谢大家赏脸看到这里，在本篇文章中，主要学习了PCA分析的基本过程，在下篇文章中，我们将接着学习系统发生树构建和祖先成分分析，精彩不容错过，请持续关注我们！ 参考文献Genome Res. 2014 Aug;24(8):1308-15. doi: 10.1101/gr.171876.113. Epub 2014 Apr 10. http://blog.csdn.net/zhongkelee/article/details/44064401]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
      <tags>
        <tag>population genetics</tag>
        <tag>pca</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始入门R语言编程]]></title>
    <url>%2F2019%2F05%2F10%2Fr-language1%2F</url>
    <content type="text"><![CDATA[R语言编程入门写在前面的 上期推文介绍了如何使用R语言编程进行样本间相关性分析，主要涉及了R语言矩阵、循环、统计分析函数、ggplot2、corrplot等函数包的使用，涉及的知识面较多，如果不是经常做生物信息或者是编程出家，很难在短时间内适应编程模式。其实，一旦适应了编程，很多问题都能轻松迎刃而解。毕竟计算机语言也是一门语言，和人与人之间打交道一样，人与计算机打交道也是很容易的。本次推文以引导大家入门R语言编程为目的，用尽可能精炼的内容教学R语言。认识R语言首先认识R语言的数据类型及控制结构。掌握这些基础以后，再认识R语言的函数包。经过一定时间的练习就可以熟悉R语言了。 R语言数据类型1：向量向量是R语言最基本的数据类型。R语言中向量和大学时候学的线性代数一样，是由一行或者一列数据组成的变量。 x&lt;-c(1,2,3,4,5) #把向量赋值给x，c是连接函数。此时x就是由数字1到5组成的向量 length(x) #获取向量长度 x[3]#x的第三个元素 x[-1:-2] #删除x前两个元素 x&lt;-x[-1:-2] #删除x前两个元素并保存 b&lt;-seq(from=1,to=100,by=2) #seq是生成等差数列函数，from是数列起始值to是终止值，by是公差 R语言数据类型2：矩阵R语言矩阵可以理解为多个长度相同的向量按照行或列排列而成的多维向量，R语言矩阵与线性代数中的矩阵稍有不同，不同点在于矩阵的乘法计算上，线性代数中矩阵乘法计算方法为设A为 mxp 的矩阵，B为 pxn 的矩阵，那么称 mxn 的矩阵C为矩阵A与B的乘积,其中矩阵C中的第 i行第 j列元素可以表示为 $$AB_{ij}=\sum_{k=1}^p a_{ik}b_{kj}=a_{i1}b_{1j}+a_{i2}b_{2j}+…+a_{ip}b_{pj}$$ 但是R语言中矩阵乘法是对应元素做乘积。举例为 f&lt;-cbind(c(1,2,3),c(4,5,6)) #将两个向量按列捆绑可得得到矩阵,如果想按照行向量捆绑为矩阵用rbind函数 z&lt;-cbind(c(1,2,3),c(4,5,6)) f*z [,1] [,2] [1,] 1 16 [2,] 4 25 [3,] 9 36 x&lt;-matrix(c(1,2,3,4,5,6),ncol=2,nrow=3)#另外一种矩阵赋值方式，ncol定义矩阵的列数，nrow定义矩阵行数，另外ncol(x)或者nrow(x)则可以求矩阵的列数或行数。 x[1,2]#查看矩阵第1行第2列元素,当然如果想查看矩阵第一行则为x[1,]查看第二列为x[,2]。 R语言数据类型3：列表列表是一种特殊的向量，不同之前的向量。之前的向量可以理解为原子型，列表中可以保存为不同格式的数据。可以是字符串型、数值型等。举例为 j&lt;-list(name=&quot;joe&quot;,salary=55000,union=T)#其中name，salary，union理解为列表的标签，等号后面的量理解为列表标签的值，可以一个标签对应多个值，但是列表的标签具有唯一性！ Names(j)#获取列表的所有标签。当然如果想使用其他方法查询列表第2个标签可以使用就j$salary，j[2]，或者j[[2]]。两者的区别读者可以通过实践发现。 R语言数据类型4：数据框数据框与矩阵有很多相似之处，很多矩阵适用的函数数据框同样适用。与矩阵不同的是，数据框里面的数据类型可以不一致，但矩阵维数必须相等，即各列数据长度相等。以一个实例简单介绍数据框 a&lt;-c(&quot;apple&quot;,&quot;pen&quot;) b&lt;-c(17,14) c&lt;-data.frame(a,b,stringsAsFactors = F) #创建一个简单的数据框，由于矩阵所用的函数多数都适应于数据框，所以这里不再详细介绍。具体操作读者们可以自行在R语言上实践。 R语言数据类型5：因子因子在实际的数据分析中用到的场合不多。在R语言中，因子（factor）表示的是一个符号一个编号或者一个等级。以一个实际例子让读者理解因子。 x&lt;-c(100,1,4,9,1320) x&lt;-factor(x) #对向量X进行因子化处理 [1] 100 1 4 9 1320 Levels: 1 4 9 100 1320 从例子中可以看出，因子实际是为向量数据中的每一个元素赋予一个级别。例如元素100，它对应的level是100，比第二个元素1对应的level大，所以在level的排序上，100在第三个level上。 R语言控制结构常用的两种控制结构为判断和循环，简单地说掌握了基本控制结构和R语言的数据类型是可以进行数据分析的，毕竟任何一种编程语言只要掌握了语法，剩下的就看个人的数学与逻辑素养了。 1.判断结构：通常为if（条件）{命令}else{命令}类型，也有if…else if…else。也可以只有一个if。举例介绍判断结构语法，输入一个数，如果它大于等于10，输出Y，如果大于等于5小于10输出T，否则输出N x&lt;-6 if(x&gt;=10){print(&quot;Y&quot;)}else if(x&lt;10 &amp;&amp; x&gt;=5){print(&quot;T&quot;)}else{print(&quot;N&quot;)} 2.循环结构：R语言中for循环使用较多，通常结构为for(条件){命令}。以简单例子举例for循环语法，定义一个数值向量，将向量中每个元素的平方赋值给新向量。 x&lt;-c(1:9) y&lt;-NULL for (i in 1:length(x)) { y&lt;-c(y,(x[i])^2) } print(y) [1] 1 4 9 16 25 36 49 64 81 综合运用R语言编程 以上是R语言编程入门最基本的语法，只有逐渐熟悉这些语法，才可以更进一步使用R语言，下面以上次推文的数据为例子，为大家讲一下如何综合运用R语言知识。上次的推文中是利用RNA-seq的reads count数据计算样本间的重复性分析。另外小明师兄的上篇推文RNA-seq中的那些统计学问题(一)为什么是负二项分布，这篇推文从统计学角度出发推理论证了RNA-seq的reads count数据为什么符合负二项分布，下面我们从R语言编程开始利用实际RNA-seq的readscount数据证明一下为什么是负二项分布 根据负二项分布的性质，均值和方差为$$\mu={pr}/{(1-p)}$$$$\sigma^2={pr}/(1-p)^2$$将P用U表示并带入方差表达式得到 $$\sigma^2=\mu^2/r+\mu$$根据数学知识可以看出方差是均值的二次函数。所以我们这次综合利用R语言编程的知识进行证明。 readcount]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅说动力学模型（下篇）]]></title>
    <url>%2F2019%2F05%2F08%2Fdynamic-model2-md%2F</url>
    <content type="text"><![CDATA[浅说动力学模型（下篇）原创： 赵洪龙 宇宙实验媛图1. 数学建模在生物学研究中的应用范式书接上文浅说动力学模型（上篇），由于本人主要研究植物学代谢建模，下面就举例说明使用模型在植物生理研究中取得的一些重要成就。在过去的研究中，植物领域对光合作用的研究积累了大量的数据，这为光合作用相关的动力学建模提供了数据基础。研究者们已经从细胞代谢层面(1)、叶片层面(2, 3)、植株水平和群体水平分别构建了模型(4)。 利用这些模型已经成功预测出了限制光合作用的关键过程，并且部分预测结果已经通过基因工程改造进行了验证。自然条件下，植物生长的环境总是不断变化的，比如一阵微风吹过就会导致植物叶片接收的光发生改变。上层叶片摆动，下层叶片就会处于一明一暗的不断变化中。正如人眼睛的瞳孔在黑暗条件下放大、在明亮条件下会缩小，而闪光对眼睛有很强的刺激作用一样，闪光对植物而言也是一种非常严重的胁迫，会导致光合作用电子传递链出现损伤，从而抑制光合作用。植物有很多机制能耗散过剩的能量和修复这种闪光带来的电子传递链损伤，以防止被‘烧伤’并最大化光能的利用效率。以前没有一个合适的系统模型评估过这种能量耗散途径对作物产量所造成的损失，故而也一直没有明确的方案能通过减少这种损失来提高作物产量的潜力。Zhu等构建的作物群体中叶片光合作用能量分配的动力学模型，结合光线追踪算法对这个过程进行了理论分析。其预测显示，如果能够加快这个修复过程，作物的生产能力能增加20%左右(5)。为了能更好地理解这个原理，我们给出了示意图（图2）。叶片在强光条件下的时候，一般来说植物的光合能力相对要高一些，而过多的能量会通过其他的途径耗散，以免叶片吸收过多的能量无法被转化成化学能导致叶片‘烧伤’。这对大部分植物叶片来讲是一种很好的防御机制（叫做光损伤防御）。然而，当植物从强光条件下转移到低光条件下的时候（t0时刻），能量往往成为光合能力的一种限制条件。此时，那些防御途径仍然“大手大脚”地耗散光能，减少了分配到化学能转化的分量。在数分钟或者数小时（假设为t0 – t2）的低光下，这些防御途径耗散的能量会慢慢减少，光合同化能力慢慢增加，我们把这段时间称之为恢复时间。就像我们从太阳底下走进相对比较暗的房间一样，眼睛会突然感觉到很暗，但过一段时间后房间里面就会变得‘亮起来’。从图2中我们可以很容易理解，如果减少这段恢复时间，就能够增加总的光能利用效率。因此，Zhu等2004年建立的模型主要描述了：**1. 植物冠层中上下不同层次的点在一天的光强变化条件下的太阳辐射截获量； 截获的太阳辐射与光合能力之间的对应关系； 接受的太阳辐射量变化时，植物由于光损伤防御机制的存在而引起的能量损失（及温度的影响）； 通过减少光损伤防御恢复时间能给作物的光合生产能力提升带来的潜在空间。图2. 植物叶片从强光变到至弱光条件下时光损伤防御恢复机制的简单示意图。红色箭头表示叶片暴露在不同的光下。如果能把光损伤防御恢复的时间从t2缩短至t1，将有可能提高整个群体光合生产能力的12-30%。历经12年其合作团队通过基因工程手段进行改造，加快了这种光损伤恢复的速度，实现了烟草的光合作用能力及生产能力的大幅增加，成功验证了这一预测(6)。不仅如此，Zhu等建立的卡尔文循环代谢模型结合进化算法，预测了光合作用碳同化代谢过程中限制其效率的关键酶，这些预测与报道的实验结果一致(1)，因此也被采纳为作物改良的主要策略之一(7, 8)。Wang等建立的C4光合作用代谢模型系统解析了C4光合作用代谢过程高效的互作机理(9)。 利用动力学模型，还可以为将来合成生物学导入新的代谢途径提供理论分析工具。** 比如通过参考模型计算和分析的结果进行基因工程改造，增加微生物中特定代谢物的产量，这在微生物研究的应用中较为广泛(10)；或者增加植物的次生代谢产物的产量(11) 等。有关复杂代谢系统生物学建模的方法，Zhu(12)和Zhao(13)等均分别进行了综述，感兴趣的读者可参考阅读。展望动力学模型虽然近些年来，随着数据测量技术和计算能力的飞速提升，系统生物学的发展势头迅猛，也取得了一些成就，然而现在动力学模型的构建过程中依然存在一些挑战。首先，动力学模型是基于具体过程的一类模型。而生物学过程总是交织成多维度的复杂网络，针对具体问题解析这些关系是一件繁琐的事情，因此模型构建是一件十分耗时的工作，且对研究的问题需要有深厚的知识背景，况且，现在我们并没有完全了解所有的生命过程。更甚，对于一个模型的参数化和验证而言，难以收集到一套完整而系统的数据集。数据库和文献中收集的数据均非常离散，环境、物种和测量过程的差异都会导致模型参数化和验证过程陷入困境。第三，模型参数化的过程和方法需要进一步提升并规范化。笔者认为，决定未来动力学模型构建与生物学研究耦合的几个重要因素如下： 第一，生物学测量技术需要进一步发展，高通量、高标准和高质量的数据是模型构建的基石。虽然目前组学数据的增长速度史无前例，但测量方法之间的差异、规范性问题、条件和细胞特异性问题等依旧是建模的重要拦路虎。 第二，多背景交叉融合需要进一步加强。生物学与化学、物理学、数学、计算机科学等学科之间的交流能在确保重要信息的前提下更好地将复杂生物学问题简单化，从而确保模型具有更强的生物学预测能力和应用价值。 第三，模型需要不断被优化和更新。所有的模型都不可能是完美的，需要针对具体问题进行结构和参数的进一步优化，因此需要构建一些大型的数据存储和模型分析平台，从而进入用模型帮助数据分析、用数据优化和修正模型的良性循环。至此，方能更好地将模型应用于生物学研究，造福于人类。 参考文献：Zhu XG, de Sturler E, &amp; Long SP (2007) Optimizing the distribution of resources between enzymes of carbon metabolism can dramatically increase photosynthetic rate: a numerical simulation using an evolutionary algorithm. Plant physiology 145(2):513-526.Tholen D &amp; Zhu XG (2011) The mechanistic basis of internal conductance: a theoretical analysis of mesophyll cell photosynthesis and CO2 diffusion. Plant physiology 156(1):90-105.Xiao Y, Tholen D, &amp; Zhu XG (2016) The influence of leaf anatomy on the internal light environment and photosynthetic electron transport rate: exploration with a new leaf ray tracing model. Journal of experimental botany 67(21):6021-6035.Song Q, Chu C, Parry MA, &amp; Zhu XG (2016) Genetics-based dynamic systems model of canopy photosynthesis: the key to improve light and resource use efficiencies for crops. Food Energy Secur 5(1):18-25.Zhu XG, Ort DR, Whitmarsh J, &amp; Long SP (2004) The slow reversibility of photosystem II thermal energy dissipation on transfer from high to low light may cause large losses in carbon gain by crop canopies: a theoretical analysis. Journal of experimental botany 55(400):1167-1175.Kromdijk J, et al. (2016) Improving photosynthesis and crop productivity by accelerating recovery from photoprotection. Science 354(6314):857-861.Miyagawa Y, Tamoi M, &amp; Shigeoka S (2001) Overexpression of a cyanobacterial fructose-1,6-/sedoheptulose-1,7-bisphosphatase in tobacco enhances photosynthesis and growth. Nat Biotech 19(10):965-969.Simkin AJ, McAusland L, Headland LR, Lawson T, &amp; Raines CA (2015) Multigene manipulation of photosynthetic carbon assimilation increases CO(2) fixation and biomass yield in tobacco. J Exp Bot 66(13):4075-4090.Wang Y, Brautigam A, Weber AP, &amp; Zhu XG (2014) Three distinct biochemical subtypes of C4 photosynthesis? A modelling analysis. Journal of experimental botany 65(13):3567-3578.Colón AM, Sengupta N, Rhodes D, Dudareva N, &amp; Morgan J (2010) A kinetic model describes metabolic response to perturbations and distribution of flux control in the benzenoid network of Petunia hybrida. The Plant Journal 62(1):64-76.Xin CP, Tholen D, Devloo V, &amp; Zhu XG (2015) The benefits of photorespiratory bypasses: how can they work? Plant physiology 167(2):574-585.Zhu XG (2010) Systems-level modeling–a new approach for engineering efficient photosynthetic machinery. Journal of biotechnology 149(3):201-208.Zhao H, Xiao Y, &amp; Zhu XG (2017) Kinetic Modeling of Photorespiration. Methods Mol Biol 1653:203-216.]]></content>
      <categories>
        <category>Bio Cooking</category>
      </categories>
      <tags>
        <tag>dynamic model</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RNA-seq中的那些统计学问题（一）为什么是负二项分布？]]></title>
    <url>%2F2019%2F05%2F07%2Frna-seq-data-analysis1%2F</url>
    <content type="text"><![CDATA[1. 转录组数据统计推断的难题在RNA-seq中进行两组间的差异分析是最正常不过的了。 我们在其它实验中同样会遇到类似的分析，通常，我们可以用方差分析判定两组“分布”数据间是否存在显著差异。原理是：当组间方差大于组内方差（误差效应），并且统计学显著时，则认为组间处理是可以引起差异的。 那这不就是咱们学过的统计学里普普通通的假设检验问题吗？用熟悉的算法简单地进行计算，分分钟就能搞定吧——凸样凸拿衣服，骚年要是事情都那么简单，要科学家干嘛，问题就在于：常规方法搞不定啊！ 其实统计学家也很无奈啊，看看我们转录组实验得到的这些数据吧：我们的实验只进行少得可怜的生物学重复（n&lt;10），而且，任何基因的表达量都不能是负数，这些数据并不符合正态分布，用于表征表达量的counts是非连续的（芯片信号是连续的），RNA-seq数据的离散通常是高度扭曲的，方差往往会大于均值……，就这些奇怪的特征，使得准确估计方差并没有想象的那么容易。 我们面临两个核心问题： 基因表达数据适合用什么统计学分布进行差异显著性检验？ 如何利用少量生物学重复数据估算基因表达的标准差？ 2. 泊松分布 or 负二项分布？从统计学的角度出发，进行差异分析肯定会需要假设检验，通常对于分布已知的数据，运用参数检验结果的假阳性率会更低。转录组数据中，raw count值符合什么样的分布呢？ count值本质是reads的数目，是一个非零整数，而且是离散的，其分布肯定也是离散型分布。对于转录组数据，学术界常用的分布包括泊松分布 (poisson)和负二项分布 (negative binomial)两种。 2.1. 为什么泊松分布不行？首先有必要简单地介绍一下泊松分布 泊松分布适合于描述单位时间（或空间）内随机事件发生的次数（事件发生的次数只能是离散的整数）。如某一服务设施在一定时间内到达的人数，电话交换机接到呼叫的次数，汽车站台的候客人数，机器出现的故障数，自然灾害发生的次数，一块产品上的缺陷数，显微镜下单位分区内的细菌分布数等等。 $$P(X=k)=\frac{\lambda^k }{k!}e^{-\lambda},\quad k=0,1,…$$ 泊松分布大概长这样： λ是波松分布所依赖的唯一参数。 λ值愈小分布愈偏倚， 随着λ的增大 ， 分布趋于对称。 当λ=20时分布接近于正态分布；当λ=50时， 可以认为波松分布呈正态分布。 在数据分析的早期，确实有学者采用泊松分布进行差异分析，但是发展到现在，几乎全部都是基于负二项分布了，究竟是什么因素导致了这种现象呢？为了解释这个问题，我们必须提到一个概念 overdispersion。 dispersion指的是离散程度，研究一个数据分布的离散程度，我们常用方差这个指标。对于泊松分布而言，其均值和方差是相等的，但是我们的数据确不符合这样的规律。通过计算所有基因的均值和方差，可以绘制如下的图片： 横坐标为基因在所有样本中的均值，纵坐标为基因在所有样本中的方差，直线的斜率为1，代表泊松分布的均值和方差的分布。可以看到，真实数据的分布是偏离了泊松分布的，方差明显比均值要大。 如果假定总体分布为泊松分布， 根据我们的定量数据是无法估计出一个合理的参数，能够符合上图中所示分布的，这样的现象就称之为overdispersion。 由于真实数据与泊松分布之间的overdispersion，选择泊松分布分布作为总体的分布是不合理。 以上只证明了泊松分布是个不太恰当的分布估计，那怎么证明负二项分布就是合适的分布估计呢？ 2.2. 为什么负二项分布行？主要是从均值与方差之间的关系去证明 同样的，也先简单介绍一下负二项分布： 二项分布描述的是n重伯努利实验，在n重贝努利试验中，事件A恰好发生x(0≤x≤n)次的概率为： $$P_n(x)=C_n^x p^x(1-p)^{n-x}$$ 它的概率分布图如下： 负二项分布描述的也是伯努利实验，不过它的目标事件变成了：对于Bernoulli过程，我们设定，当某个结果出现固定次数的时候，整个过程的数量，比如我们生产某个零件，假设每个零件的合格与否都是相互独立的，且分布相同，那么当我们生产出了五个不合格零件时，一共生产了多少合格的零件，这个数量就是一个负二项分布，公式如下： $$f(k;r;p)=P(x=k)=C_{r+k-1}^k p^k(1-p)^r$$ 该公式描述的是，在合格率为p的一堆产品中，进行连续有放回的抽样，当抽到r个次品时，停止抽样，此时抽到的正品正好为k个的概率 它的概率分布如下： 负二项分布的均值和方差分别为： $$\mu=\frac{pr}{1-p}$$ $$\sigma^2=\frac{pr}{(1-p)^2}$$ 将p用μ表示，得到： $$p=\frac{\mu}{\mu+r},\quad 1-p=\frac{r}{\mu+r}$$ 将上一步推出的p和1-p带入到方差的表达式中，得到： $$\sigma^2=\frac{\mu^2}{r}+\mu$$ 记1/r=α，则 $$\sigma^2=\mu+\alpha\mu^2$$ 从上面的式子可以看出，均值是方差的二次函数，方差随着均值的增加而进行二次函数形式的递增，正好符合上文 2.1. 为什么泊松分布不行？ 部分均值与方差分布图的情况 其中α和r被称为dispersion parameter 负二项分布与泊松分布的关系，可以用α或r推出： 当 r -&gt; ∞ 时，α -&gt; 0，此时 σ2= μ，为泊松分布； 当 r -&gt; 0 时，α -&gt; ∞，此时overdispersion 3. 方差估计在生物学重复很少时，我们是很难准确计算每个基因表达的标准差的（相当于这个数据集的离散程度）。我们很可能会低估数据的离散程度。 被逼无奈的科学家提出了一个假设：表达丰度相似的基因，在总体上标准差应该也是相似的。我们把不同生物学重复中表达丰度相同的基因的总标准差取个平均值，低于这个值的都用这个值，高于这个值的就用算出来的值。 参考资料： (1) 【生信修炼手册】负二项分布在差异分析中的应用 (2) 【 生信百科】转录组差异表达筛选的真相 (3) 【生信媛】RNA-seq分析中的dispersion，你知道吗？ (4) H. J. Pimentel, et al. Differential analysis of RNA-Seq incorporatingquantification uncertainty. bioRxiv, 2016 注：本文章已经发表于微信公众号《宇宙实验媛》，如需转载，请联系本人或该公众号 欢迎关注宇宙实验媛]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[运用AI对科研文章中的图片进行绘图]]></title>
    <url>%2F2019%2F05%2F07%2Fai-drawing%2F</url>
    <content type="text"><![CDATA[TMJ universebiologygirl本次视频主要以《Polarizing brain organoids》这篇文章内的Fig. 1为例，使用Adobe Illustrator CS6 对Fig.1 进行绘图，供大家学习参考。此次视频主要涉及的内容为“基础图形的绘制”、“渐变色添加”、“图形反光绘制”、“路径查找器工具的使用”等。AI drawing下面将对绘图过程进行展开说明：第一步：文件→新建→对画板进行设置。一般为方便投稿，配置文件选择“打印”，大小选择“A4”，颜色模式CMYK，(具体可根据投稿要求选择，CMYK模式是彩色印刷时采用的一种套色模式) （视频内设置仅为画图方便所设）。第二步：绘制图a的图片的基本形态。采用左边工具栏“画笔工具”，按住鼠标，绘制出想要的图形（画出大致轮廓即可，可后期通过锚点调整）。第三步：通过锚点调整形状，主要用到两个工具添加/删除锚点工具、直接选择工具。直接选择工具可对锚点进行调整，从而达到调整基本形状的目的。若第二步画出的基本形状的锚点不足以调整，可通过添加/删除锚点工具，在线条上添加或删除后，再用直接选择工具调整。转换工具可使锚点周围线条变得平滑。（如下图标出的红框所示）第四步：填充背景颜色。该步骤使用到几个工具：吸管工具、填色、描边、渐变。将图形填充后可用画笔用具将边缘颜色画好，同样可用锚点、直接选择工具对边缘形状进行调整。吸管工具可对已知颜色进行选取。第五步：绘制培养皿。先画出一个基本的椭圆形，再使用3D工具对这个椭圆形进行3D加工。红框界面可调整视觉角度。调整结束后可采用对象→拓展外观，然后添加描边。（本视频中采用的是添加线条的方法，没有拓展外观工具方便）。随后可用矩形工具、椭圆工具等画出培养皿底部，培养物等基本形状。第六步：继续用矩形、椭圆工具画出圆底烧杯的基本图形框架。用窗口→路径查找器→形状模式→联集，对图形轮廓进行合并（可自己尝试多种路径查找器功能，拼接出自己想要的图案）。其余填色等步骤与前文所提及的步骤异曲同工。第七步：绘制器皿的反光和阴影。用弧形工具绘制出曲线后，选择线条形状，如“等比”等，再通过锚点工具、直接选择工具对线条进行调整即可。阴影的绘制可采用镜像工具对已画出的反光图形进行翻转。第八步：先画出一个椭圆，改成白色的渐变色，采用渐变工具对渐变方向做调整，对透明度作出调整（可自行调节到需要的透明度），使用径向模糊工具作出调整。其他步骤与上述步骤均相似，不再赘述。Tip：在word等工具性软件常用的通用快捷键如下（常用的复制粘贴就不说啦） 撤销：Ctrl+z 剪切：ctrl+x 加粗：ctrl+b 绘图时希望圆形呈正圆或画出的直线是笔直不倾斜时，按住shift再进行拖拽放大/缩小 AI里整个图形放大缩小，需要按住Alt而不是Ctrl。参考文献：https://www.nature.com/articles/s41587-019-0084-4）]]></content>
      <categories>
        <category>Paper writing</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[R语言进行样本间相关性分析]]></title>
    <url>%2F2019%2F05%2F07%2Fcorrelation-analysis-in-R%2F</url>
    <content type="text"><![CDATA[熊东彦 universebiologygirl 写在前面的话生物学数据分析中相关性分析是十分重要的一个环节，但这个环节又经常被忽视。以生物信息学分析为例：非生物信息专业的学生通常在分析实验获得测序数据时都会忽略这个问题。 大多数非专业生物信息学背景学生的生物信息数据分析技巧都是从网上的一些教程或者是培训机构的视频中学到的，遗憾的是培训机构仅仅只是教会了你跑一个组学分析的流程，其中涉及的很多重要的细节并没有教给你。因此，从细节着手进行相关性分析显得尤为重要。例如：在转录组测序时候通常要选择3个生物学重复。但是选择了3个生物学重复后送去测序，拿到数据跑完一遍流程真的就可以用来进行差异表达分析吗？绝大多数情况下是可以的，但如果你研究的是细菌、病毒这些变异相对较多的物种，你的一个处理的不同生物学重复可能就不一定是一致的了。如果这种情况发生，那就要剔除这组变异数据。用剩下的数据进行差异表达分析。毕竟设置3个生物学重复一方面是保证一定的重复性，一方面也是预防某个重复值不能用的情况。当然在其他生物学数据分析中，也有这种情况。 今天介绍一个利用R语言编程来进行相关性分析的方法。毕竟当样本数据量较大的情况，手动挨个做相关性分析非常费时间。同时用R语言做完相关性分析后的最大特点是可视化，可以直接提升文章数据的档次感。 手把手教程呈现：回顾一下相关性分析概念：相关性分析是指对两个或多个具备相关性的变量元素进行分析进而衡量两个变量因素的相关密切程度。相关性的元素之间需要存在一定的联系或者概率才可以进行相关性分析。相关性分析分为pearson相关分析、spearman相关性分析和kendall相关性分析。本次主要介绍Pearson相关分析。它适用于变量为连续性变量且变量之间存在线性关系的情况，二者缺一不可。 示例数据：来源于小鼠感染某种烈性病毒前后的转录组测序数据。实验设计为随机选择3只同一窝出生的小鼠，先提取血液进行转录组测序，随后同时向同窝小鼠注射某种烈性病毒，感染病毒第7天提取血液再进行转录组测序。拿到测序数据后跑完部分转录组分析流程，获取基因表达的reads计数结果。 在进行Pearson相关分析前，按照Pearson相关分析的基本要求先判断两个变量之间是否存在线性关系。随机选取同一个处理的两个生物学重复，用ggplot2绘制两个变量之间的散点图。 library(ggplot2) ggplot(data = OUT883_withoutgenename[,1:2],mapping = aes(x=OUT883_withoutgenename[,1],y=OUT883_withoutgenename[,2],color="red"))+geom_point()+geom_smooth(method = lm) #geno_point()绘制散点图，geom_smooth()进行拟合，method选择lm是线性拟合 从上图可以看出，除去个别区间，两个变量主要呈现线性关系。之所以个别区间如x=0附近是这样的关系是由于我没有对数据进行过滤。（比如因为测序误差导致的在两个生物学重复之间由于测序仪误差一个基因在被测到而在对应生物学重复内没有测到）。 判断符合Pearson相关性分析条件后可以进行后续分析。现在要对这些数据进行两两相关性分析，以初步判断样本之间是否有较大差异。 readscount]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[如何理解和计算FDR?]]></title>
    <url>%2F2019%2F05%2F07%2Ffdr%2F</url>
    <content type="text"><![CDATA[按照惯例，先来个自我介绍： 大家好，我是实习6年的生信狗小明同学，不会唱、跳、Rap，喜欢装X，瞎逼逼和开车（话说我驾照都没啊），以后我将在这里给大家带来生信、统计学和机器学习方面的分享，希望大家能够喜欢（欢呼声和掌声现在可以起来了╭(●｀∀´●)╯） 最后给大家提供一个关注我的传送门：https://ming-lian.github.io，第一时间的更新在我的个人主页上，发到这里的都是在个人主页基础上的整理 爱你们哟｡:.ﾟヽ(｡◕‿◕｡)ﾉﾟ.:｡+ﾟ 下面是正文 1. 多重假设检验的必要性统计学中的假设检验的基本思路是： 设立零假设（null hypothesis）$H_0$，以及与零假设$H_0$相对应的非零假设（alternative hypothesis， or reject null hypothesis）$H_1$，在假设$H_0$成立的前提下，计算出$H_0$发生的概率，若$H_0$的发生概率很低，基于小概率事件几乎不可能发生，所以可以拒绝零假设 但是这些传统的假设检验方法研究的对象，都是一次试验 在一次试验中（注意：是一次试验， 即single test），0.05 或0.01的cutoff足够严格了(想象一下，一个口袋有100个球，95个白的，5个红的, 只让你摸一次，你能摸到红的可能性是多大？) 但是对于多次试验，又称多重假设检验，再使用p值是不恰当的，下面来分析一下为什么： 大家都知道墨菲定律：如果事情有变坏的可能，不管这种可能性有多小，它总会发生 用统计的语言去描述墨菲定律： 在数理统计中，有一条重要的统计规律：假设某意外事件在一次实验（活动）中发生的概率为p（p&gt;0），则在n次实验（活动）中至少有一次发生的概率为 $p_n=1-(1-p)^n$ 由此可见，无论概率p多么小（即小概率事件），当n越来越大时，$p_n$越来越接近1 这和我们的一句俗语非常吻合：常在河边走，哪有不湿鞋；夜路走多了，总能碰见鬼 在多重假设检验中，我们一般关注的不再是每一次假设检验的准确性，而是控制在作出的多个统计推断中犯错误的概率，即False Discover Rate（FDR），这对于医院的诊断情景下尤其重要： 假如有一种诊断艾滋病(AIDS)的试剂，试验验证其准确性为99%（每100次诊断就有一次false positive）。对于一个被检测的人（single test)）来说，这种准确性够了；但对于医院 （multiple test)）来说，这种准确性远远不够 因为每诊断10 000个个体，就会有100个人被误诊为艾滋病(AIDS)，每一个误诊就意味着一个潜在的医疗事故和医疗纠纷，对于一些大型医院，一两个月的某一项诊断的接诊数就能达到这个级别，如果按照这个误诊率，医院恐怕得关门，所以医院需要严格控制误诊的数量，宁可错杀一万也不能放过一个，因为把一个没病的病人误判为有病，总比把一个有病的病人误判为没病更保险一些 100 independent genes. (We have 100 hypotheses to test) No significant differences in gene expression between 2 classes (H0 is true). Thus, the probability that a particular test (say, for gene 1) is declared significant at level 0.05 is exactly 0.05. (Probability of reject H0 in one test if H0 is true = 0.05) However, the probability of declaring at least one of the 100 hypotheses false (i.e. rejecting at least one, or finding at least one result significant) is: $$1-(1-0.05)^{100}\approx 0.994$$ 2. 区别p值和q值 $H_0$ is true $H_1$ is true Total Not Significant TN FN TN+FN Significant FP TP FP+TP Total TN+FP FN+TP m 首先从上面的混淆矩阵来展示p值域q值的计算公式，就可以看出它们之间的区别： p值 p值实际上就是false positive rate(FPR，假正率)： $$p-value=FPR=\frac{FP}{FP+TN}$$ 直观来看，p值是用上面混淆矩阵的第一列算出来的 q值 q值实际上就是false discovery rate (FDR)： $$q-value=FDR=\frac{FP}{FP+TP}$$ 直观来看，q值是用上面混淆矩阵的第二行算出来的 但是仅仅知道它俩的计算公式的差别还不够，我们还有必要搞清楚一个问题：它俩在统计学意义上有什么不同呢？ p值衡量的是一个原本应该是$H_0$的判断被错误认为是$H_1 \, (reject H_0)$的比例，所以它是针对单次统计推断的一个置信度评估； q值衡量的是在进行多次统计推断后，在所有被判定为显著性的结果里，有多大比例是误判的 据此，我们可以推导出p值域q值（q值有两种定义，FWER或FDR，这里指的是FWER）之间的关系： 总共有n个features(可以是基因，GWAS中的snp位点等)，对它们执行n重假设假设检验后，得到各自对应的p值分别为$\{p^{(i)} \mid i=1,2,…,n\}$ 当p值显著性水平取$\alpha$时，得到$k$个features具有p值显著性，它们的p值为$\{p^{(i)}{(j)} \mid j=1,2,…,k\}$，其中$p^{(i)}{(j)}$表示第i个feature它的p值在升序中的排名为j，那么这k个features的FWER可以表示为： $$FWER=1-\prod_{j=1}^{k}(1-p^{(i)}_{(j)})$$ 3. 如何计算q值？统计检验的混淆矩阵： $H_0$ is true $H_1$ is true Total Significant V S R Not Significant U T m-R Total m0 m-m0 m FWER (Family Wise Error Rate) 作出一个或多个假阳性判断的概率 $$FWER=Pr(V\ge 1)$$ 使用这种方法的统计学过程： The Bonferroni procedure Tukey’s procedure Holm’s step-down procedure FDR (False Discovery Rate) 在所有的单检验中作出假阳性判断比例的期望 $$FDR=E\left[\frac{V}{R}\right]$$ 使用这种方法的统计学过程： Benjamini–Hochberg procedure Benjamini–Hochberg–Yekutieli procedure 3.1. Benjamini-Hochberg procedure (BH)对于m个独立的假设检验，它们的P-value分别为：$p_i,i=1,2,…,m$ （1）按照升序的方法对这些P-value进行排序，得到： $$p_{(1)} \le p_{(2)} \le … \le p_{(m)}$$ （2）对于给定是统计显著性值$\alpha \in (0,1)$，找到最大的k，使得 $$p_{(k)} \le \frac{\alpha * k}{m}$$ （3）对于排序靠前的k个假设检验，认为它们是真阳性 (positive ) 即：$reject \, H_0^{(i)},\, 1 \le i \le k$ $$\begin{array}{c|l}\hlineGene &amp; p-value \\\hlineG1 &amp; P1 =0.053 \\\hlineG2 &amp; P2 =0.001 \\\hlineG3 &amp; P3 =0.045 \\\hlineG4 &amp; P4 =0.03 \\\hlineG5 &amp; P5 =0.02 \\\hlineG6 &amp; P6 =0.01 \\\hline\end{array}\, \Rightarrow \,\begin{array}{c|l}\hlineGene &amp; p-value \\\hlineG2 &amp; P(1) =0.001 \\\hlineG6 &amp; P(2) =0.01 \\\hlineG5 &amp; P(3) =0.02 \\\hlineG4 &amp; P(4) =0.03 \\\hlineG3 &amp; P(5) =0.045 \\\hlineG1 &amp; P(6) =0.053 \\\hline\end{array}$$ $$\alpha = 0.05$$ $P(4) =0.03&lt;0.05*\frac46=0.033$ $P(5) =0.045&gt;0.05*\frac56=0.041$ 因此最大的k为4，此时可以得出：在FDR&lt;0.05的情况下，G2，G6，G5 和 G4 存在差异表达 可以计算出q-value： $$p_{(k)} \le \frac{\alpha*k}{m} \, \Rightarrow \, \frac{p_{(k)}*m}{k} \le \alpha$$ Gene P q-value G2 P(1) =0.001 0.006 G6 P(2) =0.01 0.03 G5 P(3) =0.02 0.04 G4 P(4) =0.03 0.045 G3 P(5) =0.045 0.054 G1 P(6) =0.053 0.053 根据q-valuea的计算公式，我们可以很明显地看出： $$q^{(i)}=p_{(k)}^{(i)}*\frac{Total \, Gene \, Number}{rank(p^{(i)})}=p_{(k)}^{(i)}*\frac{m}{k}$$ 即，根据该基因p值的排序对它进行放大，越靠前放大的比例越大，越靠后放大的比例越小，排序最靠后的基因的p值不放大，等于它本身 我们也可以从可视化的角度来看待这个问题： 对于给定的$\alpha \in (0,1)$，设函数$y=\frac{\alpha}{m}x \quad (x=1,2,…,m)$，画出这条线，另外对于每个基因，它在图上的坐标为$(rank(p_{(k)}^{(i)}),p_{(k)}^{(i)})=(k,p_{(k)}^{(i)})$，图如下： 通过设置$\alpha$可以改变图中直线的斜率，$\alpha$越大，则直线的斜率越大，落在直线下方的点就越多，通过FDR检验的基因也就越多，反之，直线的斜率越小，落在直线下方的点就越少，通过FDR检验的基因也就越少 当固定$\alpha$，而统计检验次数m增加时，这条直线的斜率变小，落在直线下方的点就越少，通过FDR检验的基因也就越少 参考资料： (1) Storey, J.D. &amp; Tibshirani, R. Statistical signifcance for genomewide studies.Proc. Natl. Acad. Sci. USA 100, 9440–9445 (2003) (2) 国科大研究生课程《生物信息学》，陈小伟《基因表达分析》]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[NCBI Taxonomy数据处理：TaxonKit工具详解]]></title>
    <url>%2F2019%2F04%2F22%2Ftaxonkit-usage%2F</url>
    <content type="text"><![CDATA[原创：老板，来一打TPU 遇到的问题 在做宏基因组分析时，通过基因注释得到一个包含10k之多种微生物物种名list(scientific name)，现在想统计这些物种在界、门、纲、目、科、属等不同分类水平的总的数量。这就是本篇推送想解决的问题，10000多种微生物的拉丁名称示例如下： [NeptuneYT$] head scientific_name.txt Abiotrophia defectivaAbiotrophia sp.Absiella dolichumAcaryochloris marinaAcetanaerobacterium sp.Acetivibrio cellulolyticusAcetoanaerobium noteraeAcetoanaerobium sticklandiiAcetobacter acetiAcetobacter ghanensis [NeptuneYT$] wc -l all_bacteria_genomic_fna.species 10146 all_bacteria_genomic_fna.species 打开NCBI Taxonomy输入一个拉丁名，如Acetobacter aceti，搜索之后默认获得完整的lineage信息，但我们这里只需要7个层次的，因此再点击一次Lineage获得缩略的谱系信息，如下：得到的Lineage字段后以分号隔开的就是对应于7个分类层次的结果，后续以分号切割之后统计不同列的结果即可。很自然的，我们想到爬虫，其搜索接口为:https://www.ncbi.nlm.nih.gov/taxonomy/?term=拉丁名（空格以+号连接），如https://www.ncbi.nlm.nih.gov/taxonomy/?term=Acetobacter+aceti，然后对结果页面进行后续解析。但是10k之多的查询量，必然要设置爬取频率，否则就要被NCBI关小黑屋了，考虑时间代价，果断放弃。其实，从网上查询的原理也是基于Taxonomy后台的数据库，而这个文件在ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz，可以从解压之后的names.dmp和nodes.dmp文件写代码解析，但是其内容过于妖孽，为了少撸掉点头发，因此先看看网上是否有造好的轮子。果然，动动手指发现有三个工具可以实现以上诉求，ETE toolkit, taxadb和 TaxonKit，这里选择最近发表的TaxonKit，优势在于其直接基于names.dmp和nodes.dmp文件的解析，本地搜索速度块，尤其是大批量的查找和格式转换，另外使用也极简单。TaxonKit paper相比于另外两种工具，TaxonKit在处理大批量数据时更快，占用内存也可接受 taxonkit 概述说完废话，进入今天的主题，说说TaxonKit这个工具的使用。TaxonKit是处理NCBI Taxonomy数据库中结构性数据的良心工具，19年1月在bioRxiv上online，作者Wei Shen, Jie Xiong，隶属于Department of ClinicalLaboratory, General Hospital of Western Theater Command，特地查了一下，原来是位于成都的中国人民解放军西部战区总医院（好牛的感觉），看来生信真是无处不在。它是Go语言编写的，可以在Windows，Linux和Mac OS X运行，直接使用NCBI Taxonomy的数据（需手动下载）而无需构建本地数据库。 taxonkit安装安装选择对应系统的版本安装，推荐conda安装。详见https://github.com/shenwei356/taxonkit,conda安装:conda install -c bioconda taxonkit 下载依赖数据下载NCBI taxonomy数据库的taxdump.tar.gz文件，解压后将names.dmp和 nodes.dmp拷贝到家目录下的.taxonkit目录下。 wget -c ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz tar -zxvf taxdump.tar.gz mkdir -p $HOME/.taxonkit cp names.dmp nodes.dmp $HOME/.taxonkit 确认文件完整： [NeptuneYT]$ ll -h ~/.taxonkit/ total 155494-rw-r–r– 1 xx UsersGrp 169.3M Mar 18 16:07 names.dmp-rw-r–r– 1 xx UsersGrp 134.4M Mar 18 16:07 nodes.dmp 配置完成，开始使用。 taonkit使用taxonkit –help Usage: taxonkit [command] Available Commands: genautocomplete generate shell autocompletion script help Help about any command lineage query lineage of given taxids list list taxon tree of given taxids name2taxid query taxid by taxon scientific name reformat reformat lineage version print version information and check for update…Use “taxonkit [command] –help” for more information about a command. taxonkit按照功能分成不同的子命令，其中最主要的功能包括4块：1）列出给定taxonomy id(taxid)的子分类树：list2）从taxid获取完整谱系：lineage3）重新构造谱系的格式：reformat4）通过物种拉丁名查询taxid：name2taxid 1）列出给定taxonomy id的子分类树[NeptuneYT$] taxonkit list --help #查看list子命令使用方法 list taxon tree of given taxidsUsage: taxonkit list [flags]Flags: -h, –help help for list –ids string taxid(s), multiple values should be separated by comma –indent string indent (default “ “) –json output in JSON format. you can save the result in file with suffix “.json” and open with modern text editor –show-name output scientific name –show-rank output rankGlobal Flags: –data-dir string directory containing nodes.dmp and names.dmp (default “/home/xx/.taxonkit”) –line-buffered use line buffering on output, i.e., immediately writing to stdin/file for every line of output -o, –out-file string out file (“-“ for stdout, suffix .gz for gzipped out) (default “-“) -j, –threads int number of CPUs. (default value: 1 for single-CPU PC, 2 for others) (default 2) –verbose print verbose information 实例：给定taxid：9606和10090 [NeptuneYT$] taxonkit list --ids 9606,10090 --show-name --show-rank -j 2 #–ids 给定的taxid，多个以英文逗号分割 #–show-name 输出科学命名 #–show-rank 输出分类等级 #-j 线程数，默认是2 9606 [species] Homo sapiens 63221 [subspecies] Homo sapiens neanderthalensis 741158 [subspecies] Homo sapiens subsp. ‘Denisova’ 10090 [species] Mus musculus 10091 [subspecies] Mus musculus castaneus 10092 [subspecies] Mus musculus domesticus 35531 [subspecies] Mus musculus bactrianus 39442 [subspecies] Mus musculus musculus 46456 [subspecies] Mus musculus wagneri 57486 [subspecies] Mus musculus molossinus 80274 [subspecies] Mus musculus gentilulus 116058 [subspecies] Mus musculus brevirostris 179238 [subspecies] Mus musculus homourus 477815 [subspecies] Mus musculus musculus x M. m. domesticus 477816 [subspecies] Mus musculus musculus x M. m. castaneus 947985 [subspecies] Mus musculus albula 1266728 [subspecies] Mus musculus domesticus x M. m. molossinus 1385377 [subspecies] Mus musculus gansuensis 1643390 [subspecies] Mus musculus helgolandicus 1879032 [subspecies] Mus musculus isatissus 2）从taxid获取完整谱系[NeptuneYT$] echo 9606|taxonkit lineage -d &quot;-&quot; -t -r #-d 输出谱系树分割符，默认分号 #-t 显示包含taxid的谱系树 #-r 显示给定taxid的分类等级 9606 cellular organisms-Eukaryota-Opisthokonta-Metazoa-Eumetazoa-Bilateria-Deuterostomia-Chordata-Craniata-Vertebrata-Gnathostomata-Teleostomi-Euteleostomi-Sarcopterygii-Dipnotetrapodomorpha-Tetrapoda-Amniota-Mammalia-Theria-Eutheria-Boreoeutheria-Euarchontoglires-Primates-Haplorrhini-Simiiformes-Catarrhini-Hominoidea-Hominidae-Homininae-Homo-Homo sapiens 131567-2759-33154-33208-6072-33213-33511-7711-89593-7742-7776-117570-117571-8287-1338369-32523-32524-40674-32525-9347-1437010-314146-9443-376913-314293-9526-314295-9604-207598-9605-9606 species 3）重新构造谱系的格式上一步通过taxid提取的谱系信息复杂，往往需要根据我们的需求重新格式化 [NeptuneYT$] echo 9606|taxonkit lineage |taxonkit reformat 9606 cellular organisms;Eukaryota;Opisthokonta;Metazoa;Eumetazoa;Bilateria;Deuterostomia;Chordata;Craniata;Vertebrata;Gnathostomata;Teleostomi;Euteleostomi;Sarcopterygii;Dipnotetrapodomorpha;Tetrapoda;Amniota;Mammalia;Theria;Eutheria;Boreoeutheria;Euarchontoglires;Primates;Haplorrhini;Simiiformes;Catarrhini;Hominoidea;Hominidae;Homininae;Homo;Homo sapiensEukaryota;Chordata;Mammalia;Primates;Hominidae;Homo;Homo sapiens 输出结果的第三列就是重新格式化的结果，默认是(“{k};{p};{c};{o};{f};{g};{s}”)7个水平。查询给定taxid9606的谱系，并按照门：科；属的格式输出 [NeptuneYT$] echo 9606|taxonkit lineage |taxonkit reformat -f &quot;{p}:{f};{s}&quot; |cut -f3 #{}内是分类等级，大括号之间是输出的连接符 Chordata:Hominidae;Homo sapiens 4）通过物种拉丁名查询taxid：name2taxid按人的拉丁名查询taxid [NeptuneYT$] echo &quot;Homo sapiens&quot; |taxonkit name2taxid Homo sapiens 9606批量查询 [NeptuneYT$] head scientific_name.txt |taxonkit name2taxid --show-rank Abiotrophia defectiva 46125 speciesAbiotrophia sp. 76631 speciesAbsiella dolichum 31971 speciesAcaryochloris marina 155978 speciesAcetanaerobacterium sp.Acetivibrio cellulolyticus 35830 speciesAcetoanaerobium noterae 745369 speciesAcetoanaerobium sticklandii 1511 speciesAcetobacter aceti 435 speciesAcetobacter ghanensis 431306 species 回到问题基于Taxonkit上述用法，回到之前的问题就好解决了1.将scientific name先转化成taxid，便于查找lineage time taxonkit name2taxid scientific_name.txt >scientific_name_taxid.txt &amp; awk -F"\t" '$2!=""{print $2}' scientific_name.txt >find_taxid.txt #去掉未查到的，即空值 awk -F"\t" '$2==""' scientific_name_taxid.txt >NotFindName.txt #输出未查到物种名 awk -F"\t" 'BEGIN{OFS="\t";print "findTaxid\tNull\tTotal"}{$2!=""?taxid++:null++}\ END{print taxid,null,taxid+null}' scientific_name_taxid.txt |column -t #统计查找到的和未查到的数量 real 0m6.279sfindTaxid Null Total9606 541 10147 可以看到，查询速度相当之快。由于待批量查询的物种名不是规范的拉丁名称，导致出现两个问题，一是输入的list是10146个,转换id后（找到和未找到）的行数却增加了1个，统计之后发现是同一个物种名有两个taxid；二是没找到的高达541个！！！为了说明完整的处理过程，541个后面再说。 [NeptuneYT$]$ awk -F"\t" '{print $1}' scientific_name_taxid.txt |sort |uniq -d Deinococcus soli 手工查询发现是两个种，但是根据部分拉丁名Deinococcus soli可以查到俩taxid我也是醉了。2.查找lineage [NeptuneYT$] time taxonkit lineage find_taxid.txt >lineage.txt&amp; real 0m7.860s 3.重新格式化lineage [NeptuneYT$] time taxonkit reformat lineage.txt|cut -f3 &gt;newformat.txt&amp; real 0m11.465s 结果：现在就按照界门纲目科属种的层次变成整齐划一的格式了，通过简单处理即可统计不同层次的物种数量和分布，首先构建一个用于循环处理的分类标签 [NeptuneYT$] cat tag.txt 1 Kingdom2 Phylum3 Class4 Order5 Family6 Genus7 Species 详细的物种分类层次数量统计： [NeptuneYT$] cat tag.txt|while read num lev;do awk -v v=$num -F";" '{print $v}' newformat.txt|sort |uniq -c|sort -nr |awk -v v=$lev '{print v"\t"$0}' \ |awk '$3!=""';done >detail_taxonomy_range.txt [NeptuneYT$] head detail_taxonomy_range.txt Kingdom 6722 BacteriaKingdom 4 EukaryotaPhylum 2682 ProteobacteriaPhylum 1390 FirmicutesPhylum 1099 ActinobacteriaPhylum 729 Bacteroidetes 按7个类别统计数量： [NeptuneYT$] cat tag.txt|while read index level;do num=$(awk -v v=$index -F";" '{print $v}' \ newformat.txt|sort |uniq |wc -l);printf "${level}\t${num}\n";done |tee taxonomy_stat.txt Kingdom 2Phylum 118Class 82Order 181Family 401Genus 1824Species 6519 简单画个图瞅瞅： databracket_minus_name.txt #minus all bracket awk '{filed1=$1;$1="";sub(/ /,"");gsub(/ /,"_");print filed1,$0}'\ NotFindName.txt >underline_plus_name.txt #plus all filed of split underline except first one 将后面查到的taxid加到之前的taxid list再按之前的流程跑一遍即可，实在查不到的那就手工吧。（此时配乐起~“这是自由的感觉，鼠标咔哒咔哒点击这些可爱的物种名称，凭着一颗永不哭泣勇敢的心”） 参考taxonkit githubTaxonKit: a cross-platform and efficient NCBI taxonomy toolkit]]></content>
      <categories>
        <category>Bioinformatics</category>
      </categories>
      <tags>
        <tag>taxonkit</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[source/about/about.md]]></title>
    <url>%2F2019%2F04%2F19%2Fsource-about-about-md%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[来吧，让我们相爱吧--硬核推送]]></title>
    <url>%2F2019%2F04%2F10%2Fpush-us%2F</url>
    <content type="text"><![CDATA[原创: 伊颜落芸 宇宙实验媛 友情公众号推送​ 一直关注我们生信技能树公众号的小伙伴肯定知道，我们一向的原则是分享学习心得，提供知识的干货。所以今天就向大友情家推荐一个亲民又有料的公众号—宇宙实验媛。 生信技能树 &amp; 宇宙实验媛十年同窗，五年同行。 因生信而结缘，因科学而励志 我们的创始人Jimmy和宇宙实验媛的主编yuzy是大学同学，都说妇女能顶半边天，yuzy既然喊出了豪言壮语，那小编不得不推荐一下了😜 关于“宇宙实验媛”​ 一听这个名字就很霸气😲，这个公众号的重点分享基因组学实验操作和生物信息学方法，但也关注优秀文献的分享，实用方法的解读，理论模型的创新。立足于全方位多角度揭示科学的故事。 下面是该公众号的往期精选 模式动物选择：​ 微生物组学研究的那些奇葩动物学模型 介绍了研究终极目标—明确三种关系：生境内各微生物之间的关系；微生物与物质代谢/物质循环的关系；微生物与生境（比如宿主内）稳态的关系 所进行的差异性模型选择。 ​ 实用论文投稿及发表技巧​ 浅谈期刊投稿与论文发表（上） 分享了如何进行目标期刊的筛选和规避投稿的雷区；浅谈期刊投稿与论文发表（下） 结合笔者自身经历讲述何为“行百里者半九十”。 复杂计算模型及概念​ 如果你对机器学习和经典的数据模型感兴趣，你可以看看机器学习数据分析极简思路及sklearn算法实践小试 实用数据挖掘​ 没有实验思路和线索怎么办？没关系，看看这些或许能给你一点思路： 基于组学数据的分子功能挖掘 骨肉瘤中发现DANCR作为ceRNA促进增殖和转移 优秀文献赏析​ “单细胞”中研究调控细胞周期起始的分子机制 深度剖析细胞周期研究的精细调控机制，明星分子非经典功能研究与组学数据挖掘的结合 紧跟热点研究方向，挖掘多组学平台对经典生物学分子的功能研究。 生物实验狗最关心的详细实验技巧CHIP-Seq经验总结 为你保驾护航，Western Blot详细攻略 让你显影的时候不再如履薄冰。 非主流的快乐运营这个公众号的小伙伴是一群对科学有兴趣的年轻人，他们不光分享干货还创造快乐，比如偶尔皮一下的编辑们，画风是这样的。。。。。。 ​ 上面的介绍如果让你心动就赶紧关注起来吧～]]></content>
      <categories>
        <category>We</category>
        <category>Photo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[浅说动力学模型（上篇）]]></title>
    <url>%2F2019%2F04%2F09%2Fdynamic-model1%2F</url>
    <content type="text"><![CDATA[原创： 赵洪龙 宇宙实验媛 插曲以前我还在中科院-马普学会计算生物学伙伴研究所的时候，常常有朋友问我诸如“会不会做DNA数据分析？”，“会不会做甲基化数据分析？”之类的生物信息学（狭义）问题。很惭愧，我没有这方面的经验。又有朋友觉得奇怪， 追问：“你不做实验，又不做生物信息学分析，那你到底是干什么的？”我是做动力学模型的。“什么模型？”基于常微分方程的动力学模型，是动力学模型中的一种。每次说到这里，对方的反应总是一愣，随后一蒙，然后一呵呵。这里，我就来简单说说动力学模型到底是个什么东西。 关于模型的简介说到模型，很多人首先就是一哆嗦，因为想到了一堆符号公式和复杂的拓扑结构图形等等。其实大可不必紧张，模型实际上就是我们对客观世界认知和归纳的一个抽象体。就像我们买房子的时候，售楼小姐姐和小哥哥给我们展示的小区规划图、等比缩放沙盘一样，这些都是模型。地产商只是把一些我们想了解的楼盘信息抽象出来，用图画或者微缩造型的形式呈现在我们面前，再或者直接用语言表达：这个楼背靠青山，那个楼面朝大海。这些模型能告诉我们哪些楼有哪些特点，让我们不用亲临现场就能明白住进去后的基本感受。同样地，科学家们说是在研究自然，实际上大部分的工作都是在观察，以及建立模型来描述研究对象，最后再基于模型的研究结果去改造这个世界。而那些带有令人望而生畏的符号和公式的数学模型与生物学中心法则的描述的差别在于，前者用的是数学语言，后者用的是自然语言。要了解数学模型，首先就要了解其基本的功能：第一，整合信息，根据所研究的问题综合现有的信息对研究对象进行系统地描述。第二，预测，通过对模型中的参数和环境条件的扰动可预测系统在宏观层面的表现和变化。第三，提出可验证的假说。 模型在生物学研究中的必要性经过过去几百年的观察和研究，人类越来越意识到生物学系统的复杂性。由于技术和资源的限制，大量的研究都是局限于特定过程的离散的研究。这些离散的研究让我们获得了对特定生命过程的了解，并建立了生物学模型。然而，在实际的生命体中，这些生命过程往往相互联系并交织成一张极为复杂的网络，很多现象并不能直观地根据已有的法则和原理所解释，也不能根据经验进行可靠地预测。即使现在我们能够轻易获得海量的组学数据，但是当面对如此复杂的网络时，我们也几乎无法对其进行人为的归纳和演绎。因此，数学建模结合计算机模拟应运而生。它们能够整合这些网络中的主要原件和内在互作过程进行模拟仿真，从而帮助我们理解复杂网络中的扰动对系统表现的影响。通过这样的方式，我们知道了，生命过程是遵循一些共性或者说是规律而自发涌现出来的一种状态。 模型构建的原则和方法数学模型包括两个基本要素：结构和参数。说白了，结构就是关系，就是所研究的问题中包含的要素（转录因子，基因，酶等等）和要素之间的相互作用与联系（如化学计量学比例，事件发生先后，调控等等）。而每一种要素的状态变化及其相互作用都需要用数学表达式来描述。建模的具体步骤如图1所示。图1. 数学建模的关键步骤下面将通过一个简单的例子来说明建模的必要性和过程（本例子也许不是最合适的，但却能够简单描述建模的过程及意义）：（1）确立研究目标：A同学研究一个酶促反应的反应特征，并希望通过构建反应速率与底物浓度之间的模型，利用模型预测来指导工程改造酶的特征，以更好地得到产物B。 S→B （2）收集信息：根据前人研究发现，该反应服从米氏动力学特征的不可逆反应。故可以用米氏动力学方程来描述这个反应过程。并且文献记载该酶的最大催化速率V1为2，米氏常数Km1为2。（3）建立模型： v=V*[S]/(Km+[S] ) 其中，[S]表示底物浓度；V表示目标酶在特定条件下的最大催化速率；Km表示米氏常数（最大反应速率一半时底物的浓度）；v表示因变量，即在底物S的浓度为[S]时对应的实际反应速率。（4）模型的参数化及验证：将V1 = 2，Km1 = 2作为参数输入模型，在所有浓度范围内，均可计算出该反应的实际反应速率（图2蓝线所示）。并且文献查找获得了一系列底物浓度条件下该反应的反应速率数据（图2中黑色实心点）。（5）模型预测并提出假设：通过改变模型中参数V和Km比较反应速率与底物浓度之间的关系，发现改变V时改变了该反应的最大催化能力，而改变Km时改变了催化过程对底物的敏感度。提出假设：在底物浓度高的反应器中需要增加该酶的量，而在反应物浓度比较低的反应器中需要改变酶对底物的敏感性！（6）用实验构建分别改变V和Km的酶，测量底物与反应速率之间的关系，如果能和预测结果吻合，说明该模型对于需要研究的目标问题具有一定的预测能力，并且用于实际的化工合成或者生物合成；否则，需要重新建模或者补充信息，直至能吻合为止。图2. 反应速率与反应物S的浓度在不同特征酶催化条件下的关系。图中的蓝线表示当前条件下模型模拟的关系趋势线，其余的线条表示模型预测改变酶特征后可能的关系；黑色实心圆点表示实验测量在当前酶特征条件下，不同底物浓度时对应的反应速率；彩色实心椭圆点表示改造后最大反应速率的一半以及对应的底物所需浓度。南京诺维赞南京诺唯赞生物科技有限公司]]></content>
      <categories>
        <category>Bio Cooking</category>
      </categories>
      <tags>
        <tag>dynamic model</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ChIP-seq实验以及分析总结(上篇)]]></title>
    <url>%2F2019%2F04%2F01%2Fchip-seq1%2F</url>
    <content type="text"><![CDATA[原创：丸子 宇宙实验媛 由于基因表达调控机制的复杂性，从不同的层面探究生物问题越来越重要，因此需要我们对多种组学数据的整合分析。从RNA-Seq层面，我们可以探究哪些基因具有显著差异，上调或下调；但是想进一步探究调控某一生物学过程的关键因子（包括顺式调控元件和转录因子），以及哪个转录因子调控了感兴趣的基因，需要结合ChIP-Seq来分析。从ChIP-Seq层面，我们可以研究某个特定转录因子的调控作用，以及调控区的组蛋白修饰等。今天，我们就来学习下ChIP-Seq的实验部分，之后的推送也会为您献上ChIP-Seq的数据分析。 概念染色质免疫共沉淀技术(Chromatin Immunoprecipitation,ChIP)是一种用于研究蛋白质DNA的体内相互作用的经典实验技术。采用特异性抗体将目的蛋白进行免疫沉淀，由此可以把目的蛋白所结合的基因组DNA片段也富集下来。通过与高通量测序技术的结合，对ChIP后的DNA产物进行测序分析，从全基因组范围内寻找目的蛋白的DNA结合位点，以高效率的测序手段得到高通量的数据结果。ENCODE数据总览 技术原理在生理状态下，把细胞内的DNA与蛋白质交联（Crosslink）后裂解细胞，分离染色体，通过超声或酶处理将染色质随机切割； 利用抗原抗体的特异性识别反应，将与目的蛋白相结合的DNA片段沉淀下来； 再通过反交联（Reverse crosslink）释放结合蛋白的DNA片段； 纯化； 测序获得DNA片段的序列，最后将这些DNA片段比对到对应的参考基因组上。ChIP实验原理 应用领域以及技术优势由于 ChIP-Seq 的数据是 DNA 测序的结果，为研究者提供了进一步深度挖掘生物信息的资源，研究者可以在以下几方面展开研究：（1）判断 DNA 链的某一特定位置会出现何种组蛋白修饰； （2）检测 RNA polymerase II 及其它反式因子在基因组上结合位点的精确定位；（3）研究组蛋白共价修饰与基因表达的关系； （4）转录因子研究。ChIP-Seq能够在全基因范围内捕获转录因子或者表观修饰标记结合的目标DNA，鉴定转录因子结合位点，揭示基因调控网络，并且适合多种多样的样本。表观遗传修饰与转录调控 实验流程（1）甲醛处理细胞，使DNA-protein的相互结合作用被交联固定（2）裂解细胞，得到全细胞的裂解液（3）超声处理或者用限制性内切酶处理，将基因组DNA打断至100-500bp（4）抗体免疫沉淀，在细胞裂解液中加入一抗和beads，进行孵育（5）采用合适的实验条件进行洗脱，并进行解交联（6）通过qPCR对ChIP结果进行验证（7）准备好的ChIP后的DNA样品用于ChIP-Seq建库ChIP实验流程 建库流程（1）DNA片段末端修复（2）3’端加A碱基（3）连接测序接头（4）PCR扩增及DNA产物片段大小选择（一般为100-300bp，包括接头序列在内）想要了解详细步骤，请参考诺唯赞公司VAHTS™ Universal DNA Library Prep Kit for Illumina® V3。VAHTS™ Universal DNA Library Prep Kit for Illumina® V3建库原理以及流程VAHTS™ Universal DNA Library Prep Kit for Illumina® V3建库&nbsp;Input DNA量最低可至100 pg，且DNA片段末端修复&amp;加A尾，一步完成。经过了严格的质量控制和功能验证，最大程度上保证了文库构建的稳定性和重复性。 注意事项 10^6~10^7 个细胞才能保证最终得到10到100ng ChIP DNA。一般10^6可以满足高丰度蛋白（如RNA polymerase II）和局部组蛋白修饰（如H3K4me3）的ChIP。如果是低丰度的转录因子蛋白和其他组蛋白修饰则需要10^7个细胞。 超声处理在含有SDS的缓冲液中可能会破坏蛋白质－蛋白质和蛋白质－DNA相互作用。但是含有SDS的缓冲液能增加超声的效率，适应与DNA紧密结合的转录因子的ChIP-Seq，如果结合较弱的话不推荐使用加SDS的缓冲液。 ChIP 样品中如含有明显的蛋白质或离子浓度过高或其它杂质污染，可能会使库检时 2100峰图异常，对建库过程中的酶反应产生影响，导致建库失败。建议在完成ChIP实验后，选择某一已知的阳性DNA结合区域设计Q-PCR实验，由此验证ChIP实验的可靠性。但此建议不适合于没有阳性对照序列的ChIP实验。 IgG通常pull down非常少的DNA，这样导致在后期的建库过程中PCR Cycles 数增加，导致不能达到作为control去除背景噪音的目的（会缺失和放大部分信息）。因此比较而言，Input更适合作为control。建议用不同公司的抗体来做生物重复，以避免抗体导致的结果差异，保证结果的准确性。参考文献1.Jothi et al. (2008) Genome-wide identification of in vivo protein–DNA binding sites from ChIP-seq data. Nucleic Acids Res 36(16) 5221–5231.2.Bernstein, BE; et al. (2005). “Genomic maps and comparative analysis of histone modifications in human and mouse”. Cell. 120: 169–181.3.Johnson, DS; Mortazavi, A; et al. (2007). “Genome-wide mapping of in vivo protein–DNA interactions”. Science. 316: 1497–1502.]]></content>
      <categories>
        <category>Bio Cooking</category>
      </categories>
      <tags>
        <tag>chip-seq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈期刊筛选与文章投稿-上篇]]></title>
    <url>%2F2019%2F03%2F31%2Fpaper-writing01%2F</url>
    <content type="text"><![CDATA[原创： 伊颜落芸 宇宙实验媛 1月3日 随着近十年生物医学科技的迅猛发展，SCI文章的在线发表量以及PubMed等主流数据库对文章的收录数呈指数型增长；尤其是伴随着几大出版集团&gt;的合并重组，开源期刊（OA）的流行，科学研究者越来越重视成果发表的时效性、深刻度和影响力。笔者也参与过多篇SCI论文发表，现就期刊筛选&gt;和论文发表的流程分享一些心得体会，由于篇幅限制，将分为两期讨论，请读者持续关注。 编者按随着近十年生物医学科技的迅猛发展，SCI文章的在线发表量以及PubMed等主流数据库对文章的收录数呈指数型增长；尤其是伴随着几大出版集团的合并重组，开源期刊（OA）的流行，科学研究者越来越重视成果发表的时效性、深刻度和影响力。笔者也参与过多篇SCI论文发表，现就期刊筛选和论文发表的流程分享一些心得体会，由于篇幅限制，将分为两期讨论，请读者持续关注。 期刊筛选在进行目标杂志的筛选和确定时，采取以下几个核心的原则：1.综合考虑期刊影响力和近５年平均影响因子：发CNS是生物医学研究者的最高目标，它们也是影响力最高的综合性期刊，无论从数据的质量、完整度，科学问题的选择和语言撰写方面都是一流的。取法乎上，得乎其中，取法乎中，得乎其下。我们追求的不是发表CNS的结果，而应该在数据呈现形式，实验设计的角度以及英文文章的撰写这三个方面尽可能高标准看齐。建议遵循小领域期刊&gt;泛领域期刊&gt;综合性期刊。例如一篇研究临床心血管致病机制和临床相关性的文章，选择层级是： JACC&gt; JCI/JEM&gt;Nature Communications/Science Advances。在这些层级范围内，方向的聚焦性逐层递减，而科学问题的接受度和广阔性逐层增加，显而易见反映到次要指标IF上也有所体现。另一个重要的指标是五年平均影响因子 受市场因素和编辑部综合决策影响，IF值在2-3年内存在波动较为常见，而五年的时间范围内，能够尽可能排除非学术因素的影响，像Elsevier旗下的Cell系列子刊和Nature旗下子刊，在相当长的周期内，IF的波动都不会太明显，而同出版集团的有些OA杂志可能有比较明显的波动，特别是显著下跌的期刊，存在着大量灌水的可能，比如Cellular Signalling, Stem Cell Reports这几年影响因子下跌，发文量也明显增多，还有已经被列入黑名单和准黑名单的scientific reports，tumor biology以及oncotarget等杂志。2.综合考虑期刊发文量和审稿周期从出版周期上看，一般期刊分为周刊，双周刊，月刊，季刊。有的杂志只有电子版没有纸质版，比如Nature Communications, Scientific Reports等大部分open access期刊，这类期刊的发文量比较多，这给了我们研究该期刊文章风格的参考性，同时也增大了被录用的概率。由于文章投稿人和in press的期刊之间供需关系不平衡，可能存在审稿周期慢的情况，通常在文章的核心论点突出，基本证据链完备的情况下就应该尽早考虑期刊的筛选和布局。因为一旦进入审稿周期，就是编辑方为主的考量，我们虽然可以发站内信催促，但是主动权还是不在我们。3.兼顾考虑期刊编辑友好度及与本研究相关度须知：建立在相当程度的文献阅读基础上，我们才能知道国内外同行的关注度和竞争性。比如研究XX蛋白质的翻译后修饰在某种肿瘤代谢疾病模型中的应用。 第一步，文献查对我们需要知道国内外研究该基因的实验室和课题组有哪些？他们对该基因蛋白质翻译后修饰的研究在哪个层面，特别是首次报道的某种修饰及其调控模式的团队或相关成员，例如近年来关注较多的细胞能量代谢与乙酰化修饰，现在研究系统性较高、占比量较大的工作基本来自于Choudhary、Matthias 团队和华人教授管坤良、熊跃团队，既有蛋白质组学层面的全谱研究【1,2】，又有单基因层面的机制功能研究【3-5】。知悉这些，我们才能大致知道需要关注哪些学者，其基本观点和研究出发点等, 以其投稿结果和期刊类型为参照制定我们的投稿目标；第二步，前审稿阶段 类似各种基金的小同行评审，最好能给了解你工作的相关教授审阅并提出文章架构和数据完整度方面的意见，这是十分必要的，毕竟在实验研究中我们要保持思维方式的独立性，而在投稿过程中需要依赖于导师的学术判断和投稿经验。这一方面取决于之前成功发表的期刊编辑团队是否青睐于本团队所在方向的工作，另一方面取决于拟投稿文章和该期刊近期相关研究方向的工作是否具有研究点的深入和研究面的开发，并在投稿内容中突出和强化这一点；第三步，热点分析 利用Web of Science 或者 基于perl的网络爬虫方法，可以对相关领域内的发文量，以及资助情况进行分析总结。以热点的研究方向Crispr-Cas9介导的基因编辑为例。可见资助情况和发文情况在相关领域的关注度逐年提升。提示我们在投稿杂志选择方面，近3-5年侧重交叉热点整合的文章可能会另编辑更感兴趣。4.兼顾考虑版面费与是否为开源期刊考虑到科研经费的有限性和时效性，在保证发表期刊质量的前提下，版面费也必须具有高性价比。通常来讲，高版面费的杂志很多是开源期刊，相关文章水平参差不齐的几率更大，所以我们必须综合考虑这些因素并珍惜每个目标期刊的初次投稿机会。 参考文献： Choudhary, C. et al. Lysine acetylation targets protein complexes and co-regulates major cellular functions. Science 325, 834-840 (2009). Zhao, S. et al. Regulation of cellular metabolism by protein lysine acetylation. Science327, 1000-1004 (2010). Zhao, D. et al. Lysine-5 acetylation negatively regulates lactate dehydrogenase A and is decreased in pancreatic cancer. Cancer cell 23, 464-476 (2013). Yang, H.B. et al. Acetylation of MAT IIalpha represses tumour cell growth and is decreased in human hepatocellular cancer. Nature communications 6, 6973 (2015).5.Saito, M. et al. Acetylation of intrinsically disordered regions regulates phase separation. Nature chemical biology 15, 51-61 (2019).这期的总结就到这里，欢迎各位关注者们的分享和讨论。这期重在理论经验，下次将着重就投稿细节展开讨论。]]></content>
      <categories>
        <category>Paper Writing</category>
      </categories>
      <tags>
        <tag>paper writing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[师兄又在胡扯了-Western Blot]]></title>
    <url>%2F2019%2F03%2F31%2Fwestern-blot%2F</url>
    <content type="text"><![CDATA[大家好，阿呆师兄又出现了。不知道上个月说的几本书有几个人看了或准备看的。其实原本我应该深藏功与名，大隐隐于市了，不知道大家还记不记得短腿师妹前阵子说“宇宙实验媛喊你来拿三位数稿费了”阿呆 宇宙实验媛大家好，阿呆师兄又出现了。不知道上个月说的几本书有几个人看了或准备看的。其实原本我应该深藏功与名，大隐隐于市了，不知道大家还记不记得短腿师妹前阵子说“宇宙实验媛喊你来拿三位数稿费了”一开始在考虑要不要投稿的时候，其实我是拒绝的。因为，你不能让我投，我就马上去投，第一我要酝酿一下，因为我不愿意投完了以后再加一些特技上去，稿子“咣”一下，很亮、很炫，这样观众出来一定会骂我，根本没有内容的稿子，就证明是个假的。后来发生了一件惨绝人寰的事情……是的，我的肾6摔了个脸着地。当时第一反应就是，啊！又是一笔开销啊！！手里没粮，心里很慌！于是我就来水了……除了短腿师妹变成了短腿老板外，其他也没什么嘛……（凑表脸）可是写啥呢，大家知道短腿他们偏干实验，而我是个纯种湿实验员，没有干货只会浑水摸鱼……直到有一次和师妹聊天提到Western Blot（WB）。好吧我就来试试看写这个，虽然我觉得可能没有观众愿意看这些……毕竟作为一个生物狗，不可能不知道WB啊！我打开了某度，输入“Western Blot”去掉最上面的广告，大家可以看到，确实是随便找找一大把，这还需要我写？就随便点开那个“这一篇就足够了!Western blot详细步骤与经验交流”看下。Emmm……写得蛮清楚的嘛……（唉咦，谁扔的番茄？）好吧好吧，如果你跟着我点进去看了，会发现这个网页里的内容确实基础地描述了WB的过程。但我知道各位看官一定是不会满足于这些。来来反正你都打开网页了，不如点到某度文库里去，请输入“Western_Blot详解及问题分析”看见那个53页的PPT没？没有下载券就和我一样在线看吧，把它仔细看完，WB的理论知识可以说已经掌握了。如果你是个WB小白，现在已经升级成WB王语嫣；如果已经有点经验了，那现在有没有一种通透了的感觉。然而上手做起来依然会有各种问题。很正常啊，不然怎么会有那么多WB技术求教贴、经验分享贴一直在冒出来？做实验不会不知道丁香园论坛的吧，emmm…对对就是那个临床的丁香园。去丁香园论坛搜索“Western Blot”我只是随便截图的，各种技术问题几乎都能在上面找到，毕竟你不是第一个吃螃蟹的人。当新手光环还在的时候，一般都会觉得WB果然是个基础实验。当光环褪去的时候，就知道自己连个基础都没有。当然了，有些人可能光环的时间比较长，有些人可能……没有光环……不记得谁说的选择比努力重要，在WB里真是体现的淋漓尽致。上样前你得选合适浓度与孔数的胶，（取决于目标蛋白大小和上样体积）；run起来的时间不能太长也不能太短（还是大小）；转起来除了选时间，先要选择转膜方式和膜的种类（蛋白大小、结果显影还是扫描）；封闭你是选奶粉还是BSA（搞不搞磷酸化）；孵个一抗简直就是WB的灵魂啊没选好全都白辛苦（这个靠try啊）；准备化学发光呢还是荧光……哪一条没选好，努力产生的结果大概率是垃圾（戾气了，要“inner peace”……呼）。如果对WB有点情怀，可以看看丁香园的那篇帖子“【建议】穷人的劳斯莱斯-我的五年western blot体会”老人大概会有点感触吧。好了，就不多说了，字数凑够了，拿去换点米……授人以鱼不如授人以渔嘛，是吧…（还想要鱼？没看见前面写的么，跟我这个浑水摸鱼的人要鱼？）好吧，来个太长不看版吧：去看文库的ppt，不会的朋友当科普，会点的朋友当详细复习，万一醍醐灌顶了呢。动手问题去丁香园，生物秀啥的也行，平时多看看抗体说明书也会有惊喜。反正我就是这么过来的……想苦练WB技艺的朋友可能会对此篇不满吧，哎朋友啊你是不知道，其实我是在帮你啊……贴一个全自动WB仪器的视频若干年后：“在岗位上竞争不过机器是种怎样啊体验”……瑟瑟发抖.jpg（所有图片均来源于网页截图或自己手机截图）]]></content>
      <categories>
        <category>Bio Cooking</category>
      </categories>
      <tags>
        <tag>western blot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习数据分析极简思路及sklearn算法小试]]></title>
    <url>%2F2019%2F03%2F27%2FMachine-learnig-KFC%2F</url>
    <content type="text"><![CDATA[原创：老板，来一打TPU 机器学习拥有庞大的知识体系，这里对机器学习的数据分析的整个思路和流程作最最简单的归纳。机器学习的步骤大致包括：1）理解和清理数据2）特征选择3）算法建模4）测试评估模型 机器学习数据分析极简思路1）理解和清理数据 理解数据数据是机器学习大餐的原始食材，对数据分析起着至关重要的作用，理解原始数据的含义将有助于进一步分析。例如，甲基化图谱与年龄有着显著的相关性，而与性别关系不大，因此在数据分析中，对这两个特征（faeature）需要区别对待。更好的理解方式是直接可视化某些数据，例如对于经典的鸢尾花数据集，可以通过python seaborn绘图包可视化各个特征（feature）之间的关系；对于大数据，则可以进行降维分析（PCOA、tSNE），理解数据组成主成分贡献度。 #pip install seaborn import matplotlib.pyplot as pyplot import seaborn as sb import pandas as pd %matplotlib inline data=pd.read_csv('iris.csv') #pandas 读入数据 data.head(3) #查看数据 data.describe() #数据基本统计 sb.pairplot(data.dropna(),hue='Species') 鸢尾花数据组成鸢尾花数据所有feature基本数理统计鸢尾花数据不同feature相互关系 剔除异常值清理数据的目的在于去除原始数据中的异常值和想办法处理缺失值，我们拿到手上的数据不可能尽善尽美，总有一些妖孽作祟，对于异常值我们应当剔除。举个栗子，假设在鸢尾花数据集中，有一个样本显示鸢尾花花瓣长度10m，其他诸如花瓣宽度、花萼长宽值都正常，可以脑补一下这是一朵什么样的花，那么这个样本显然应该剔除。 处理缺失值缺失值在数据分析中很常见，总有一些样本观测值会因为这那的问题缺失，处理缺失值如果样本数量很大，而包含缺失值的样本又少，这个时候果断去掉这些样本，眼不见为净；如果因为样本有限或者缺失值太多，就要想办法补全缺失值（imputation）,常常利用逻辑回归建立模型，找出这些数据变化的规律，从而预测缺失值。如何合理推断和填回这些缺失值是一门大学问，哪种方法好，我也不敢妄言。 2）特征选择工业界广泛流传的一句话是：数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。由此可见，特征选择是机器学习的关键的关键。特征选择目的在于提取跟目标最为有效的信息，降低数据维度和计算成本，同时防止过拟合（overfitting，训练集特征用的太多太细，以致于在测试集中不适）。要知道，并不是特征越多，结果越好，没有严格意义上的特征累加效应，有时候好的几个特征胜于大量零碎的特征取得的效果。在许多大数据挖掘竞赛中（国内的阿里天池和国外的kaggle平台），最复杂的过程莫过于特征工程建立阶段，大概占据了整个竞赛过程的70%的时间和精力，最终建立的模型的好坏大多也取决于特征工程建立的好坏。遗憾的是，特征工程不像模型建立的过程有着固定的套路，特征工程的建立凭借的更多的是经验，因此没有统一的方法。这里抛砖引玉介绍一些常见的办法，更为详细的内容请参考文后链接。a)特征过滤法比较简单，它按照特征的发散性或者相关性指标对各个特征进行评分，设定评分阈值或者待选择阈值的个数，选择合适特征。例如，我们可以简单的计算出每个feature的方差，方差越大说明这个feature在样本中变异大，即有区分性；而越小的（极端时方差为0），即表示在所有样本中一样，特征选择时则可不考虑这些特征。我们可以选择方差最大的前n个feature用于建模，这就是最为简单的方差筛选法。b)包装法根据目标函数，通常是预测效果评分，每次选择部分特征，或者排除部分特征。c)嵌入法则稍微复杂一点，它先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据权值系数从大到小来选择特征。类似于过滤法，但是它是通过机器学习训练来确定特征的优劣，而不是直接从特征的一些统计学指标来确定特征的优劣。踩雷说：特征选择之后，需要从原始数据矩阵提取相应的特征重构矩阵，那么训练集和测试集的特征经过你各种变换之后，应当保持一一对应，类别和顺序在矩阵中都应该一致。目前，已经有一些套路化的特征选择工具，例如python的FeatureSlector包，见链接。 3）算法建模针对具体的问题，是分类问题？回归问题？还是其他？选择合适的模型，或者使用集成的算法模型。常见的算法模型包括：对于回归问题：a)线性回归（回归，LinearRegression）b)岭回归（回归，Ridge）Ridge是线性回归加L2正则平方，以防止过拟合c)拉索回归（Lasso），加入惩罚函数L1正则绝对值，防止过拟合d)弹性网络回归（回归），同时使用L1和L2正则。e)K近邻（回归和分类，KNeighborsRegressor）f)决策树（回归和分类，DecisionTreeRegressor）g)支持向量机（回归和分类，SVR） 对于分类问题：a)支持向量机（回归和分类，SVC）b决策树（回归与分类,DecisionTreeClassifier）c)逻辑回归（分类,LogisticRegression)d)LDA线性判别分析（分类,LinearDiscriminantAnalysis)e)K近邻（分类,KNeighborsClassifier)值得一提的是无论是分类还是回归问题，基于决策树和SVM的算法都有比较好的表现。 4）测试评估模型测试评估模型的目的在于，解决模型的欠拟合（under-fitting）和过拟合（over-fitting）问题，通过即时的反馈不断调整模型、优化模型，使得模型更加稳健。实际上，测试评估模型应该在你建模之前就考虑，例如是否需要设置纯粹的外部数据验证集，若没有这样的数据，你怎样划分数据进行建模预测？在实际训练中，模型通常对训练数据好，但是对训练数据之外的数据拟合程度差。用于评价模型的泛化能力（即模型普适性）。交叉验证的基本思想是把在某种意义下将原始数据进行分组,一部分做为训练集(train set),另一部分做为验证集(validation set or test set),首先用训练集对模型进行训练,再利用验证集来测试模型的泛化误差。另外，现实中数据总是有限的，为了对数据形成重用，比较常用的是k-fold交叉验证法。测试评估模型的时候，常常结合AUC曲线判断模型好坏。 sklearn 算法小试实现目的sklearn是python中一个强大的机器学习模块,拥有众多的机器学习算法和功能。这里，通过sklearn的datasets构建一个数据集，并用4种常用算法：逻辑回归（LogisticRegression）、支持向量机（SVM）、决策树（DecisionTree）和集成算法（VotingClassifier）对训练集建模，然后对测试集预测，最终通过得分看一下4种算法的差异。 步骤1）构建本次使用的数据集2）将数据拆分成训练集和测试集3）用4种算法分别建模、预测 代码环境python版本：python31）如果不想被python各种安装包困扰，推荐Jupyter在线python，Jupyter官网，点击”Try Jupyter with Python”，点击“+”号即可。安装包的时候直接pip install packages_name,例如pip install sklearn,点击“Run”，提示“Successfully installed sklearn-0.0”即安装完成。2）Pycharm，专业、高效、强大的python开发端。 源码 import numpy as np import matplotlib.pyplot as plt from sklearn import datasets #built-in datasets #make_moons,generated datasets X,y = datasets.make_moons(n_samples=500,noise=0.3,random_state=42) plt.scatter(X[y==0,0],X[y==0,1]) plt.scatter(X[y==1,0],X[y==1,1]) plt.show() #plot for datasets #split datasets for train and test part from sklearn.model_selection import train_test_split X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42) #1.logistic regression model from sklearn.linear_model import LogisticRegression log_clf=LogisticRegression() #create LR classifer log_clf.fit(X_train,y_train) #train and fit the model log_score=log_clf.score(X_test,y_test) #test the model #2.svm model from sklearn.svm import SVC svm_clf=SVC() svm_clf.fit(X_train,y_train) svm_score=svm_clf.score(X_test,y_test) #3.decision tree model from sklearn.tree import DecisionTreeClassifier dt_clf=DecisionTreeClassifier() dt_clf.fit(X_train,y_train) dt_score=dt_clf.score(X_test,y_test) #4.ensemble method from sklearn.ensemble import VotingClassifier voting_clf=VotingClassifier(estimators= [('log_clf',LogisticRegression()), ('svm_clf',SVC()), ('dt_clf',DecisionTreeClassifier()) ],voting="hard") voting_clf.fit(X_train,y_train) voting_score=voting_clf.score(X_test,y_test) print("log score:%s"%log_score) print("svm score:%s"%svm_score) print("dt score:%s"%dt_score) print("voting score:%s"%voting_score) 构建数据集：4种算法预测结果：可以看到，单一算法SVM比较好，集成算法较单一的算法还是有一定的提高。 参考机器学习中，有哪些特征选择的工程方法？FeatureSlector:一个可以进行机器学习特征选择的python工具机器学习中的交叉验证]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UBGBs]]></title>
    <url>%2F2019%2F03%2F20%2FUBGBs%2F</url>
    <content type="text"><![CDATA[Yeap,Family!]]></content>
      <categories>
        <category>Photo</category>
      </categories>
      <tags>
        <tag>photo</tag>
        <tag>family</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈科研论文发表与投稿-下篇]]></title>
    <url>%2F2019%2F03%2F19%2Fpaper-submit-experience%2F</url>
    <content type="text"><![CDATA[原创： 伊颜落芸 宇宙实验媛生物医学科学研究的过程犹如马拉松长跑，投稿的经历更是具有西天取经般的朝圣感。上线发表的文章只是最后的工作总结之一，背后的故事&gt;往往更精彩。 上篇我们已经就期刊筛选分享了初步经验，本期将以发表在Nature Communications中题为Znhit1 controls intestinal stem cell maintenance by regulating H2A.Z incorporation的工作为例，就论文投稿和回修过程论述心得体会，便于读者熟悉投稿的关键环节并规避常见误区。 Nature Communications 杂志作为NPG旗下开源期刊，近几年不光收录文章数量质量上升，交互性功能也日趋开放。值得一提的是，大多数文章上传了同行评审（peer review process）的记录，使读者在欣赏高水平研究工作的同时，也能全面了解到文章的逻辑性提升过程和实验证据完善的过程。 这篇文章讲述的是SRCAP染色质重塑复合体的重要组分—锌指蛋白Znhit1，通过促进组蛋白变异体H2A.Z整合到和Lgr5+肠道上皮细胞命运决定相关基因的TSS区域，增强其分子伴侣YL1的相互作用和磷酸化，进而维持Lgr5+细胞的干性稳态并促进小肠上皮平衡的故事。结合peer review file。我们凝练出以下五个投稿过程要点： 1. 初次投稿注意事项 首先要根据拟投稿的期刊，认真阅读投稿指南，确认正文材料和补充材料在形式和内容上符合杂志社的要求。如果能在初次投稿时候准备完善，将给编辑和审稿人工作可信度高和完成度高的良好印象。即使是同一个出版社的不同子刊也可能在这些方面存在很大差异。以NPG集团为例：主刊和经典子刊，uncropped scans 和 reportingsummary （图1）应该在数据的整理阶段完成，后者的统计学分析方法，原始生物信息的溯源，和其他重要实验方法的陈述亦可辅助文章的撰稿过程。这些细节也是审稿人评价本工作严谨性和完成度的重要层面，因此不可掉以轻心。该集团其余代表性刊物ncogene、Cell Death andDsease虽然要求不同，但是如能秉承“取法乎上”的原则一定能得到好的效果。 其次的重点就是Cover Letter了，好的CV应该观点突出，逻辑清楚。通常都是体现“发现了什么”，“几大创新点”这样的逻辑思路，当然也可根据投稿杂志的领域类型和刊物亚类进行相应调整。如果一份cover letter不能吸引到审稿人，就像招聘时简历不能得到HR的青睐，由此摘要的部分可能也不够凝练突出。总结起来这两个部分就是功夫在平时，投稿再加强。 2. 如何评价初次回修审稿意见 从这篇文章的投稿历程看，初次投稿时间2017.07.07，收稿日期为2019.02.15。虽然不能轻易评价该工作在初次投稿时与杂志社的投稿标准间的差距和差异，但从审稿人提出的问题来看，原始版本还是存在很多细节问题和可提升空间。可以总结为以下几个层面：a. 检测指标：审稿人认为需要关注Znhit1在小肠上皮中的表达模式，才能更好的确认Znhit1对肠稳态的调控是依赖对小肠上皮干细胞的调节实现，而非通过goblet细胞或肠内分泌细胞；b. 评价指标：审稿人认为单独通过Lgr5+ISC的缺失不足以评价小肠上皮的稳态失衡，毕竟还有+4 Bmi1+ 细胞、Dll1+分泌前体细胞、Alpi+肠上皮前体细胞发挥的代偿效应；c. 模型选择：审稿人认为经典的表征Lgr5+的小肠干细胞所选用的工具鼠模型还应包括Lgr5-EGFP-IRES-creERT，作者选择的模型Olfm4-CreER似乎并不是一个最佳的标记指标。d. 开放问题:审稿人认为作者提到的Znhit1调控yl-1磷酸化的机制不足从机制上揭示Znhit1对Lgr5+干细胞的调控，需要提供更多的修饰启动者，修饰位点，修饰模式和结合区域等的详细信息。虽然不同的文章研究内容迥异，但是审稿人提出问题的架构有迹可循。这篇文章在初次投稿时获得了3个审稿人的评审意见，其中a.c.d三点在多个审稿人处提及，结合这篇文章的审稿意见和笔者自身的投稿经历，可总结为：共性问题着重回答，开放问题难点攻破，合理质疑引用文献，数据highlight，remark中关注态度。 3. 写好回修信 在对审稿意见进行客观全面的分析+针对性的实验补充后，回修信的撰写是对文章调整进步的总结。核心原则有四：1) 点对点 作者对所有调整的图片内容进行了详细的梳理和总结（图2）；2) 全面性 不论该问题出现在审稿人的综合评述中还是单独的major/minor concerns，建议均进行必要的回复；3) 不卑不亢 审稿人提出的质疑及其佐证的参考文献，如能丰富文章架构和纠正偏颇，应该加以肯定；如果原有数据与提出质疑一致，也可以rebuttal，但前期是基于对所指文献和观点的充分了解和对观点原创者的深入认知，否则还是不要班门弄斧；4) 忌画蛇添足 这一点在回答开放问题时尤其注意。 4. 多轮回修的心理和技术储备 文章一旦踏上投稿过程，便是开弓没有回头箭之感，多轮回修更是司空见惯。在这个过程中需要积极的寻求相关领域专家专业意见和导师的讨论指点，寻求合作来弥补技术的短板；需要补实验小分队相互沟通缓解疲乏和压力。或许成为一个杰出的科学研究工作者，最重要的是心态，其次才是技术，一篇篇的文章是提升和总结也是修行，切不可偏执。 5. 正式发表前的材料准备 编辑部的主体接受或接收函到手并不能马上庆祝最后的胜利，还需按照要求重点准备各项材料。原始图片：尤其在分辨率和图片的格式要求上进行二次审核；统计学方法的选择，代表性图片的呈现是否符合投稿要求；文字的表示和单词拼写是否有错误。特别注意：图片的信息标注要反复检查一遍，这是最容易出现疏漏的环节。 最后祝大家都能享受充实的投稿过程！！科研论文写作与投稿-上篇]]></content>
      <categories>
        <category>Paper Writing</category>
      </categories>
      <tags>
        <tag>paper writing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Date with Bioawk]]></title>
    <url>%2F2019%2F03%2F16%2FDate-with-Bioawk%2F</url>
    <content type="text"><![CDATA[原创：老板，来一打TPU 废话这是生平第一篇blog，因此难免要说点废话，所以前戏，啊不对，前言就叫废话吧。git+hexo搭建的平台杠杠的，感谢师弟MingLian小大佬成功带入写blog的坑。第一篇就拿生信李恒大神的神作bioawk镇楼吧！** bioawk简介除了鼎鼎大名的BWA、samtools等，李恒大神应群众呼声又做了出于awk又胜于awk的bioawk供广大生信猿们玩耍。bioawk是用C写的，用法和awk基本一样。 安装分步安装 $ sudo apt-get install byacc git #安装git $ git clone git://github.com/lh3/bioawk.git #默认安装在当前目录 git clone git://github.com/lh3/bioawk.git ${path}/bioawk #若加path则必须为空，因此可在想要安装的目录下起一个新名称，git会clone到其下 $ cd bioawk $ make $ echo "export PATH=${path}/bioawk/:\${PATH}" >> ~/.bashrc $ . ~/.bashrc ### 一步安装脚本 #!/bin/bash read -p 'Input installed path: ' tmp_path path=${tmp_path/\//} if [ ! -d "${path}" ];then echo "No such file:${path}" &amp;&amp; exit ;fi git clone git://github.com/lh3/bioawk.git ${path}/bioawk &amp;&amp; \ cd ${path}/bioawk &amp;&amp; make &amp;&amp; \ echo "export PATH=${path}/bioawk/:\${PATH}" >> ~/.bashrc &amp;&amp; . ~/.bashrc &amp;&amp; \ echo 'Successfully install bioawk!' || echo 'Failed install bioawk' 使用bioawk基本思想是把组成不同类型的文件（sam、bam、fasta、fastq、vcf etc）的基本元素封装成变量，直接调用即可。 语法usage: bioawk [-F fs] [-v var=value] [-c fmt] [-tH] [-f progfile | 'prog'] [file ...] -c 支持输入文件格式，查看帮助： bioawk -c -h bed: 1:chrom 2:start 3:end 4:name 5:score 6:strand 7:thickstart 8:thickend 9:rgb 10:blockcount 11:blocksizes 12:blockstarts sam: 1:qname 2:flag 3:rname 4:pos 5:mapq 6:cigar 7:rnext 8:pnext 9:tlen 10:seq 11:qual vcf: 1:chrom 2:pos 3:id 4:ref 5:alt 6:qual 7:filter 8:info gff: 1:seqname 2:source 3:feature 4:start 5:end 6:score 7:strand 8:frame 9:attribute fastx: 1:name 2:seq 3:qual 4:comment 上面出现的名称即可引用其变量 实例 打印fasta序列ID、序列、长度、GC含量 bioawk -cfastx '{print "ID: "$name"\tlength: "length($seq)"\tGC: "gc($seq)"\t"$seq}' demo.fa 结果: ID: g1 length: 18 GC: 0.722222 atcccccccccccccttt ID: g2 length: 26 GC: 0.0769231 cattatatcttatttttttttttttt ID: g3 length: 48 GC: 0.229167 acccccccccctttttttttttttcatttttttttttttttttttttt原始文件： g1 atccccccccccccctttg2 enzyme cattatatcttattttttttttttttg3 accccccccccttttttttttttt catttttttttttttttttttttt注意：可以看到bioawk在提取ID的时候只提取了空格分隔后的第一个字段，默认输出域分割符是\t 输出反向互补序列 bioawk -cfastx '{print $name,$seq,revcomp($seq)}' demo.fa 结果： g1 atcccccccccccccttt aaagggggggggggggat g2 cattatatcttatttttttttttttt aaaaaaaaaaaaaataagatataatg g3 acccccccccctttttttttttttcatttttttttttttttttttttt aaaaaaaaaaaaaaaaaaaaaatgaaaaaaaaaaaaaggggggggggt 根据序列ID提取序列，这个是最爱的功能 bioawk -cfastx 'BEGIN{while((getline k &lt;"id.txt")>0)i[k]=1}{if(i[$name])print ">"$name"\n"$seq}' demo.fa id.txt: g1 g2 g2 enzyme g3 结果： &gt;g1 atcccccccccccccttt &gt;g2 cattatatcttatttttttttttttt注意：id.txt 需要去掉&gt; 当然还有其他的花样，大家自己去探索吧 Referencehttps://github.com/lh3/bioawk]]></content>
      <categories>
        <category>Software</category>
      </categories>
      <tags>
        <tag>bioinformatics</tag>
        <tag>bioawk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MARKDOWN's KFC]]></title>
    <url>%2F2019%2F03%2F16%2FMARKDOWN-s-KFC%2F</url>
    <content type="text"><![CDATA[Markdown 是一种轻量级的「标记语言」，创始人为约翰·格鲁伯，用简洁的语法代替排版，目前被越来越多的知识工作者、写作爱好者、程序员或研究员广泛使用。其常用的标记符号不超过十个，相对于更为复杂的 HTML 标记语言来说，Markdown 十分的轻量，学习成本也不需要太多，且一旦熟&gt;悉这种语法规则，会有沉浸式编辑的效果。 MARKDOWN使用Markdown 是什么？Markdown 是一种轻量级的「标记语言」，创始人为约翰·格鲁伯，用简洁的语法代替排版，目前被越来越多的知识工作者、写作爱好者、程序员或研究员广泛使用。其常用的标记符号不超过十个，相对于更为复杂的 HTML 标记语言来说，Markdown 十分的轻量，学习成本也不需要太多，且一旦熟悉这种语法规则，会有沉浸式编辑的效果。另外，印象笔记 Markdown 支持 CommonMark 和 GFM (GitHub Flavored Markdown) 标准。 印象笔记里 Markdown 有什么特点？ 一键创建：支持 Markdown 独立的一键新建入口，为深度 Markdown 用户提供更好的效率体验； 支持丰富的主流 Markdown 语法：支持文字相关样式、序号列表、任务列表、表格、TOC 目录、多种图表、数学公式、流程图、时序图、甘特图等； 支持插入图片：可插入网络图片 或 直接拖动本地图片、复制剪贴板中的图片到 Markdown 笔记中； 支持多种模式切换：编辑与预览模式、纯编辑模式以及纯预览模式； 支持多种编辑主题：预置了白色、黑色、深空灰和印象绿主题，默认为印象绿，未来会有更多主题提供； 跨平台同步：创建的 Markdown 笔记可在登录了印象笔记帐户的各端查看，未来更多端会支持创建和编辑 Markdown 笔记； 演示模式：Markdown 笔记支持演示模式查看； 支持其他印象笔记特点功能：笔记标注、导出 PDF、设置提醒、工作群聊共享-查看&amp;编辑笔记等。 如何创建 Markdown 笔记？ 点击左上角「新建 Markdown 笔记」来创建新的 Markdown 笔记 点击顶部菜单栏-文件-新建Markdown笔记 使用快捷键 CMD+D 来快速创建 Markdown 笔记 印象笔记 Markdown 笔记支持哪些语法？—— 以下语法均支持在编辑工具栏直接操作 —— 设置分级标题语法示例： 一级标题二级标题三级标题四级标题五级标题六级标题 加粗文本语法示例：印象笔记 斜体语法示例：印象笔记 下划线语法示例：印象笔记 删除线语法示例：印象笔记不支持Markdown 添加分隔线语法示例： 引用文本语法示例： 近日，印象笔记宣布完成重组。作为Evernote已在中国独立运营近6年的品牌，印象笔记将成为由中方控股的中美合资独立运营实体，并获得红杉宽带跨境数字产业基金首轮数亿元人民币投资。 添加符号列表或者数字列表语法示例：使用 iOS 版本印象笔记如何快速保存内容？ 启用印象笔记 Widget ——印象笔记·剪贴板 复制粘贴任意内容 微信 滑动到 Widget 插件区域即可完成保存印象笔记·剪贴板有什么特点？ 快：开启自动模式，可以自动保存剪贴板的任意内容 一切：只要可以复制粘贴就可以保存 有序：全部保存在「我的剪贴板」笔记本并以时间来命名 添加待办事项语法示例：三只青蛙 第一只青蛙 第二只青蛙 第三只青蛙 插入链接语法示例：印象笔记官网 插入图片印象笔记支持嵌入网络图片或者直接拖入本地图片，其中本地图片格式支持 jpg、png 和 gif。语法示例： 另外，针对插入的本地图片可以控制图片大小，在拖拽、拷贝或者点击插入本地图片之后，直接在图片名称后面（无需空格）添加以下语法均可以按照以下要求控制图片大小： @w=300 @h=150 @w=200h=100 @h=100w=200示例笔记782d277a1dbc7dea8480267cf5f87ebd.png@w=300 插入表格语法示例：| 帐户类型 | 免费帐户 | 标准帐户 | 高级帐户 || — | — | — | — || 帐户流量 | 60M | 1GB | 10GB || 设备数目 | 2台 | 无限制 | 无限制 || 当前价格 | 免费 | ￥8.17/月 | ￥12.33/月| 插入图表目前支持饼状图、折线图、柱状图和条形图，只需将 type 改为对应的pie、line、column 和 bar。 ,预算,收入,花费,债务 June,5000,8000,4000,6000 July,3000,1000,4000,3000 Aug,5000,7000,6000,3000 Sep,7000,2000,3000,1000 Oct,6000,5000,4000,2000 Nov,4000,3000,5000, type: pie title: 每月收益 x.title: Amount y.title: Month y.suffix: $ 插入行内代码或代码块印象笔记 Markdown 语法支持几十种编程语言的高亮的显示。语法示例： #!/usr/bin/python import re line = "Cats are smarter than dogs" matchObj = re.match( r'(.*) are (.*?) .*', line, re.M|re.I) if matchObj: print "matchObj.group() : ", matchObj.group() print "matchObj.group(1) : ", matchObj.group(1) print "matchObj.group(2) : ", matchObj.group(2) else: print "No match!!" 插入数学公式印象笔记 Markdown 支持绝大多数的 LaTeX 数学公式语法示例： e^{i\pi} + 1 = 0 更多数学公式的输入可以参考： https://khan.github.io/KaTeX/docs/supported.html 插入流程图语法示例： graph TD A[模块A] -->|A1| B(模块B) B --> C{判断条件C} C -->|条件C1| D[模块D] C -->|条件C2| E[模块E] C -->|条件C3| F[模块F] 插入时序图语法示例：sequenceDiagram A->>B: 是否已收到消息？ B-->>A: 已收到消息 插入甘特图语法示例： gantt title 甘特图 dateFormat YYYY-MM-DD section 项目A 任务1 :a1, 2018-06-06, 30d 任务2 :after a1 , 20d section 项目B 任务3 :2018-06-12 , 12d 任务4 : 24d 设置目录设置之后可以自动根据设置的分级标题来自动生成目录。语法示例：[TOC] FBI waring本篇文档完全搬运自markdown官方中文文档，仅供测试网页使用，如侵删。https://list.yinxiang.com/markdown/eef42447-db3f-48ee-827b-1bb34c03eb83.php]]></content>
      <categories>
        <category>Software</category>
      </categories>
      <tags>
        <tag>markdown usage</tag>
      </tags>
  </entry>
</search>
